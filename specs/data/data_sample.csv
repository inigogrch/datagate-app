id,external_id,source_id,title,url,content,summary,tags
2b9c3a4f-e323-4a99-8e8b-f27f9b244be3,https://openai.com/global-affairs/eu-code-of-practice,00fa815e-01e7-4635-9bc3-3c6e06adc897,The EU Code of Practice and future of AI in Europe,https://openai.com/global-affairs/eu-code-of-practice,"OpenAI joins the EU Code of Practice, advancing responsible AI while partnering with European governments to drive innovation, infrastructure, and economic growth.","OpenAI joins the EU Code of Practice, advancing responsible AI while partnering with European governments to drive innovation, infrastructure, and economic growth.","[""artificial-intelligence"",""breakthrough"",""devops"",""openai"",""anthropic"",""huggingface""]"
a22d915f-bcc4-49d6-890b-b3b68b7f27ba,https://openai.com/sam-and-jony,00fa815e-01e7-4635-9bc3-3c6e06adc897,Sam & Jony,https://openai.com/sam-and-jony,Building a family of AI products for everyone.,Building a family of AI products for everyone.,"[""artificial-intelligence"",""anthropic"",""huggingface"",""openai"",""apple"",""stanford""]"
404ece9c-f901-43c7-bf8a-c49b2cb04b17,https://openai.com/global-affairs/aft,00fa815e-01e7-4635-9bc3-3c6e06adc897,"Working with 400,000 teachers to shape the future of AI in schools",https://openai.com/global-affairs/aft,"OpenAI partners with the American Federation of Teachers to launch a 5-year initiative equipping 400,000 K-12 educators to lead AI innovation in classrooms.","OpenAI partners with the American Federation of Teachers to launch a 5-year initiative equipping 400,000 K-12 educators to lead AI innovation in classrooms.","[""announcement"",""artificial-intelligence"",""breakthrough"",""openai"",""anthropic"",""huggingface""]"
4431e0ea-2e06-4f58-b973-09c7af497067,https://openai.com/index/genspark,00fa815e-01e7-4635-9bc3-3c6e06adc897,"No-code personal agents, powered by GPT-4.1 and Realtime API",https://openai.com/index/genspark,Learn how Genspark built a $36M ARR AI product in 45 days—with no-code agents powered by GPT-4.1 and OpenAI Realtime API.,Learn how Genspark built a $36M ARR AI product in 45 days—with no-code agents powered by GPT-4.1 and OpenAI Realtime API.,"[""api"",""artificial-intelligence"",""energy"",""openai"",""spark"",""transformers""]"
93eca276-48bf-4d59-a641-39c67376383f,https://openai.com/global-affairs/openais-australia-economic-blueprint,00fa815e-01e7-4635-9bc3-3c6e06adc897,AI in Australia—OpenAI’s Economic Blueprint,https://openai.com/global-affairs/openais-australia-economic-blueprint,"Today, OpenAI, in partnership with Mandala Partners, is sharing the OpenAI AI Economic Blueprint for Australia. At a time when boosting productivity has emerged as a national priority for Australia, the Blueprint provides a clear, actionable plan for how Australia can unlock the full economic and social potential of artificial intelligence.","Today, OpenAI, in partnership with Mandala Partners, is sharing the OpenAI AI Economic Blueprint for Australia. At a time when boosting productivity has emerged as a national priority for Australia, t...","[""artificial-intelligence"",""collaboration"",""openai"",""anthropic"",""huggingface"",""agents""]"
2b80f6a6-f1e0-4904-a0d3-8ac3fde9e318,https://openai.com/index/retell-ai,00fa815e-01e7-4635-9bc3-3c6e06adc897,"Customizable, no-code voice agent automation with GPT-4o",https://openai.com/index/retell-ai,"Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs, boost CSAT, and automate customer conversations—without scripts or hold times.","Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs,...","[""artificial-intelligence"",""computer-science"",""devops"",""energy"",""tensorflow"",""transformers""]"
a03e4db0-5b57-4f46-97d8-145eb188d7f0,https://openai.com/index/unify,00fa815e-01e7-4635-9bc3-3c6e06adc897,"Driving scalable growth with OpenAI o3, GPT-4.1, and CUA",https://openai.com/index/unify,"Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.","Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams ge...","[""artificial-intelligence"",""energy"",""python"",""research"",""tensorflow"",""transformers""]"
bd1f4ab8-574b-486e-ab62-7da95bb6fb0e,aa60f79d1fccc39b,08c7a8dc-5726-457d-9465-978e5eabff10,Graph foundation models for relational data,https://research.google/blog/graph-foundation-models-for-relational-data/,"# Graph foundation models for relational data

Google Research blog post published on July 10, 2025.

**Research Areas:** algorithms & theory, machine intelligence

**Published:** July 10, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/graph-foundation-models-for-relational-data/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# Graph foundation models for relational data

Google Research blog post published on July 10, 2025.

**Research Areas:** algorithms & theory, machine intelligence

**Published:** July 10, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI...","[""algorithms"",""artificial-intelligence"",""computer-science"",""education"",""machine-learning"",""research""]"
a5fbfb25-0d26-4305-9981-e312fd3435a8,aead8190ead32bde,08c7a8dc-5726-457d-9465-978e5eabff10,MedGemma: Our most capable open models for health AI development,https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/,"# MedGemma: Our most capable open models for health AI development

Google Research blog post published on July 9, 2025.

**Research Areas:** generative ai, health & bioscience, machine intelligence

**Published:** July 9, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# MedGemma: Our most capable open models for health AI development

Google Research blog post published on July 9, 2025.

**Research Areas:** generative ai, health & bioscience, machine intelligence

**Published:** July 9, 2025
**Source:** Google Research Blog

This post covers cutting-edge research...","[""artificial-intelligence"",""computer-science"",""education"",""machine-learning"",""mobile-development"",""research""]"
c4630f3f-12b5-46c1-9451-9f830f3bfeee,13e56aaf2b12c3cf,08c7a8dc-5726-457d-9465-978e5eabff10,Making group conversations more accessible with sound localization,https://research.google/blog/making-group-conversations-more-accessible-with-sound-localization/,"# Making group conversations more accessible with sound localization

Google Research blog post published on July 2, 2025.

**Research Areas:** human-computer interaction and visualization, sound & accoustics, speech processing

**Published:** July 2, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/making-group-conversations-more-accessible-with-sound-localization/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# Making group conversations more accessible with sound localization

Google Research blog post published on July 2, 2025.

**Research Areas:** human-computer interaction and visualization, sound & accoustics, speech processing

**Published:** July 2, 2025
**Source:** Google Research Blog

This post...","[""artificial-intelligence"",""computer-science"",""data-science"",""education"",""imaging"",""machine-learning""]"
0899e93c-1904-4edc-8576-4b49b9ad1a2b,c8850bbeafee27d1,08c7a8dc-5726-457d-9465-978e5eabff10,How we created HOV-specific ETAs in Google Maps,https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/,"# How we created HOV-specific ETAs in Google Maps

Google Research blog post published on June 30, 2025.

**Research Areas:** algorithms & theory, data mining & modeling, machine intelligence

**Published:** June 30, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# How we created HOV-specific ETAs in Google Maps

Google Research blog post published on June 30, 2025.

**Research Areas:** algorithms & theory, data mining & modeling, machine intelligence

**Published:** June 30, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and d...","[""algorithms"",""artificial-intelligence"",""computer-science"",""data-analysis"",""education"",""machine-learning""]"
dae42e09-b455-4cb9-9ebd-4abf8bd08438,0bc765e609095f1f,08c7a8dc-5726-457d-9465-978e5eabff10,REGEN: Empowering personalized recommendations with natural language,https://research.google/blog/regen-empowering-personalized-recommendations-with-natural-language/,"# REGEN: Empowering personalized recommendations with natural language

Google Research blog post published on June 27, 2025.

**Research Areas:** data mining & modeling, general science, machine intelligence

**Published:** June 27, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/regen-empowering-personalized-recommendations-with-natural-language/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# REGEN: Empowering personalized recommendations with natural language

Google Research blog post published on June 27, 2025.

**Research Areas:** data mining & modeling, general science, machine intelligence

**Published:** June 27, 2025
**Source:** Google Research Blog

This post covers cutting-ed...","[""artificial-intelligence"",""computer-science"",""data-analysis"",""energy"",""machine-learning"",""natural-language-processing""]"
cc5b45f3-689d-4409-ba76-1264acaafc74,6fcd766a4b0a3b2c,08c7a8dc-5726-457d-9465-978e5eabff10,MUVERA: Making multi-vector retrieval as fast as single-vector search,https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/,"# MUVERA: Making multi-vector retrieval as fast as single-vector search

Google Research blog post published on June 25, 2025.

**Research Areas:** algorithms & theory, machine intelligence

**Published:** June 25, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# MUVERA: Making multi-vector retrieval as fast as single-vector search

Google Research blog post published on June 25, 2025.

**Research Areas:** algorithms & theory, machine intelligence

**Published:** June 25, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and dev...","[""algorithms"",""artificial-intelligence"",""computer-science"",""education"",""machine-learning"",""research""]"
b09aef40-1ab0-4560-9b0c-859aedec455f,6083c6d0ae3fb79e,08c7a8dc-5726-457d-9465-978e5eabff10,From research to climate resilience,https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/,"# From research to climate resilience

Google Research blog post published on June 24, 2025.

**Research Areas:** climate & sustainability, general science, generative ai

**Published:** June 24, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Google's AI and research teams.

[Read Full Article](https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/)

---
*This is a research update from Google's official research blog, covering developments in artificial intelligence, machine learning, and computer science.*","# From research to climate resilience

Google Research blog post published on June 24, 2025.

**Research Areas:** climate & sustainability, general science, generative ai

**Published:** June 24, 2025
**Source:** Google Research Blog

This post covers cutting-edge research and developments from Goog...","[""artificial-intelligence"",""climate-science"",""computer-science"",""education"",""machine-learning"",""research""]"
86001336-51be-41cc-8d2b-020eb3981a2f,https://news.mit.edu/2025/scientists-discover-compounds-helping-cells-fight-wide-range-viruses-0714,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,Scientists discover compounds that help cells fight a wide range of viruses,https://news.mit.edu/2025/scientists-discover-compounds-helping-cells-fight-wide-range-viruses-0714,"Researchers at MIT and other institutions have identified compounds that can fight off viral infection by activating a defense pathway inside host cells. These compounds, they believe, could be used as antiviral drugs that work against not just one but any kind of virus.

The researchers identified these compounds, which activate a host cell defense system known as the integrated stress response pathway, in a screen of nearly 400,000 molecules. In tests in human cells, the researchers showed that the compounds help cells fend off infection from RSV, herpes virus, and Zika virus. They also proved effective in combating herpes infection in a mouse model.

The research team now plans to test the compounds against additional viruses, in hopes of developing them for eventual clinical trials.

“We’re very excited about this work, which allows us to harness the stress response of the host cells to arrive at a means to identify and develop broad-spectrum antivirals,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering.

Collins and Maxwell Wilson, an associate professor of molecular biology at the University of California, Santa Barbara and chief scientific officer of Integrated Biosciences, are the senior authors of the new study, which [appears in _Cell_](https://www.cell.com/cell/abstract/S0092-8674\(25\)00690-7). Felix Wong, a former MIT postdoc and chief executive officer of Integrated Biosciences, is the lead author of the paper. In addition to MIT, UCSB, and Integrated Biosciences, the research team also includes scientists from Illumina Ventures and Princeton University.

**Boosting cell defense**

In human cells, the integrated stress response pathway is turned on in response to viral infection as well as other types of stress such as starvation. During viral infection, the pathway is triggered by double-stranded RNA, a molecule produced during the replication cycle of viruses. When that RNA is detected, the cell shuts down protein synthesis, which blocks the virus from producing the proteins it needs to replicate.

Compounds that boost this pathway, the researchers believe, could be good candidates for new antiviral drugs that could combat any type of virus.

“Typically, how antivirals are developed is that you develop one antiviral for one specific virus,” Wong says. “In this case, we hypothesized that being able to modulate the host cell stress response might give us a new class of broad-spectrum antivirals — compounds that directly act on the host cells to alter something fundamental about how all viruses replicate.”

To help them identify compounds that would enhance the activity of this pathway during viral infection, the researchers invented a novel optogenetic screen. Optogenetics is a bioengineering technique that allows researchers to insert light-sensitive proteins into the genome of a cell. In this case, the researchers engineered modifications to a protein called PKR, which turns on the stress pathway, so that they could turn it on with light.

Using this technique, the researchers screened a library of nearly 400,000 commercially available and proprietary chemical compounds. Each of these compounds was applied to human cells as the cells were also exposed to blue light, which simulated viral infection by activating PKR.

By measuring the cells’ survival rates, the researchers could determine which compounds boosted activation of the pathway and amplified the cells’ ability to shut down viral reproduction. This screen yielded about 3,500 compounds with potential antiviral activity, which were evaluated further.

“If the pathway were turned on in response to viral infection, what our compounds do is they turn it on full blast,” Wong says. “Even in the presence of a small amount of virus, if the pathway is triggered, then the antiviral response is also maximized.”

**Fighting infection**

The researchers then selected eight of the most promising compounds and screened them for their ability to kill viruses while avoiding harmful effects in human cells. Based on these tests, the researchers chose three top candidates, which they called IBX-200, IBX-202, and IBX-204.

In cells that were infected with either Zika virus, herpes virus, or RSV, treatment with these compounds significantly reduced the amount of virus in the cells. The researchers then tested one of the compounds, IBX-200, in mice infected with herpes virus, and found that it was able to reduce the viral load and improve symptoms.

Experiments showed that these compounds appear to turn on an enzyme that is involved in detecting stress. This activates the stress response pathway and primes the cells to become more responsive to viral infection. When applied to cells that are not already infected, the compounds have no effect.

The researchers now plan to evaluate their lead candidates against a broader range of viruses. They also aim to identify additional compounds that activate the integrated stress response, as well as other cellular stress pathways with the potential to clear viral or bacterial infections.

The research was funded by the Defense Threat Reduction Agency, the National Science Foundation, the U.S. Army Research Office, and Integrated Biosciences.","Researchers at MIT and other institutions have identified compounds that can fight off viral infection by activating a defense pathway inside host cells. These compounds, they believe, could be used a...","[""artificial-intelligence"",""biology"",""biomedical-engineering"",""chemistry"",""computer-science"",""reinforcement-learning""]"
a5cbd460-897f-49e7-9713-e4b398e4a0ef,https://news.mit.edu/2025/simulation-based-pipeline-tailors-training-data-dexterous-robots-0711,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,Simulation-based pipeline tailors training data for dexterous robots,https://news.mit.edu/2025/simulation-based-pipeline-tailors-training-data-dexterous-robots-0711,"When ChatGPT or Gemini give what seems to be an expert response to your burning questions, you may not realize how much information it relies on to give that reply. Like other popular generative artificial intelligence (AI) models, these chatbots rely on backbone systems called foundation models that train on billions, or even trillions, of data points.

In a similar vein, engineers are hoping to build foundation models that train a range of robots on new skills like picking up, moving, and putting down objects in places like homes and factories. The problem is that it’s difficult to collect and transfer instructional data across robotic systems. You could teach your system by teleoperating the hardware step-by-step using technology like virtual reality (VR), but that can be time-consuming. Training on videos from the internet is less instructive, since the clips don’t provide a step-by-step, specialized task walk-through for particular robots.  
  
A simulation-driven approach called “PhysicsGen” from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Robotics and AI Institute customizes robot training data to help robots find the most efficient movements for a task. The system can multiply a few dozen VR demonstrations into nearly 3,000 simulations per machine. These high-quality instructions are then mapped to the precise configurations of mechanical companions like robotic arms and hands. 

PhysicsGen creates data that generalize to specific robots and condition via a three-step process. First, a VR headset tracks how humans manipulate objects like blocks using their hands. These interactions are mapped in a 3D physics simulator at the same time, visualizing the key points of our hands as small spheres that mirror our gestures. For example, if you flipped a toy over, you’d see 3D shapes representing different parts of your hands rotating a virtual version of that object.  
  
The pipeline then remaps these points to a 3D model of the setup of a specific machine (like a robotic arm), moving them to the precise “joints” where a system twists and turns. Finally, PhysicsGen uses trajectory optimization — essentially simulating the most efficient motions to complete a task — so the robot knows the best ways to do things like repositioning a box.

Each simulation is a detailed training data point that walks a robot through potential ways to handle objects. When implemented into a policy (or the action plan that the robot follows), the machine has a variety of ways to approach a task, and can try out different motions if one doesn’t work.

“We’re creating robot-specific data without needing humans to re-record specialized demonstrations for each machine,” says Lujie Yang, an MIT PhD student in electrical engineering and computer science and CSAIL affiliate who is the lead author of a new [paper](https://arxiv.org/abs/2502.20382) introducing the project. “We’re scaling up the data in an autonomous and efficient way, making task instructions useful to a wider range of machines.”  
  
Generating so many instructional trajectories for robots could eventually help engineers build a massive dataset to guide machines like robotic arms and dexterous hands. For example, the pipeline might help two robotic arms collaborate on picking up warehouse items and placing them in the right boxes for deliveries. The system may also guide two robots to work together in a household on tasks like putting away cups.  
  
PhysicsGen’s potential also extends to converting data designed for older robots or different environments into useful instructions for new machines. “Despite being collected for a specific type of robot, we can revive these prior datasets to make them more generally useful,” adds Yang.

**Addition by multiplication**  
  
PhysicsGen turned just 24 human demonstrations into thousands of simulated ones, helping both digital and real-world robots reorient objects.  
  
Yang and her colleagues first tested their pipeline in a virtual experiment where a floating robotic hand needed to rotate a block into a target position. The digital robot executed the task at a rate of 81 percent accuracy by training on PhysicGen’s massive dataset, a 60 percent improvement from a baseline that only learned from human demonstrations.  
  
The researchers also found that PhysicsGen could improve how virtual robotic arms collaborate to manipulate objects. Their system created extra training data that helped two pairs of robots successfully accomplish tasks as much as 30 percent more often than a purely human-taught baseline.

In an experiment with a pair of real-world robotic arms, the researchers observed similar improvements as the machines teamed up to flip a large box into its designated position. When the robots deviated from the intended trajectory or mishandled the object, they were able to recover mid-task by referencing alternative trajectories from their library of instructional data.  
  
Senior author Russ Tedrake, who is the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT, adds that this imitation-guided data generation technique combines the strengths of human demonstration with the power of robot motion planning algorithms.  
  
“Even a single demonstration from a human can make the motion planning problem much easier,” says Tedrake, who is also a senior vice president of large behavior models at the Toyota Research Institute and CSAIL principal investigator. “In the future, perhaps the foundation models will be able to provide this information, and this type of data generation technique will provide a type of post-training recipe for that model.”

**The future of PhysicsGen**

Soon, PhysicsGen may be extended to a new frontier: diversifying the tasks a machine can execute.

“We’d like to use PhysicsGen to teach a robot to pour water when it’s only been trained to put away dishes, for example,” says Yang. “Our pipeline doesn’t just generate dynamically feasible motions for familiar tasks; it also has the potential of creating a diverse library of physical interactions that we believe can serve as building blocks for accomplishing entirely new tasks a human hasn’t demonstrated.”

Creating lots of widely applicable training data may eventually help build a foundation model for robots, though MIT researchers caution that this is a somewhat distant goal. The CSAIL-led team is investigating how PhysicsGen can harness vast, unstructured resources — like internet videos — as seeds for simulation. The goal: transform everyday visual content into rich, robot-ready data that could teach machines to perform tasks no one explicitly showed them.

Yang and her colleagues also aim to make PhysicsGen even more useful for robots with diverse shapes and configurations in the future. To make that happen, they plan to leverage datasets with demonstrations of real robots, capturing how robotic joints move instead of human ones.

The researchers also plan to incorporate reinforcement learning, where an AI system learns by trial and error, to make PhysicsGen expand its dataset beyond human-provided examples. They may augment their pipeline with advanced perception techniques to help a robot perceive and interpret their environment visually, allowing the machine to analyze and adapt to the complexities of the physical world.

For now, PhysicsGen shows how AI can help us teach different robots to manipulate objects within the same category, particularly rigid ones. The pipeline may soon help robots find the best ways to handle soft items (like fruits) and deformable ones (like clay), but those interactions aren’t easy to simulate yet.

Yang and Tedrake wrote the paper with two CSAIL colleagues: co-lead author and MIT PhD student Hyung Ju “Terry” Suh SM ’22 and MIT PhD student Bernhard Paus Græsdal. Robotics and AI Institute researchers Tong Zhao ’22, MEng ’23, Tarik Kelestemur, Jiuguang Wang, and Tao Pang PhD ’23 are also authors. Their work was supported by the Robotics and AI Institute and Amazon.  
  
The researchers recently presented their work at the Robotics: Science and Systems conference.","When ChatGPT or Gemini give what seems to be an expert response to your burning questions, you may not realize how much information it relies on to give that reply. Like other popular generative artif...","[""algorithms"",""artificial-intelligence"",""hardware"",""networking"",""python"",""reinforcement-learning""]"
a5bea769-4bb0-442c-8775-d6e3ca5ebf5d,https://news.mit.edu/2025/ai-system-uncovers-hidden-cell-subtypes-boosts-precision-medicine-0711,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,"New AI system uncovers hidden cell subtypes, boosts precision medicine",https://news.mit.edu/2025/ai-system-uncovers-hidden-cell-subtypes-boosts-precision-medicine-0711,"In order to produce effective targeted therapies for cancer, scientists need to isolate the genetic and phenotypic characteristics of cancer cells, both within and across different tumors, because those differences impact how tumors respond to treatment.

Part of this work requires a deep understanding of the RNA or protein molecules each cancer cell expresses, where it is located in the tumor, and what it looks like under a microscope.

Traditionally, scientists have looked at one or more of these aspects separately, but now a new deep learning AI tool, CellLENS (Cell Local Environment and Neighborhood Scan), fuses all three domains together, using a combination of convolutional neural networks and graph neural networks to build a comprehensive digital profile for every single cell. This allows the system to group cells with similar biology — effectively separating even those that appear very similar in isolation, but behave differently depending on their surroundings.

The study, [published recently in _Nature Immunology_](https://pubmed.ncbi.nlm.nih.gov/40404817/), details the results of a collaboration between researchers from MIT, Harvard Medical School, Yale University, Stanford University, and University of Pennsylvania — an effort led by Bokai Zhu, an MIT postdoc and member of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org/) and the [Ragon Institute of MGH, MIT, and Harvard](https://ragoninstitute.org/).

Zhu explains the impact of this new tool: “Initially we would say, oh, I found a cell. This is called a T cell. Using the same dataset, by applying CellLENS, now I can say this is a T cell, and it is currently attacking a specific tumor boundary in a patient.

“I can use existing information to better define what a cell is, what is the subpopulation of that cell, what that cell is doing, and what is the potential functional readout of that cell. This method may be used to identify a new biomarker, which provides specific and detailed information about diseased cells, allowing for more targeted therapy development.”

This is a critical advance because current methodologies often miss critical molecular or contextual information — for example, immunotherapies may target cells that only exist at the boundary of a tumor, limiting efficacy. By using deep learning, the researchers can detect many different layers of information with CellLENS, including morphology and where the cell is spatially in a tissue.

When applied to samples from healthy tissue and several types of cancer, including lymphoma and liver cancer, CellLENS uncovered rare immune cell subtypes and revealed how their activity and location relate to disease processes — such as tumor infiltration or immune suppression.

These discoveries could help scientists better understand how the immune system interacts with tumors and pave the way for more precise cancer diagnostics and immunotherapies.

“I’m extremely excited by the potential of new AI tools, like CellLENS, to help us more holistically understand aberrant cellular behaviors within tissues,” says co-author [Alex K. Shalek](https://chemistry.mit.edu/profile/alex-k-shalek/), the director of the [Institute for Medical Engineering and Science](https://imes.mit.edu/) (IMES), the J. W. Kieckhefer Professor in IMES and Chemistry, and an extramural member of the [Koch Institute for Integrative Cancer Research at MIT](https://ki.mit.edu/), as well as an Institute member of the Broad Institute and a member of the Ragon Institute. “We can now measure a tremendous amount of information about individual cells and their tissue contexts with cutting-edge, multi-omic assays. Effectively leveraging that data to nominate new therapeutic leads is a critical step in developing improved interventions. When coupled with the right input data and careful downsteam validations, such tools promise to accelerate our ability to positively impact human health and wellness.”","In order to produce effective targeted therapies for cancer, scientists need to isolate the genetic and phenotypic characteristics of cancer cells, both within and across different tumors, because tho...","[""api"",""artificial-intelligence"",""computer-science"",""deep-learning"",""neural-networks"",""python""]"
a5a2cc2c-7f13-4268-b5cb-18c9873718a0,https://news.mit.edu/2025/study-shows-link-between-obesity-and-local-restaurant-menus-0711,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,Study shows a link between obesity and what’s on local restaurant menus,https://news.mit.edu/2025/study-shows-link-between-obesity-and-local-restaurant-menus-0711,"For many years, health experts have been concerned about “food deserts,” places where residents lack good nutritional options. Now, an MIT-led study of three major global cities uses a new, granular method to examine the issue, and concludes that having fewer and less nutritional eating options nearby correlates with obesity and other health outcomes.

Rather than just mapping geographic areas, the researchers examined the dietary value of millions of food items on roughly 30,000 restaurant menus and derived a more precise assessment of the connection between neighborhoods and nutrition.

“We show that what is sold in a restaurant has a direct correlation to people’s health,” says MIT researcher Fabio Duarte, co-author of a newly published paper outlining the study’s results. “The food landscape matters.”

The open-access paper, “[Data-driven nutritional assessment of urban food landscapes: insights from Boston, London, Dubai](https://www.nature.com/articles/s41598-025-08098-9),” was published this week in _Nature: Scientific Reports_.

The co-authors are Michael Tufano, a PhD student at Wageningen University, in the Netherlands; Duarte, associate director of MIT’s Senseable City Lab, which uses data to study cities as dynamic systems; Martina Mazzarello, a postdoc at the Senseable City Lab; Javad Eshtiyagh, a research fellow at the Senseable City Lab; Carlo Ratti, professor of the practice and director of the Senseable City Lab; and Guido Camps, a senior researcher at Wageningen University.

**Scanning the menu**

To conduct the study, the researchers examined menus from Boston, Dubai, and London, in the summer of 2023, compiling a database of millions of items available through popular food-delivery platforms. The team then evaluated the food items as rated by the USDA’s FoodData Central database, an information bank with 375,000 kinds of food products listed. The study deployed two main metrics, the Meal Balance Index, and the Nutrient-Rich Foods Index.

The researchers examined about 222,000 menu items from over 2,000 restaurants in Boston, about 1.6 million menu items from roughly 9,000 restaurants in Dubai, and about 3.1 million menu items from about 18,000 restaurants in London. In Boston, about 71 percent of the items were in the USDA database; in Dubai and London, that figure was 42 percent and 56 percent, respectively.

The team then rated the nutritional value of the items appearing on menus, and correlated the food data with health-outcome data from Boston and London. In London, they found a clear correlation between neighborhood menu offerings and obesity, or the lack thereof; with a slightly less firm correlation in Boston. Areas with food options that include a lot of dietary fibers, sometimes along with fruits and vegetables, tend to have better health data.

In Dubai, the researchers did not have the same types of health data available but did observe a strong correlation between rental prices and the nutritional value of neighborhood-level food, suggesting that wealthier residents have better nourishment options.

“At the item level, when we have less nutritional food, we see more cases of obsesity,” Tufano says. “It’s true that not only do we have more fast food in poor neighborhoods, but the nutritional value is not the same.”

**Re-mapping the food landscape**

By conducting the study in this fashion, the scholars added a layer of analysis to past studies of food deserts. While past work has broken ground by identifying neighborhoods and areas lacking good food access, this research makes a more comprehensive assessment of what people consume. The research moves toward evaluating the complex mix of food available in any given area, which can be true even of areas with more limited options.

“We were not satisfied with this idea that if you only have fast food, it’s a food desert, but if you have a Whole Foods, it’s not,” Duarte says. “It’s not necessarily like that.”

For the Senseable City Lab researchers, the study is a new technique further enabling them to understand city dynamics and the effects of the urban environment on health. Past lab studies have often focused on issues such as urban mobility, while extending to matters such as mobility and air pollution, among other topics.

Being able to study food and health at the neighborhood level, though, is still another example of the ways that data-rich spheres of life can be studied in close detail.

“When we started working on cities and data, the data resolution was so low,” Ratti says. “Today the amount of data is so immense we see this great opportunity to look at cities and see the influence of the urban environment as a big determinant of health. We see this as one of the new frontiers of our lab. It’s amazing how we can now look at this very precisely in cities.”","For many years, health experts have been concerned about “food deserts,” places where residents lack good nutritional options. Now, an MIT-led study of three major global cities uses a new, granular m...","[""api"",""artificial-intelligence"",""computer-science"",""database"",""reinforcement-learning"",""tensorflow""]"
41712e90-3f79-4865-baad-f92ad0237121,https://news.mit.edu/2025/bionic-knee-integrated-into-tissue-can-restore-natural-movement-0710,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,A bionic knee integrated into tissue can restore natural movement,https://news.mit.edu/2025/bionic-knee-integrated-into-tissue-can-restore-natural-movement-0710,"MIT researchers have developed a new bionic knee that can help people with above-the-knee amputations walk faster, climb stairs, and avoid obstacles more easily than they could with a traditional prosthesis.

Unlike prostheses in which the residual limb sits within a socket, the new system is directly integrated with the user’s muscle and bone tissue. This enables greater stability and gives the user much more control over the movement of the prosthesis.

Participants in a small clinical study also reported that the limb felt more like a part of their own body, compared to people who had more traditional above-the-knee amputations.

“A prosthesis that's tissue-integrated — anchored to the bone and directly controlled by the nervous system — is not merely a lifeless, separate device, but rather a system that is carefully integrated into human physiology, offering a greater level of prosthetic embodiment. It’s not simply a tool that the human employs, but rather an integral part of self,” says Hugh Herr, a professor of media arts and sciences, co-director of the K. Lisa Yang Center for Bionics at MIT, an associate member of MIT’s McGovern Institute for Brain Research, and the senior author of the new study.

Tony Shu PhD ’24 is the lead author of the [paper](http://doi.org/10.1126/science.adv3223), which appears today in _Science_.

**Better control**

Over the past several years, Herr’s lab has been working on new prostheses that can extract neural information from muscles left behind after an amputation and use that information to help guide a prosthetic limb.

During a traditional amputation, pairs of muscles that take turns stretching and contracting are usually severed, disrupting the normal agonist-antagonist relationship of the muscles. This disruption makes it very difficult for the nervous system to sense the position of a muscle and how fast it’s contracting.

Using the new surgical approach developed by Herr and his colleagues, known as agonist-antagonist myoneuronal interface (AMI), muscle pairs are reconnected during surgery so that they still dynamically communicate with each other within the residual limb. This sensory feedback helps the wearer of the prosthesis to decide how to move the limb, and also generates electrical signals that can be used to control the prosthetic limb.

In a [2024 study](https://news.mit.edu/2024/prosthesis-helps-people-with-amputation-walk-naturally-0701), the researchers showed that people with amputations below the knee who received the AMI surgery were able to walk faster and navigate around obstacles much more naturally than people with traditional below-the-knee amputations.

In the new study, the researchers extended the approach to better serve people with amputations above the knee. They wanted to create a system that could not only read out signals from the muscles using AMI but also be integrated into the bone, offering more stability and better sensory feedback.

To achieve that, the researchers developed a procedure to insert a titanium rod into the residual femur bone at the amputation site. This implant allows for better mechanical control and load bearing than a traditional prosthesis. Additionally, the implant contains 16 wires that collect information from electrodes located on the AMI muscles inside the body, which enables more accurate transduction of the signals coming from the muscles.

This bone-integrated system, known as e-OPRA, transmits AMI signals to a new robotic controller developed specifically for this study. The controller uses this information to calculate the torque necessary to move the prosthesis the way that the user wants it to move.

“All parts work together to better get information into and out of the body and better interface mechanically with the device,” Shu says. “We’re directly loading the skeleton, which is the part of the body that’s supposed to be loaded, as opposed to using sockets, which is uncomfortable and can lead to frequent skin infections.”

In this study, two subjects received the combined AMI and e-OPRA system, known as an osseointegrated mechanoneural prosthesis (OMP). These users were compared with eight who had the AMI surgery but not the e-OPRA implant, and seven users who had neither AMI nor e-OPRA. All subjects took a turn at using an experimental powered knee prosthesis developed by the lab.

The researchers measured the participants’ ability to perform several types of tasks, including bending the knee to a specified angle, climbing stairs, and stepping over obstacles. In most of these tasks, users with the OMP system performed better than the subjects who had the AMI surgery but not the e-OPRA implant, and much better than users of traditional prostheses.

“This paper represents the fulfillment of a vision that the scientific community has had for a long time — the implementation and demonstration of a fully physiologically integrated, volitionally controlled robotic leg,” says Michael Goldfarb, a professor of mechanical engineering and director of the Center for Intelligent Mechatronics at Vanderbilt University, who was not involved in the research. “This is really difficult work, and the authors deserve tremendous credit for their efforts in realizing such a challenging goal.”

**A sense of embodiment**

In addition to testing gait and other movements, the researchers also asked questions designed to evaluate participants’ sense of embodiment — that is, to what extent their prosthetic limb felt like a part of their own body.

Questions included whether the patients felt as if they had two legs, if they felt as if the prosthesis was part of their body, and if they felt in control of the prosthesis. Each question was designed to evaluate the participants’ feelings of agency, ownership of device, and body representation.

The researchers found that as the study went on, the two participants with the OMP showed much greater increases in their feelings of agency and ownership than the other subjects.

“Another reason this paper is significant is that it looks into these embodiment questions and it shows large improvements in that sensation of embodiment,” Herr says. “No matter how sophisticated you make the AI systems of a robotic prosthesis, it’s still going to feel like a tool to the user, like an external device. But with this tissue-integrated approach, when you ask the human user what is their body, the more it’s integrated, the more they’re going to say the prosthesis is actually part of self.”

The AMI procedure is now done routinely on patients with below-the-knee amputations at Brigham and Women’s Hospital, and Herr expects it will soon become the standard for above-the-knee amputations as well. The combined OMP system will need larger clinical trials to receive FDA approval for commercial use, which Herr expects may take about five years.

The research was funded by the Yang Tan Collective and DARPA.","MIT researchers have developed a new bionic knee that can help people with above-the-knee amputations walk faster, climb stairs, and avoid obstacles more easily than they could with a traditional pros...","[""api"",""artificial-intelligence"",""computer-science"",""robotics"",""sensors"",""transformers""]"
9db73b96-fd58-48f8-96d2-a55d5ad8fbf2,https://news.mit.edu/2025/changing-conversation-health-care-0709,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,Changing the conversation in health care,https://news.mit.edu/2025/changing-conversation-health-care-0709,"Generative artificial intelligence is transforming the ways humans write, read, speak, think, empathize, and act within and across languages and cultures. In health care, gaps in communication between patients and practitioners can worsen patient outcomes and prevent improvements in practice and care. The Language/AI Incubator, made possible through funding from the [MIT Human Insight Collaborative](https://shass.mit.edu/human-insight-collaborative/) (MITHIC), offers a potential response to these challenges. 

The project envisions a research community rooted in the humanities that will foster interdisciplinary collaboration across MIT to deepen understanding of generative AI’s impact on cross-linguistic and cross-cultural communication. The project’s focus on health care and communication seeks to build bridges across socioeconomic, cultural, and linguistic strata.

The incubator is co-led by [Leo Celi](https://imes.mit.edu/people/celi-leo), a physician and the research director and senior research scientist with the [Institute for Medical Engineering and Science](https://imes.mit.edu/) (IMES), and [Per Urlaub](https://languages.mit.edu/people/per-urlaub/), professor of the practice in German and second language studies and director of MIT’s [Global Languages](https://languages.mit.edu/) program. 

“The basis of health care delivery is the knowledge of health and disease,” Celi says. “We’re seeing poor outcomes despite massive investments because our knowledge system is broken.”

**A chance collaboration**

Urlaub and Celi met during a MITHIC launch event. Conversations during the event reception revealed a shared interest in exploring improvements in medical communication and practice with AI.

“We’re trying to incorporate data science into health-care delivery,” Celi says. “We’ve been recruiting social scientists \[at IMES\] to help advance our work, because the science we create isn’t neutral.”

Language is a non-neutral mediator in health care delivery, the team believes, and can be a boon or barrier to effective treatment. “Later, after we met, I joined one of his working groups whose focus was metaphors for pain: the language we use to describe it and its measurement,” Urlaub continues. “One of the questions we considered was how effective communication can occur between doctors and patients.”

Technology, they argue, impacts casual communication, and its impact depends on both users and creators. As AI and large language models (LLMs) gain power and prominence, their use is broadening to include fields like health care and wellness. 

Rodrigo Gameiro, a physician and researcher with MIT’s Laboratory for Computational Physiology, is another program participant. He notes that work at the laboratory centers responsible AI development and implementation. Designing systems that leverage AI effectively, particularly when considering challenges related to communicating across linguistic and cultural divides that can occur in health care, demands a nuanced approach. 

“When we build AI systems that interact with human language, we’re not just teaching machines how to process words; we’re teaching them to navigate the complex web of meaning embedded in language,” Gameiro says.

Language’s complexities can impact treatment and patient care. “Pain can only be communicated through metaphor,” Urlaub continues, “but metaphors don’t always match, linguistically and culturally.” Smiley faces and one-to-10 scales — pain measurement tools English-speaking medical professionals may use to assess their patients — may not travel well across racial, ethnic, cultural, and language boundaries.

**“Science has to have a heart”** 

LLMs can potentially help scientists improve health care, although there are some systemic and pedagogical challenges to consider. Science can focus on outcomes to the exclusion of the people it’s meant to help, Celi argues. “Science has to have a heart,” he says. “Measuring students’ effectiveness by counting the number of papers they publish or patents they produce misses the point.”

The point, Urlaub says, is to investigate carefully while simultaneously acknowledging what we don’t know, citing what philosophers call Epistemic Humility. Knowledge, the investigators argue, is provisional, and always incomplete. Deeply held beliefs may require revision in light of new evidence. 

“No one’s mental view of the world is complete,” Celi says. “You need to create an environment in which people are comfortable acknowledging their biases.”

“How do we share concerns between language educators and others interested in AI?” Urlaub asks. “How do we identify and investigate the relationship between medical professionals and language educators interested in AI’s potential to aid in the elimination of gaps in communication between doctors and patients?” 

Language, in Gameiro’s estimation, is more than just a tool for communication. “It reflects culture, identity, and power dynamics,” he says. In situations where a patient might not be comfortable describing pain or discomfort because of the physician’s position as an authority, or because their culture demands yielding to those perceived as authority figures, misunderstandings can be dangerous. 

**Changing the conversation**

AI’s facility with language can help medical professionals navigate these areas more carefully, providing digital frameworks offering valuable cultural and linguistic contexts in which patient and practitioner can rely on data-driven, research-supported tools to improve dialogue. Institutions need to reconsider how they educate medical professionals and invite the communities they serve into the conversation, the team says. 

‘We need to ask ourselves what we truly want,” Celi says. “Why are we measuring what we’re measuring?” The biases we bring with us to these interactions — doctors, patients, their families, and their communities — remain barriers to improved care, Urlaub and Gameiro say.

“We want to connect people who think differently, and make AI work for everyone,” Gameiro continues. “Technology without purpose is just exclusion at scale.”

“Collaborations like these can allow for deep processing and better ideas,” Urlaub says.

Creating spaces where ideas about AI and health care can potentially become actions is a key element of the project. The Language/AI Incubator hosted its first colloquium at MIT in May, which was led by Mena Ramos, a physician and the co-founder and CEO of the [Global Ultrasound Institute](https://globalultrasoundinstitute.com/). 

The colloquium also featured presentations from Celi, as well as Alfred Spector, a visiting scholar in MIT’s [Department of Electrical Engineering and Computer Science](https://www.eecs.mit.edu/), and Douglas Jones, a senior staff member in the MIT Lincoln Laboratory’s Human Language Technology Group. A second Language/AI Incubator colloquium is planned for August.

Greater integration between the social and hard sciences can potentially increase the likelihood of developing viable solutions and reducing biases. Allowing for shifts in the ways patients and doctors view the relationship, while offering each shared ownership of the interaction, can help improve outcomes. Facilitating these conversations with AI may speed the integration of these perspectives. 

“Community advocates have a voice and should be included in these conversations,” Celi says. “AI and statistical modeling can’t collect all the data needed to treat all the people who need it.”

Community needs and improved educational opportunities and practices should be coupled with cross-disciplinary approaches to knowledge acquisition and transfer. The ways people see things are limited by their perceptions and other factors. “Whose language are we modeling?” Gameiro asks about building LLMs. “Which varieties of speech are being included or excluded?” Since meaning and intent can shift across those contexts, it’s important to remember these when designing AI tools. 

**“AI is our chance to rewrite the rules”**

While there’s lots of potential in the collaboration, there are serious challenges to overcome, including establishing and scaling the technological means to improve patient-provider communication with AI, extending opportunities for collaboration to marginalized and underserved communities, and reconsidering and revamping patient care. 

But the team isn’t daunted.

Celi believes there are opportunities to address the widening gap between people and practitioners while addressing gaps in health care. “Our intent is to reattach the string that’s been cut between society and science,” he says. “We can empower scientists and the public to investigate the world together while also acknowledging the limitations engendered in overcoming their biases.”

Gameiro is a passionate advocate for AI’s ability to change everything we know about medicine. “I’m a medical doctor, and I don’t think I’m being hyperbolic when I say I believe AI is our chance to rewrite the rules of what medicine can do and who we can reach,” he says.

“Education changes humans from objects to subjects,” Urlaub argues, describing the difference between disinterested observers and active and engaged participants in the new care model he hopes to build. “We need to better understand technology’s impact on the lines between these states of being.”

Celi, Gameiro, and Urlaub each advocate for MITHIC-like spaces across health care, places where innovation and collaboration are allowed to occur without the kinds of arbitrary benchmarks institutions have previously used to mark success.

“AI will transform all these sectors,” Urlaub believes. “MITHIC is a generous framework that allows us to embrace uncertainty with flexibility.”

“We want to employ our power to build community among disparate audiences while admitting we don’t have all the answers,” Celi says. “If we fail, it’s because we failed to dream big enough about how a reimagined world could look.”","Generative artificial intelligence is transforming the ways humans write, read, speak, think, empathize, and act within and across languages and cultures. In health care, gaps in communication between...","[""api"",""artificial-intelligence"",""natural-language-processing"",""networking"",""reinforcement-learning"",""sensors""]"
fb4bd7cc-14fc-4f65-974b-eb716a705d63,https://news.mit.edu/2025/ai-shapes-autonomous-underwater-gliders-0709,0b6ffd69-0eb9-4c71-96eb-bb41cf626cb0,AI shapes autonomous underwater “gliders”,https://news.mit.edu/2025/ai-shapes-autonomous-underwater-gliders-0709,"Marine scientists have long marveled at how animals like fish and seals swim so efficiently despite having different shapes. Their bodies are optimized for efficient, hydrodynamic aquatic navigation so they can exert minimal energy when traveling long distances.  
  
Autonomous vehicles can drift through the ocean in a similar way, collecting data about vast underwater environments. However, the shapes of these gliding machines are less diverse than what we find in marine life — go-to designs often resemble tubes or torpedoes, since they’re fairly hydrodynamic as well. Plus, testing new builds requires lots of real-world trial-and-error.  
  
Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the University of Wisconsin at Madison propose that AI could help us explore uncharted glider designs more conveniently. Their method uses machine learning to test different 3D designs in a physics simulator, then molds them into more hydrodynamic shapes. The resulting model can be fabricated via a 3D printer using significantly less energy than hand-made ones.  
  
The MIT scientists say that this design pipeline could create new, more efficient machines that help oceanographers measure water temperature and salt levels, gather more detailed insights about currents, and monitor the impacts of climate change. The team demonstrated this potential by producing two gliders roughly the size of a boogie board: a two-winged machine resembling an airplane, and a unique, four-winged object resembling a flat fish with four fins.

Peter Yichen Chen, MIT CSAIL postdoc and co-lead researcher on the project, notes that these designs are just a few of the novel shapes his team’s approach can generate. “We’ve developed a semi-automated process that can help us test unconventional designs that would be very taxing for humans to design,” he says. “This level of shape diversity hasn’t been explored previously, so most of these designs haven’t been tested in the real world.”

But how did AI come up with these ideas in the first place? First, the researchers found 3D models of over 20 conventional sea exploration shapes, such as submarines, whales, manta rays, and sharks. Then, they enclosed these models in “deformation cages” that map out different articulation points that the researchers pulled around to create new shapes.

The CSAIL-led team built a dataset of conventional and deformed shapes before simulating how they would perform at different “angles-of-attack” — the direction a vessel will tilt as it glides through the water. For example, a swimmer may want to dive at a -30 degree angle to retrieve an item from a pool.

These diverse shapes and angles of attack were then used as inputs for a neural network that essentially anticipates how efficiently a glider shape will perform at particular angles and optimizes it as needed.  
  
**Giving gliding robots a lift**

The team’s neural network simulates how a particular glider would react to underwater physics, aiming to capture how it moves forward and the force that drags against it. The goal: find the best lift-to-drag ratio, representing how much the glider is being held up compared to how much it’s being held back. The higher the ratio, the more efficiently the vehicle travels; the lower it is, the more the glider will slow down during its voyage.

Lift-to-drag ratios are key for flying planes: At takeoff, you want to maximize lift to ensure it can glide well against wind currents, and when landing, you need sufficient force to drag it to a full stop.

Niklas Hagemann, an MIT graduate student in architecture and CSAIL affiliate, notes that this ratio is just as useful if you want a similar gliding motion in the ocean.  
  
“Our pipeline modifies glider shapes to find the best lift-to-drag ratio, optimizing its performance underwater,” says Hagemann, who is also a co-lead author on a [paper](https://arxiv.org/abs/2505.00222) that was presented at the International Conference on Robotics and Automation in June. “You can then export the top-performing designs so they can be 3D-printed.”

**Going for a quick glide**  
  
While their AI pipeline seemed realistic, the researchers needed to ensure its predictions about glider performance were accurate by experimenting in more lifelike environments.

They first fabricated their two-wing design as a scaled-down vehicle resembling a paper airplane. This glider was taken to MIT’s Wright Brothers Wind Tunnel, an indoor space with fans that simulate wind flow. Placed at different angles, the glider’s predicted lift-to-drag ratio was only about 5 percent higher on average than the ones recorded in the wind experiments — a small difference between simulation and reality.  
  
A digital evaluation involving a visual, more complex physics simulator also supported the notion that the AI pipeline made fairly accurate predictions about how the gliders would move. It visualized how these machines would descend in 3D.  
  
To truly evaluate these gliders in the real world, though, the team needed to see how their devices would fare underwater. They printed two designs that performed the best at specific points-of-attack for this test: a jet-like device at 9 degrees and the four-wing vehicle at 30 degrees.

Both shapes were fabricated in a 3D printer as hollow shells with small holes that flood when fully submerged. This lightweight design makes the vehicle easier to handle outside of the water and requires less material to be fabricated. The researchers placed a tube-like device inside these shell coverings, which housed a range of hardware, including a pump to change the glider’s buoyancy, a mass shifter (a device that controls the machine’s angle-of-attack), and electronic components.  
  
Each design outperformed a handmade torpedo-shaped glider by moving more efficiently across a pool. With higher lift-to-drag ratios than their counterpart, both AI-driven machines exerted less energy, similar to the effortless ways marine animals navigate the oceans.  
  
As much as the project is an encouraging step forward for glider design, the researchers are looking to narrow the gap between simulation and real-world performance. They are also hoping to develop machines that can react to sudden changes in currents, making the gliders more adaptable to seas and oceans.  
  
Chen adds that the team is looking to explore new types of shapes, particularly thinner glider designs. They intend to make their framework faster, perhaps bolstering it with new features that enable more customization, maneuverability, or even the creation of miniature vehicles.  
  
Chen and Hagemann co-led research on this project with OpenAI researcher Pingchuan Ma SM ’23, PhD ’25. They authored the paper with Wei Wang, a University of Wisconsin at Madison assistant professor and recent CSAIL postdoc; John Romanishin ’12, SM ’18, PhD ’23; and two MIT professors and CSAIL members: lab director Daniela Rus and senior author Wojciech Matusik. Their work was supported, in part, by a Defense Advanced Research Projects Agency (DARPA) grant and the MIT-GIST Program.","Marine scientists have long marveled at how animals like fish and seals swim so efficiently despite having different shapes. Their bodies are optimized for efficient, hydrodynamic aquatic navigation s...","[""artificial-intelligence"",""hardware"",""machine-learning"",""python"",""react"",""reinforcement-learning""]"
63cf308a-1cc6-4933-87ee-b1ba7b6013c2,5e70830e99efe38fc86b473e5944fdd3487bb406ce841a1ff3b43d71bd1bcb04,0d379404-a15d-4c5b-8a86-5f48b82e93a4,Advancing Sustainability in a Climate of Silence,https://sloanreview.mit.edu/article/advancing-sustainability-in-a-climate-of-silence/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Winston-BizCase-1290x860-1.jpg)

Aron Vellekoop León/Ikon Images

It’s no secret that it’s been a rough year for corporate sustainability. Just a few years ago, large companies were eager to publicize bold goals around their climate change and equity efforts. Today, most U.S. companies are keeping their heads down and avoiding drawing attention to those topics. While I believe that most companies are still doing the work (to varying degrees), a chill in the space is palpable.

The reasons for the sustainability malaise are clear: aggressive social media campaigns against so-called woke companies, and direct pressure from the U.S. federal government to stop diversity and climate efforts. Aside from rare exceptions, talking about diversity has almost completely stopped. A Gravity Research study found that from 2024 to 2025, use of the term “DEI” in Fortune 100 company reports [dropped by 98%](https://www.forbes.com/sites/conormurray/2025/05/29/corporate-mentions-of-diversity-and-dei-dropped-72-in-2025-analysis-finds/).

The fundamental drivers of sustainability have not changed — we face enormous shared challenges. Time is running short and the stakes are too high, for both society and business, for companies to go backward. But it’s not easy to keep moving forward when those working on sustainability are scrambling to justify their work (after some years of strong support from C-suites). Anyone making the case for sustainability faces renewed skepticism and outright fear from their senior executives. And this is not just a problem for sustainability officers; any midlevel leader trying to, say, procure more sustainable materials or source clean energy is facing new questions.

How can we keep momentum going in the face of intense pressure to retreat? PepsiCo recently released “revised” sustainability goals, offering an example of a multinational trying to thread the needle: It is continuing or expanding important efforts on regenerative agriculture and renewable energy while pulling back on innovation around plastics and reusable containers.

The main answer to the momentum question, though — one that I’ve heard in sustainability events, LinkedIn debates, and many conversations with execs — seems to be a nearly universal conclusion that sustainability advocates in companies must “get back to the business case.” This is partly a logical reaction to what’s coming from the other side of the conference room table. From those C-suite execs who need to be convinced, I’m hearing things like, “We’ll still do sustainability, but only if there’s a clear ROI.” (That’s a direct quote.)

There’s nothing wrong with financial rigor — it is, of course, essential. But it’s odd to act as if we ever strayed from it. In 25 years in the field, I’ve never seen a time when sustainability managers did _not_ make the business case. Yes, nongovernmental organizations and activists make moral and existential arguments — as they should — and those deeper reasons will always creep into corporate discussions. But we sustainability advocates have always described the rationale for action largely in financial terms. We’ve often focused on long-term value creation over quick wins, but we’ve always talked about dollars.

So the concern I have is not about “getting back” to the business case per se; we never left it. The real challenge I see is that a narrowing of the discussion to a more basic ROI mindset may do some real harm to our cause. Here are three reasons why I think it’s a problem, followed by five ways that I urge companies to frame the challenge instead.

### Three Downsides to Excessive Focus on ROI

All the talk about return on investment reminds me of when companies were less bold about their sustainability ambitions. This simpler business case focuses on a few big buckets of value creation: slashing costs, reducing risk, driving innovation and revenues, and maybe building brand value (including, critically, employee attraction and retention).

I’ve spent decades advocating for sustainability in business, so I’m no stranger to this business case. But I’m worried that this narrower argument could hinder progress on our existential challenges and make (primarily U.S.) companies less competitive and resilient.

Here are my three fundamental concerns:

**There is a broader, stronger case than just ROI.** Tools like ROI, internal rate of return, and net present value are useful, but they’re also flawed; they lean heavily on short-term and easily measured outcomes.

The logic for larger action on sustainability is broader than just what those kinds of metrics tell us. As I argued in an earlier column, there are three main buckets for action on our great environmental and social challenges: Businesses and their leaders [should act because we need to, have to, and want to](https://sloanreview.mit.edu/article/why-we-need-to-have-to-want-to-act-on-climate/).

We “need to” because climate change, biodiversity loss, and rising inequality are existential threats that risk destabilizing society. Companies still “have to” take action to respond to both increased regulatory demands (like disclosure laws in the European Union and California) and rising stakeholder expectations, especially from younger consumers and employees around the world who are demanding more from companies. And we “want to” act because sustainable strategies, done well, can create value in core ways (cost, risk, revenue, and brand).

When people say “get back to the business case,” they usually mean only this third bucket. But all three parts of the logic for action matter, and they create a more complete, urgent rationale for bigger and bolder actions.

**A narrow ROI case consistently undervalues sustainability.** Most investment return tools are blunt instruments. We measure cash into a project and direct value out (such as reduced costs or increased sales). But these tools struggle to include hard-to-measure benefits that sustainability is chock-full of, such as resilience, talent attraction, employee engagement, and reputational risk. How would you price the risk of fossil fuel volatility that you can avoid through decarbonization? Or measure the risk reduction from avoiding human rights issues, or the value of higher employee engagement resulting from increased wages?

These intangibles create very tangible value. For instance, while paying some of the highest wages in retail, [Costco has consistently outperformed](https://www.economist.com/business/2024/02/15/why-costco-is-so-loved) its peers on a range of key metrics, including employee turnover — which is just 8% per year versus an industry average of 60%. And Costco is one of the very few large U.S. companies that has [publicly maintained its commitment to diversity and inclusion](https://finance.yahoo.com/news/costco-thriving-retailers-target-roll-210004431.html?guccounter=1).

Companies have also consistently overestimated the cost of action. It’s hard to grasp things moving at exponential speed, like the vast reduction in the cost of clean technologies. Solar and wind are the cheapest energy sources in most regions, and electric vehicles, over their lifetimes, cost less to operate. What other decarbonization tools and technologies are following this curve? (AI-driven efficiencies in grid, buildings, and transportation are good bets.) What savings might your business be missing if you choose not to invest in climate tech?

The societal costs of these blinders can be substantial. When you overestimate the cost of action and underestimate the value, you move more slowly and incrementally. It may be progress, but it’s not nearly enough to meet the moment or what the science demands.

**The simple business case reinforces a false story.** The most persistent myth I’ve battled in my work is the assumption that everything in sustainability _always_ creates a drag on earnings. This view is taken as gospel, even when plenty of evidence says otherwise. For a large majority of executives, sustainability either equals philanthropy or is seen as a Trojan horse for a liberal agenda.

So when even well-intentioned people say, “We must get back to the business case,” it carries a bit of a self-defeating message — that is, that all the sustainability work that’s been done so far was, as critics said, _not_ good for business (or was just a reaction to pushy stakeholders). It makes it sound like sustainability was never a strategic choice or a path to long-term value creation.

### A Better Way Forward

What should leaders do? I have five recommendations.

**If you’re under pressure to justify sustainability, then, sure, start with the narrower traditional ROI.** But don’t stop there. Broaden the case you’re making, and bring in longer time horizons and categories of value that are harder to quantify.

**Remind the senior decision makers of the “have to” and “need to” logic.** The problems the world is facing are real and serious. They’re not abstractions, and they affect business directly; extreme weather can seriously disrupt supply chains at enormous expense. Laws are forcing more transparency, and stakeholders are watching. B2B customers still care about environmental footprint, and employees want to work for companies that stand for something.

**Resist retreating.** Pulling back or hiding can feel safer in the short run, but it can cost you in boycotts, lost business, and lower engagement. Look at those law firms that capitulated to White House demands to drop certain clients and make other concessions. They’re [losing some major customers](https://www.msn.com/en-us/politics/government/the-law-firms-that-appeased-trump-and-angered-their-clients/ar-AA1FTFLZ) (and the braver firms are gaining them). I’ve talked to friends and family members at these firms. The morale among the younger lawyers is not high.

**Ask executives to think about the broader operating environment.** The U.S. is big, but the world is much bigger, and outside the U.S., the clean economy and sustainability agendas are still advancing. China is barreling forward into the clean tech future — more than [half of the country’s new car sales are electric or hybrid](https://www.cnbc.com/2024/08/09/more-than-half-of-new-cars-sold-in-china-are-now-electric-or-hybrid.html). No matter what the current U.S. administration says, the world needs to decarbonize to ensure a thriving future. Ask yourself, what skills do companies need to keep up? And if society is getting more diverse, what practices and policies will help attract the broadest array of talent? Multinationals can’t ignore where the markets and society are going.

**Finally, help leaders connect with purpose.** There are moments where you can go beyond the numbers and reach executives as people, calling some assumptions into question. Our short-term profit obsession has greatly contributed to the mess we’re in. With those goals at the center of our collective story, we get what we seek — constant growth, but at the cost of a livable planet where everyone can thrive. Talk about meaning in business and in leaders’ lives. Ask them, “What legacy are you leaving? What do your kids think about your work?” The executives I’ve seen transition into having a real understanding of sustainability have most often cited conversations with their own children as a turning point.

There has rarely, if ever, been a time like this in American business. The fear of missteps or getting the wrong kind of attention is real. But the challenges we face and the pressures to do something about them haven’t changed — the urgency has only grown. This is a time for courage. We need the audacity to do the right thing for the world, and also to position companies for long-term resilience and relevance. Help your company resist the temptation to scale back its sustainability and diversity efforts. Make the full case with courage, clarity, and conviction.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Winston-BizCase-1290x860-1.jpg),"[""api"",""aws"",""docker"",""python"",""react"",""tensorflow""]"
b232610b-b36c-4979-9404-3e6b26c5a7fa,01acc24d835768008915d6d7dfa4eb9273b3092146ed2e8faa287008552fee9b,0d379404-a15d-4c5b-8a86-5f48b82e93a4,How Leaders Help Teams Manage Stress,https://sloanreview.mit.edu/article/how-leaders-help-teams-manage-stress/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Morrison-1290x860-1.jpg)

Carolyn Geason-Beissel/MIT SMR | Getty Images

Stress has become a defining feature of modern organizational life. When channeled constructively, stress can act as a powerful motivator — fueling productivity, innovation, and change. Conversely, unmanaged stress can breed dysfunction, lower morale, and lasting psychological harm. Yet most organizations still lack systematic approaches for managing stress across teams.

To address this gap, we launched a multiyear study of leadership and employee behavior that focused on organizational politics and psychological safety. Our research combined structured interviews, case analyses, practitioner insights, and a survey of more than 150 senior business leaders across Europe, the Asia-Pacific region, the Middle East, and the United States. Our findings are both encouraging and sobering. We learned that most workplace stress is episodic and manageable with proper support, but a troubling pattern emerged: Leaders — tasked with modeling resilience — often amplify stress instead. Rather than easing pressure, their behaviors frequently intensify it, undermining team cohesion and performance.

A striking example comes from a professional services firm where employees initially coped well with typical pressures like divorces, car repairs, deadlines, and office politics. That balance shifted with the arrival of a new unit director. Aiming to boost productivity, he introduced few procedural changes but brought a leadership style that some employees described as “highly toxic.” His controlling, confrontational approach eroded trust. Personal stress became harder to compartmentalize as the work environment grew volatile. Absenteeism rose, engagement fell, and some employees left meetings in tears.

Instead of adjusting his leadership style, the director doubled down, likely influenced by his own burnout and assumptions of team underperformance. Public reprimands became routine, and within 18 months, 75% of the team had resigned. Many sought counseling to recover from the emotional toll. The CEO eventually stepped in and terminated the director — a move widely seen as a necessary reset.

Some of you have encountered a leader like this, or perhaps you’ve seen [similar tendencies in yourself](https://sloanreview.mit.edu/article/are-you-being-a-nice-jerk/) at times. The good news is that leadership doesn’t have to amplify stress. With the right approach, leaders can redirect both the pressure they generate and the strain their teams carry, turning stress into a source of momentum rather than burnout.

### How Stress Affects Engagement

Many leaders perceive stress as a personal issue that employees should manage on their own. Some interpret an inability to handle it as a reflection of individual weakness. Too often, leaders also believe that employees can and should compartmentalize stresses related to their personal lives. These leaders focus narrowly on work performance outcomes and metrics.

But that is short-sighted, given that emotional engagement is foundational to employees’ capacity to create long-term value. When employees are emotionally engaged, they don’t just complete tasks; they invest their full selves. They bring energy, commitment, and creativity to their roles. These individuals consistently go the extra mile, not out of obligation but from a genuine desire to contribute meaningfully. As one quality control supervisor at a global food company shared, “Sometimes when I am at work, I get absorbed in my job. I get laser focused on the task at hand and lose all track of time. It’s in those moments I can’t imagine a happier place to be than at work.”

However, emotional engagement is fragile — and stress is one of its most powerful influences. Well-managed stress can heighten employees’ focus and energize them, deepening their connection to the work. But chronic or overwhelming stress depletes people’s emotional reserves. Our research identified three key factors that determine how stress shapes emotional engagement:

**Intensity.** The degree to which stress affects engagement depends on its severity as experienced by an employee. Leaders often misjudge the full range of stressors employees face, fixating on workplace demands while overlooking personal pressures like financial worries, interpersonal conflicts, health issues, or caregiving responsibilities.

**Perspective.** Personality, goals, and values all influence whether stress is experienced as motivating or paralyzing. For instance, the pressure of long hours may feel worthwhile to someone chasing a promotion but burdensome to someone juggling certain family obligations. Organizational decisions — such as closing down an office or disciplining a toxic colleague — can also spark divergent reactions, relieving some people while unsettling others.

**Capacity.** Some people thrive under pressure, using it to fuel innovation and performance. Others become overwhelmed, leading to burnout. Life experience, personality, genetics, and support systems all play a role in determining stress resilience.

As we’ll explore later, leaders can shape, but not fully control, each of these factors.

### Fight Versus Flight at Work

In a healthy environment, most employees typically operate within a band of high functionality — a zone marked by emotional engagement, cognitive focus, and strong alignment with responsibilities. Within this band, performance is optimized: Employees are attentive, energized, and productively immersed in their work.

As shown in the graphic below, emotional engagement fluctuates over time, influenced by both individual experiences and broader workplace dynamics. A team member might feel disconnected after a difficult interaction but later become fully engaged when tackling a meaningful project.

High stress — whether driven by major challenges or the cumulative weight of minor pressures — intensifies both ends of the engagement spectrum.

### Emotional Engagement and Employee Functionality

During times of high stress, many employees will mentally switch into fight-or-flight mode — either obsessing about performance (high engagement) or shutting down (low engagement). Both responses are coping mechanisms. The band of high functionality or effectiveness, for individuals and teams, occurs when employees operate in the mid-range: They don’t expend a dangerously high or low amount of emotion on work. Many employees will cycle through temporary fight or freeze responses, perhaps several times a year, self-correcting these brief periods of strain without lasting harm. But when stress becomes chronic, those reactions can deepen into trauma.

![Emotional Engagement and Employee Functionality chart](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Morrison_Stress_Fig.png)  

**Fight mentality.** During overwhelming stress, some employees double down on performance as a coping mechanism. They become intensely work-focused, often at the expense of their personal well-being. Obsessive tendencies emerge: They fixate not only on their own output but also on the perceived shortcomings of colleagues. Traits such as impatience, time urgency, perfectionism, and hypercompetitiveness dominate their behavior. Sleep is often disrupted as their minds remain locked on work-related challenges. As one senior marketing executive shared, “I am tired, but it’s as if I can’t switch my thinking off. I spend my night in bed trying to see ways I can get my colleagues to understand that they are wrong or lazy. It makes me so angry.”

Neurologically, elevated stress triggers the release of cortisol — a hormone linked to emotional dysregulation, reduced impulse control, and heightened aggression. This biological shift often manifests in a “fight” response: interpersonal tension, combative communication, and a general breakdown in team cohesion. Unfortunately, this behavior tends to alienate colleagues, prompting them to withdraw or disengage. The result is a negative feedback loop for the team: Conflict escalates, collaboration declines, and overall stress intensifies.

**Freeze mentality.** Other employees adopt a “freeze” response to stress — withdrawing from meetings, staying silent in discussions, and disconnecting from workplace interactions to reduce their own mental pressure. While this disengagement may serve as a short-term coping mechanism, leaders and colleagues often misread it as apathy, further isolating the employee and intensifying their stress.

Clinical studies link chronic stress to disruptions in serotonin and other mood-related neurotransmitters that can trigger sullen or withdrawn behavior. As these individuals become harder to reach emotionally and socially, colleagues may stop engaging, deepening the employee’s feelings of hopelessness. Like the fight response, freezing can become a self-reinforcing cycle — eroding collaboration, diminishing team morale, and fueling organizational dysfunction.

### The Traumatized Workplace

It’s normal for employees to cycle through temporary fight or freeze responses, perhaps several times a year. Most people self-correct during these brief periods of strain, without lasting harm. But when stress becomes chronic, those fleeting reactions can harden into something deeper and more damaging: trauma. Sustained high levels of perceived stress overwhelm people’s emotional resilience, degrade their coping mechanisms, and accelerate a downward spiral.

Over time, chronic stress corrodes focus, collaboration, and trust — leading to visible declines in both individual and team performance. These patterns are rarely random. When they surface across a unit, they almost always can be traced back to a dysfunctional leader: someone whose actions, inactions, or leadership style intensify rather than alleviate pressure. Employees caught in such dynamics often feel powerless and isolated. When their calls for help go unanswered, the result is predictable: burnout, emotional withdrawal, and disengagement. Even when they remain on the job, they are no longer fully present. Without support, their well-being deteriorates and the emotional toxicity can quietly ripple outward, undermining the broader culture.

In virtually every organization we studied, we found some lingering trauma rooted in the behavior of a current or former leader, often stretching back years — even decades. While it’s often said that time heals all wounds, healing in the workplace usually comes only when the wounded leave, taking their scars with them to their next employer. At a minimum, the stories remain behind, continuing to circulate and exerting their own quiet but corrosive force.

### Leading Better Through the Stress

The effects of stress — and trauma, in particular — are deeply personal. Yet employees rarely communicate these struggles directly to their managers. Leaders may notice signs of fight-or-freeze behaviors but often fail to connect them to their underlying causes. Ironically, leaders frequently overlook their own contributions to the problem, blinded by pride, indifference, or their own unmanaged stress. To take the first step toward meaningful change, leaders must acknowledge this dynamic.

Our research yielded three key recommendations to help teams manage stress better.

#### 1\. Look in the Mirror: Leaders Set the Tone

Some leaders resemble storm chasers pursuing tornadoes — not just unfazed by the danger but drawn to the thrill. Others delay difficult strategic decisions, waiting for clarity that emerges only once the crisis is in full swing. While crises may sometimes be inevitable, leaders must avoid creating them.

Whether through emotional detachment or a lack of awareness, leaders may also be blind to the daily pressures their employees face. When performance slips, the instinct to increase pressure can backfire, amplifying stress instead of resolving it.

A [toxic leader](https://sloanreview.mit.edu/article/how-to-fix-a-toxic-culture/) erodes trust, drives anxiety, and crushes morale. We’ve worked with organizations where entire teams were left reeling from a single destructive leader. As one newly appointed CEO put it: “I inherited a traumatized company. My predecessor ruled through fear for years, leaving only the walking wounded. The signs of burnout and abuse were everywhere. Turning that around was the hardest challenge of my leadership career.”

Leaders cannot support others without first managing their own well-being, though. While stress is part of leadership, many respond with counterproductive coping strategies — aggression, control, emotional withdrawal — that destabilize the team and make it harder for people to seek the leader’s support.

Regulation starts with awareness: Avoiding reactive interactions and setting boundaries around draining relationships can help leaders preserve energy and increase their own effectiveness. One CEO told us, “Sometimes leaders need to go slow to get there faster.” Consider engaging in these practices to manage your own stress level:

*   **Building self-awareness.** Identify your stress triggers, and recognize how your reactions affect others.
*   **Adopting stress-reduction habits.** Mindfulness, exercise, and reflection can keep emotions in check and prevent stress contagion.
*   **Seeking support.** Take time to engage with peers, mentors, or coaches to gain perspective and reduce your isolation.

Self-regulation isn’t just a personal skill: It helps create a healthier, higher-functioning workplace.

#### 2\. Help Others to ‘Expand the Band’

No leader can optimize all of the stresses affecting an organization. There are simply too many variables. Supporting employees in _expanding the band_ of functionality means building their ability to navigate the stressors they will encounter.

This doesn’t eliminate stress, but it helps people remain emotionally connected and engaged, even as pressures mount. To that end, leaders should help employees do the following:

*   **Strengthen coping skills.** While these skills evolve over years and are influenced by personality and genetics, coaching can be a powerful tool. Our research shows strong links between coaching and improved stress management. Effective personal coaches help employees contextualize stress, prioritize effectively, and regain a sense of control.
*   **Reshape perceptions of stress.** Leaders can shape how employees perceive stress. Fairness, precise expectations, and clear explanations about business decisions all influence how stress is processed. When leaders communicate clearly, set achievable goals, and foster agency, they reduce unnecessary stress and improve team function.
*   **Address structural sources of stress.** Tight deadlines and rigid systems heighten pressure and compromise decision-making. By managing systemic constraints and buffering demands, leaders can help prevent burnout and create conditions for sustained performance.

#### 3\. Prioritize Emotional Integrity and Team Resilience

Helping employees manage stress goes beyond creating a “safe space.” It requires leaders to establish a climate where emotional integrity is respected and team members are empowered to support one another. In such environments, leaders don’t just encourage people to speak up: People trust that their emotional experiences will be taken seriously and responded to constructively.

One head of logistics at a global engineering firm recalled how his manager’s empathy during a personal crisis left a lasting impression: “After all my boss did for me when my dad was dying of cancer, I will never leave this company. He didn’t change anything, but he was there for me.” That kind of emotional steadiness can anchor people when everything else feels unstable.

Similarly, a finance manager at a pharmaceutical company shared, “Once my boss actually listened to me, I realized I needed to talk to my team. It renewed my confidence to open up and explore ways to work more productively together. Simply being listened to helped me organize my emotions.”

Leaders don’t need to “solve” employee stress — they can validate it and foster conditions where peers provide support and collaborate on solutions. Since leaders can’t be everywhere, and hierarchies limit openness, teams should develop microclimates of trust. When team members feel responsible for each other’s well-being, it strengthens the organization’s social fabric.

Stress doesn’t have to corrode culture — it can forge it. When people utilize empathy, self-awareness, and intentional leadership, stress becomes fuel for transformation. It’s how turbulence sparks vitality and how teams evolve from merely productive to deeply connected and fully engaged.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Morrison-1290x860-1.jpg),"[""artificial-intelligence"",""helm"",""networking"",""react"",""rust"",""spark""]"
660d5f8b-db2a-4b7a-92c1-be1b5ae8b026,92dee0e402e49bf8797668a1fc785aeb0a15191cc934eefbef737596261574aa,0d379404-a15d-4c5b-8a86-5f48b82e93a4,Delegate to Build Stronger Teams,https://sloanreview.mit.edu/article/delegate-to-build-stronger-teams/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Laker-delegation-1290x860-1.jpg)

Carolyn Geason-Beissel/MIT SMR | Getty Images

As the pace of work has accelerated and the line between “on” and “off” has blurred, you’ve probably felt it: There just isn’t enough time. With endless notifications, shifting priorities, and expanding responsibilities, trying to do everything yourself is no longer realistic. And that’s actually good news, because delegation isn’t just a necessity for freeing up your time. Done well, it can help you build your team’s capabilities, develop emerging leaders faster, and create a culture of trust and autonomy.

Delegation is still widely misunderstood and often treated as a last resort when a leader is stretched too thin. Or, worse, it’s seen as risky: What if the work isn’t done well or on time? So, many leaders hold on. They tweak slide decks late at night, rewrite emails their team drafted, or stay involved in every detail of a project, long past the point of adding value.

When you don’t delegate, you don’t just burn yourself out; you also block others from growing. You send a message (often without realizing it) that trust must be earned, that autonomy is dangerous, and that leadership means control.

Delegation isn’t a transaction but a development tool. It’s not about dropping tasks onto someone else; it’s about building their capacity and confidence in the process. Start with the person, not the task. Ask yourself: What is this individual ready for? What would challenge them without overwhelming them? How could this support their goals or learning? When you frame delegation as an investment in someone’s growth rather than a way to lighten your own load, your approach becomes more strategic and more effective.

Clarity is critical. Vague handoffs like “Can you run with this?” invite confusion and rework. Instead, define the “why” behind the task, outline what success looks like, and clarify where the person has decision-making freedom: “Here’s what good looks like, here are the nonnegotiables, and here’s where your judgment comes in.” That clarity should extend to timelines, communication channels, and escalation paths. What should be shared in real time? What’s expected by the end of the week? What qualifies as something to check in on versus something to run with? When these details are unclear, even strong performers will second-guess themselves or fall into analysis paralysis.

Even your most capable team members may hesitate when stepping into unfamiliar territory. But they might not tell you they’re unsure — especially if the task seems high-profile. That’s why check-ins matter. Use them to coach, not control. Create a space where people can test ideas, raise concerns, and build confidence.

You might start by delegating a project outline to a newer team member while keeping final delivery with a more senior colleague. Over time, as confidence builds, so does independence. Progressively expanding the scope of responsibility is one of the most effective ways to build leadership readiness without overwhelming people or compromising quality.

In remote or hybrid teams, this becomes even more important. When a leader lacks visibility into their team’s day-to-day activities, it’s easy for team members to feel abandoned when given a new responsibility. Stay present enough to offer needed support without pulling the task back into your orbit. Your role is to guide, not to reinsert yourself at the first sign of discomfort.

This is also where role-modeling comes in. If you want your direct reports to delegate, they need to see you doing it thoughtfully. Share your decision-making logic: why you’re delegating a task, how you chose the person you delegated it to, and what you expect from the process. This kind of transparency demystifies delegation and encourages others to adopt it as a leadership tool, not just a time-saver.

Recognition is part of the equation too. When things go well, make sure your team gets the credit — publicly and specifically. And when something goes sideways, treat it as a learning opportunity, not proof that delegation doesn’t work. Every misstep is a chance to coach, clarify, and calibrate, not to retreat.

Over time, your willingness to delegate will become a signal that risk is acceptable, learning is encouraged, and performance is not just about individual output but collective capability. Teams that operate this way build resilience, not dependency — which is exactly what high-performing organizations need right now.

Finally, the goal of management isn’t to be indispensable. In fact, if your team can thrive without you micromanaging every outcome, that’s not a weakness — it’s a mark of strong leadership. The most effective leaders today aren’t the ones who do it all. They’re the ones who build others who can.

Don’t just look for ways to work faster. Purposefully look for opportunities to let go. When you delegate with clarity, care, and consistency, you don’t just ease your burden — you unlock your team’s potential.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Laker-delegation-1290x860-1.jpg),"[""artificial-intelligence"",""helm"",""networking"",""performance"",""rust"",""tensorflow""]"
2021f46e-9c89-4a8f-b77f-e990d212cac9,0e72c606d0e4d94b8e7a5aacf82821b3d910ca695056c2fd2cd6a2a21ced75cc,0d379404-a15d-4c5b-8a86-5f48b82e93a4,How to Scale GenAI in the Workplace,https://sloanreview.mit.edu/article/how-to-scale-genai-in-the-workplace/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Wade-1290x860-1.jpg)

Carolyn Geason-Beissel/MIT SMR | Getty Images

As generative AI’s evolution continues, the next challenge for leaders is clear: making GenAI scale and deliver measurable value across their organizations.[1](#ref1)

As companies move from experimentation to enterprisewide adoption, many struggle not with the tools themselves but with the organizational transformation required to integrate them meaningfully into people’s daily work. Tools will keep evolving: It is the human side of the equation that determines whether GenAI initiatives truly succeed.

We studied one of the largest real-world generative AI deployments to date — at multinational pharmaceutical company Novo Nordisk. Its experience shows that success hinges not just on infrastructure but on how people think, adapt, and collaborate with AI. One critical lesson: While GenAI adoption and broader digital transformations have common roots, generative AI is uniquely disruptive, reshaping the nature of work itself in unprecedented ways.

Like many organizations, Novo Nordisk began with a familiar expectation: that GenAI would primarily drive productivity.[2](#ref2) Guided by the leadership principle “time is the ultimate currency” and a campaign dubbed “Make Your Time Count,” the company launched its enterprisewide rollout of Microsoft’s Copilot GenAI tool in early 2024, with the goal of saving time and improving efficiency. And in many ways, the company hit that goal.

Each employee saved 2.17 hours per week, on average, once they began using the tool. But something unexpected also happened: Those hours weren’t what employees valued most. Employee satisfaction with Copilot was three times more strongly correlated with perceived improvements in _work quality_ than with time saved. Employees reported quality enhancements in content summarization, content creation, and ideation. Interestingly, many employees reinvested the time they saved into people interactions, strategic planning, and creative work. As one put it, “I can spend more time and energy dedicated to strategizing and planning the rollout of my project.”

This insight challenges a central assumption of many GenAI rollouts: that the main value of the technology lies in raw efficiency. In practice, the promise of generative AI is broader and more human-centered.

Novo Nordisk’s rollout experience, growing from a few hundred Copilot users in January 2024 to 20,000 in February 2025, offers important lessons for leaders who are grappling with scaling GenAI. Through surveys of over 3,000 employees, internal analytics, and front-line interviews, we uncovered both employee dynamics and field-tested leadership approaches for driving meaningful adoption at scale.

### Scaling GenAI: It’s Not Plug-and-Play

Scaling generative AI isn’t just a technical challenge — it’s a change management marathon. The real work lies in supporting employees as they experiment, struggle, and eventually find their stride with these new tools.

At Novo Nordisk, Copilot adoption was nonlinear, unfolding in three phases. First came a surge: After about a month, 23% of people were frequent users and 74% were moderate users. Then came an interest dip, where 15% of the early-adopter group became inactive after three to four months, and their average time saved decreased from 2.29 to 2.14 hours per week. One user shared, “I have not really found out what to use it for. I tried a bit in the beginning, with limited success, and have not really tried since.” A similar pattern appeared in app usage: Productivity and quality gains dipped after users expanded their use of Copilot from one or two apps (such as Word and Excel) to four or five, but it then rebounded at six or more.

This midcycle dip is typical of GenAI adoption. People’s initial excitement gives way to frustration as easy wins are exhausted and integration challenges mount. Left unchecked, this dip can calcify into abandonment. Our research revealed that it takes only a few disappointments to destroy people’s enthusiasm for the technology; in other words, a user may give up on using Copilot after a few unsuccessful tries. However, employees who persist beyond this dip often report substantial performance gains, likely due to accumulated learning effects. This is a pivotal moment for targeted training interventions.

To counter the midcycle dip, Novo Nordisk implemented a suite of enablement strategies, including:

*   Targeted training interventions timed around key adoption phases.
*   License reallocation and waiting lists to reinvigorate interest.
*   A network of GenAI champions to provide contextual guidance and sustain adoption momentum.
*   Targeted microcommunications, such as tip-focused newsletters, to address user-reported challenges.
*   Ongoing feedback loops, such as pulse surveys, usage dashboards, and competitor benchmark tracking, to evolve support as user needs changed.

As the company’s experience highlights, generative AI requires sustained training and support. GenAI effectiveness is about training people, not just AI models. Leaders should foster an ecosystem where employees feel supported, informed, and inspired to push the technology’s capabilities further.[3](#ref3)

### Tailor GenAI Enablement by Business Function

Different business functions require different types of support, however. Recognizing how people across roles and mindsets engage with generative AI is key to driving meaningful adoption.

At Novo Nordisk, Copilot’s impact varied significantly across functions, prompting a shift from uniform deployment to targeted enablement. Analysis comparing time savings and quality improvements across corporate, commercial, manufacturing, and research areas revealed sharp disparities.

Corporate and commercial teams were among the highest-ranking in terms of improvements in productivity and quality of work with Copilot. On the flip side, departments like Research, Data & AI, and Clinical Development, while still deriving benefits, reported smaller gains in both time savings and quality improvements. STEM employees, accustomed to deterministic systems with consistent outputs, often struggled with GenAI’s probabilistic nature. Because generative AI operates on models that produce variable responses, its outputs can clash with precision-driven workflows that demand reliability. AI hallucinations (incorrect or nonsensical outputs) further complicated tool integration into research-oriented tasks, where accuracy and reliability are paramount. These hallucinations manifested in various ways, including fabrications, factual inaccuracies, logic or reasoning errors, mathematical mistakes, and making choices based on irrelevant patterns.[4](#ref4)

This variability undermined people’s trust in the system and posed significant barriers to adoption. As one researcher put it, “I don’t see how to apply it in my type of work.” In contrast, a sales user described it as “a game changer for me in many facets of my job.” Adapting to GenAI would require a mindset shift among the STEM employees: They would have to learn to navigate and manage such tools’ inherent unpredictability.

To facilitate the process, Novo Nordisk pivoted from a uniform rollout to tailored enablement. It launched function-specific onboarding, created use-case playbooks and learning libraries aligned to job roles, and worked with Microsoft to customize Copilot features for different teams. This flexibility ensured that employees in both creative and precision-driven roles could find meaningful applications aligned with their workflows.

### Surprise Champions: More Experienced Employees

Contrary to some myths about digital natives, the data showed that Novo Nordisk’s senior employees tended to use generative AI more effectively than their younger peers. Survey results indicated that experienced workers outperformed junior colleagues in both productivity gains and work quality improvements. Why? These more senior employees’ deep understanding of workflows allowed them to quickly identify places where tools like Copilot could add value. Additionally, they were better equipped to assess AI-generated outputs and integrate them into complex tasks with greater precision.

Younger employees, in contrast, often lacked the context to spot high-impact opportunities. One junior staffer said, “I don’t know enough actual use cases; what can I use it for?” Another one noted, “I have not had a specific use case where I could clearly see the advantages of using/exploring Copilot.”

The lesson: Rather than assuming that younger workers will lead the GenAI charge, organizations should empower experienced employees to act as amplifiers.

After this insight upended Novo Nordisk’s leaders’ assumptions, they established a cross-functional champion network composed largely of experienced staff members to lead peer demo sessions, deliver role-relevant training, and share practical examples tailored to specific job contexts. Meanwhile, internal corporate social media communities like Viva Engage (a social collaboration tool integrated into Microsoft Teams) enabled knowledge-sharing between senior and junior employees, furthering adoption.

By investing in senior employees as adoption champions, leaders can tap into these people’s contextual expertise to drive meaningful GenAI use by tailoring training to workflows and seniority, and ensuring that junior staff members receive clear, accessible use cases to build their confidence. By aligning enablement with experience, organizations can unlock GenAI’s potential across all levels.

Generative AI performance isn’t about tech savviness — it’s about contextual fluency, confidence, and the human ability to integrate new tools into nuanced workflows.

### Overcoming Cultural Resistance and AI Shaming

Not everyone at Novo Nordisk welcomed Copilot. Cultural resistance and so-called AI shaming posed significant hurdles, with some employees viewing GenAI as unethical or akin to cheating. One user said, “I find Copilot to be ethically dubious: extremely high energy consumption, built on shameless practices of privacy invasion and rights violations.”

Others feared disrupting routines, making mistakes, or facing scrutiny for AI-generated outputs. One user said, “I’m afraid that what I do with Copilot is wrong or I will do a faulty job.” Such attitudes resulted in employee reluctance to integrate the technology into their workflows. Concerns about output ownership and workflow changes fueled further resistance to GenAI.

Such cultural resistance, subtle but real, can stall adoption even in tech-heavy organizations. Novo Nordisk tackled the challenge with a multifaceted strategy, focusing on transparency and trust. The company rolled out ethical use guidelines, clarified expectations around output ownership and disclosure, and launched the “Spend Time to Save Time” campaign to reframe GenAI as a strategic enabler, not a shortcut.

Mark Navas, the Novo Nordisk executive in charge of the Copilot rollout, reinforced this message: “Copilot is about empowering our people to do better work, not cutting corners.” Regular feedback loops, including surveys and usage analytics, allowed the company to monitor resistance and adapt support. Champion-led demos normalized adoption by showcasing real-world examples of GenAI’s successful use. Safe spaces like the Viva Engage platform allowed employees to ask questions, share concerns, and build confidence without fear of judgment.

Leaders scaling GenAI can adopt these strategies, as Novo Nordisk did, to mitigate cultural resistance:

*   **Clarifying ethical use.** Develop and communicate guidelines on AI use, ownership, and disclosure. Ensure that employees understand how to integrate GenAI transparently.
*   **Normalizing GenAI use through champions.** Deploy experienced employees to demonstrate practical, ethical applications, which makes GenAI more relatable and credible.
*   **Fostering safe spaces.** Build internal communities for peer support, where employees can voice concerns and share successes without judgment.
*   **Addressing trust issues proactively.** Train employees on data privacy, environmental impact, and responsible use to build trust and counter ethical objections.
*   **Reframing AI’s role.**Position GenAI as a tool for enhancing work quality, not replacing human effort, through consistent leadership messaging.

Addressing cultural barriers head-on, Novo Nordisk took these approaches to transform resistance into engagement and pave the way for sustained adoption.

#### Six Key Levers to Scale Enterprise GenAI

These six key levers for scaling adoption effectively are drawn from Novo Nordisk’s generative AI rollout and the academic authors’ experience supporting organizations in GenAI strategy. The levers work best when used in combination and when leaders see generative AI rollouts as organizational transformations rather than just technology deployments.

Lever

Description

Example Tactics

Key Challenges Addressed

Layered training and onboarding

Role- and function-specific training led by peers and experts

• Senior-led champion sessions  
• Contextual onboarding  
• Microlearning modules  
• SharePoint as knowledge repository

Initial learning curve; function-specific needs

Champion networks

Embedded experts providing domain-specific support

• Champions with workflow and AI fluency  
• Function-specific use case libraries  
• Peer shadowing sessions

Resistance; lack of relatable examples

Internal communities of practice

Peer forums to support learning, share use cases, and build trust

• Q&A channels on online platforms  
• Peer-led demo sessions  
• Weekly tip-sharing posts

Cultural resistance; AI shaming

Communication and framing

Messaging to shift perceptions and set expectations

• Reframing campaigns  
• Myth-busting newsletters  
• Usage leaderboards

Misconceptions; ethical concerns

Targeted guidance

Contextualized use cases and ethical guidelines

• Function-specific playbooks  
• Prompt cheat sheets  
• Ethical-use posters

Inconsistent adoption; confusion over output ownership

Adaptive governance

Feedback loops to monitor and adjust support

• License reallocation  
• Real-time usage dashboards  
• Pulse surveys

Adoption dips; inactive users

Vendor and ecosystem collaboration

Partnerships to optimize tools and share best practices

• Vendor collaboration for feature customization  
• Industry AI consortia participation  
• Codeveloped plug-ins

Tool limitations; industry alignment

  
![Six Key Levers to Scale Enterprise GenAI](https://sloanreview.mit.edu/wp-content/uploads/2025/07/wade-0725-mobile-fallback.jpg)

### People Are the Platform

Novo Nordisk plans to further expand its Copilot rollout, from approximately 20,000 to 37,000 employees in 2025. (The company has approximately 75,000 workers globally.) According to chief digital and information officer (and coauthor) Anders Romare, the company’s future with generative AI will depend on moving beyond initial excitement to building systems that support ongoing learning, trust, and integration into real workflows.

As the company continues its rollout, one lesson has become increasingly clear: GenAI success is less about the tools themselves and more about the people who use them, and how they adapt, collaborate, and take ownership of change.

That success isn’t driven by automation alone but by people willing to rethink how they work, support one another, and build confidence in new ways of working alongside AI. Champions become change agents. Communities become accelerators. And experience, not youth, emerges as a hidden catalyst of adoption.

If you want to scale generative AI successfully, start with people, not code.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/07/Wade-1290x860-1.jpg),"[""api"",""artificial-intelligence"",""data-analysis"",""networking"",""rust"",""tensorflow""]"
3efa55e1-e0ed-4ae5-880f-bff433addfe7,4aec8d96156e58f5ed4693e8257476c70dd57a2f92a3f763f66e379356ffc111,0d379404-a15d-4c5b-8a86-5f48b82e93a4,Scenario Planning Amid Radical Uncertainty,https://sloanreview.mit.edu/article/scenario-planning-how-to/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Weber-Scenario-1290x860t.jpg)

Carolyn Geason-Beissel/MIT SMR | Getty Images

“What’s the best-case scenario that you can logically describe?” As an academic and adviser who has spent more than 30 years forecasting future intersections of people and the political economy through the lens of technological change, I’m often asked that question. My answer is always the same: Best-case scenarios do not exist. Neither do worst-case scenarios. Human-powered futures are always a mix of misunderstood and unimagined downsides and upsides. What’s different from one era to another? The range and breadth of uncertainty that decision makers must navigate as they try to amplify the upsides and reduce the downsides of change.

That’s important because politicians, CEOs, financial markets, and everyday people have one thing in common: They abhor radical uncertainty. It makes their purpose and mission seem impossible.

Radical uncertainty is entirely different from conventional risk, which positively powers markets and creates opportunities for leaders to make decisions with potentially outsize political and economic payoffs. Risk has boundaries and relevant data; it can be priced and hedged because it represents a partially knowable probability distribution among outcomes. That’s why the saying “no risk, no return” makes sense and why we comfortably talk about and understand “risk-on” and “risk-off” behavior in markets and politics.

Radical uncertainty has none of those positive characteristics. When events move far outside the boundaries of what we know from the past, and when the shape of the probability distribution can’t be mapped, business and political systems tend toward paralysis. People become deeply anxious. Markets gyrate. C-suite confidence corrodes and decision-making freezes up.

There is clear evidence for each of those effects right now in the United States’ political economy. [Consumer confidence](https://www.sca.isr.umich.edu/) has been volatile in 2025, including some lows not seen since the 2008 financial crisis. During the second week of April, the S&P 500 rose 9.5% in one day and dropped 3.5% the following day, transiting from bear to bull market in less than two months in the spring. Walmart [declined to provide earnings guidance](https://finance.yahoo.com/news/walmart-ditching-earnings-guidance-tariffs-143247978.html) for the first quarter, and United Airlines offered two separate and [entirely different profit forecasts](https://www.wsj.com/business/earnings/united-airlines-tops-expectations-despite-travel-demand-worries-abe0bbb0?st=jpudLK&reflink=desktopwebshare_permalink) for the second quarter.

The common thread is radical uncertainty engineered by the Trump administration’s notable first six months. Whether you support or oppose the Trump agenda, the fact is that no American administration of at least the past century, and possibly ever, has created such radical uncertainty around such a broad swath of the American political economy, even within the first 100 days. That includes President [Franklin D. Roosevelt’s first 100 days](https://www.fdrlibrary.org/documents/356632/390886/actionguide.pdf/07370301-a5c1-4a08-aa63-e611f9d12c34), as remarkable as they seemed in 1933. Again, that is no judgment on the agenda but simply an observation of a reality experienced by supporters and opponents of the new administration alike.

### Two Key Scenario-Planning Lessons

I’ve spent much of my academic career studying what happens in moments like these — [times of high-stress decision-making](https://sloanreview.mit.edu/article/how-to-strategize-in-an-out-of-control-world/), when people and organizations are functioning (often badly) under radical uncertainty. It’s been a natural extension of that research to advise companies and governments about how to better [navigate these moments](https://sloanreview.mit.edu/tag/scenario-planning/) and deploy and refine tools and methods that can help them make better choices. There are two important lessons about [scenario planning](https://sloanreview.mit.edu/article/scenario-planning-a-tool-for-strategic-thinking/) to share from that experience.

#### 1\. Beware Overindexing on Short-Term Signals

First, certain identifiable and dysfunctional patterns of information processing and decision-making are painfully common. The most visible is overindexing on very short-term signals, whether it be a single day’s news headlines or a social media post. People tend to chase what looks (for a short time) like a high-salience signal or interpretation, which leads them to oscillate too fast and too far between optimism and pessimism.

Instead of incorporating a new piece of evidence as simply the next data point in a longer series (in technical language, [Bayesian updating](https://plato.stanford.edu/entries/bayes-theorem/)), the person gives in to the anxiety of radical uncertainty — and magnifies the meaning of what might have been just a blip, a quip, or the musings of a particular pundit.

If you watch your own mind over the course of a day, you’ll probably see exactly this tendency. And if you look a bit deeper, you’ll see that overindexing on one signal engages both “hot” and “cold” elements of your experience — emotions and cognition. If you were to make decisions on this basis right now, you’d almost certainly make bad decisions that wouldn’t stand the test of much time. (If you find it hard to see this tendency in yourself at first, look for it in others — say, your collaborators or competitors. Then look back in the mirror one more time.)

This kind of oscillation or ping-ponging isn’t just bad for decision-making; it is utterly exhausting to both individuals and groups.

Another dysfunctional tendency often creeps in to compensate when people feel overwhelmed: the search for false certainty that may stop the exhausting struggle. An article with a bold assertion or advice from a consultant with a strong point of view, for instance, may get passed around as “the answer we’ve been looking for,” given people’s desire to anchor on a point prediction about what it all means.

Within your team, you can almost hear the sigh of relief: What were confusing and discrepant signals yesterday now suddenly look like a puzzle with all of the pieces in place. Decision makers feel that they can work from that anchor point to define that supposed best-practice answer that everyone is hungry for and then execute on it. Contingency plans for being wrong are sometimes part of the answer, but surprisingly often, companies create no such plans or largely ignore them. Newly emerging evidence is interpreted to be consistent with the point prediction rather than evaluated on its own terms.

People and organizations sometimes place enormous bets on these point predictions, as President Jimmy Carter did when he ordered the Iran hostage rescue effort to proceed without meaningful plans for what to do if it went wrong. It might appear courageous for individuals or smaller companies to bet the farm in this way, and it can certainly feel brave in the moment. But it’s not strategic, particularly for large companies: The chances of getting it wrong are vastly greater than the chances of getting it right.

#### 2\. Rethink Preparation

The second lesson I’ve learned is that there is a way to do better. Scenario thinking starts from a different premise: that we cannot predict the future and therefore shouldn’t try. It was [pioneered at Royal Dutch Shell](https://www.shell.com/news-and-insights/scenarios.html) in the 1970s and developed for use outside the energy sector by my colleagues and me at Global Business Network and the Monitor Group in subsequent decades. Particularly during periods of high uncertainty, and by necessity under radical uncertainty, the guiding principle for strategic decision-making needs to be preparation, not prediction. The goal is robust strategy development that is designed to perform regardless of where reality lands on the map.

The key to scenario thinking is to render that landscape map of what is possible expansively and broadly enough to incorporate what is _actually_ possible rather than what a person or an organization _wishes_ were possible because it would be easier to handle. That means stretching the boundaries of the critical uncertainties — those factors that are at once both the most important and the most uncertain — a little bit further out than seems plausible now.

To make sense of today’s U.S. political economy, I recently built [scenarios looking two years ahead](https://breakwaterstrategy.com/app/uploads/2025/04/Scenarios_2026_Final.pdf) that incorporate a Trump economy that could shrink by more than 6% (nearly Great Depression magnitude) or grow by the same percentage — greater than the best peacetime years in the 20th century.

My scenarios combine this range of economic possibilities with a second critical uncertainty, which is where the countervailing forces that support and oppose the administration’s agenda will arise and exert power in public and policy debate. On one side of the continuum, we may see a revitalization of traditional media and political institutions — _The New York Times_, the Democratic Party, Fox News. On the other end of the continuum, we may see the continued rise to dominance of new power centers that shape debate — podcasts, YouTube influencers, Discord server groups that seamlessly meld entertainment, information, commerce, and mobilization to action. For example, “cutefluencers” (social influencers who gain credence by trying to be cute) are a real force right now; so, what does it look like if they become even more influential on public and policy debate than any reasonable person might currently imagine possible?

Considering these two critical uncertainties, this scenario model yields four distinct futures that, as a set, map a possibility space for the U.S. political economy out to the 2026 midterm elections. Those futures range from a 1970s redux world but with profound American decline, deep stagflation, and a punishing trans-Pacific cold war that might not stay cold for the duration; to a techno-libertarian-rationalist political synthesis that leaves both Republicans and Democrats behind in an AI- and data-powered push for growth that has little to no room for ideology and culture wars.

Is this just undisciplined imagination or fantastical fiction? I’ve produced scenarios for organizations that needed to look much further out — 5, 10, or even [25 years into the future](https://www.nytimes.com/2003/04/07/business/new-economy-scenario-planning-explores-many-routes-chaos-could-take-for-business.html) — and I’ve learned one simple lesson about the rate and magnitude of change: It’s generally faster and greater than the decision makers at “go time” expect.

You can’t and shouldn’t try to plan against science fiction. But the tendency of people and organizations to pull back toward the familiar and label what are plausible outcomes as “too far out” is powerful — and bad.

### Build an Expansive Scenario Map

Good scenarios press back, gently but firmly, against the tendency to turn away from the unfamiliar. At moments of radical uncertainty like the present, the need to do so should come into useful contact with visible reality. In the past six months, the Trump administration has expanded the [Overton window](https://www.britannica.com/topic/Overton-window) of what is possible from a policy standpoint beyond what almost any sober pundit or strategic planner was incorporating into their models a few months ago — to say nothing of a year or two ago. Why should we expect the rate of change to slow? The fact that decision makers would like that to happen or would find it easier to do their jobs if it did is not the foundation for a rational planning approach.

An expansive scenario map provides a better foundation. Such maps help leaders make sense of incoming evidence in a systematic way: They can place each piece of evidence on the map where it belongs and carefully watch the evidence accumulate over time so they can continually adjust their estimates of where the future is likely to land. Radical uncertainty will eventually begin to stabilize into something more akin to risk. Decision makers who can track that process accurately and not jump too soon (or too late) have the long-term advantage.

Scenario thinking is a model that generates a more accurate reflection of the world we’re living in right now. It is also less vulnerable to arbitrage by people who might want to exacerbate radical uncertainty in an intentional way, to advantage their goals at the expense of yours. And scenario thinking is manageable for human decision makers, who can’t hold an infinite number of possibilities in their minds at once and should not trust an opaque generative AI model to try to do so for them.

It isn’t easy for corporate leaders to say “We don’t know” out loud and to act with courage and conviction in the face of that fact. Scenario thinking uniquely works to enable both at the same time. Now is the moment to do it.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Weber-Scenario-1290x860t.jpg),"[""artificial-intelligence"",""helm"",""machine-learning"",""reinforcement-learning"",""rust"",""transformers""]"
96e2bb6b-66f4-4c1e-8155-dff754b652d1,201e7aa5e1b09ba0b3adcb00b9d0b273483a6655151430db97269fa25d9a09f7,0d379404-a15d-4c5b-8a86-5f48b82e93a4,The Six Most Popular Stories of 2025 — So Far,https://sloanreview.mit.edu/article/the-six-most-popular-stories-of-2025-so-far/,"![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Top62025-tile-angleREV-1290x860-1.jpg)

Sometimes a concept captivates our readers. With the publication of “Philosophy Eats AI” in January, Michael Schrage and David Kiron didn’t just put an intellectual stake in the ground: They started a business leadership discussion. Consider their assertion:

> Philosophy is eating AI: As a discipline, data set, and sensibility, philosophy increasingly determines how digital technologies reason, predict, create, generate, and innovate. The critical enterprise challenge is whether leaders will possess the self-awareness and rigor to use philosophy as a resource for creating value with AI or default to tacit, unarticulated philosophical principles for their AI deployments. Either way — for better and worse — philosophy eats AI. For strategy-conscious executives, that metaphor needs to be top of mind.

Indeed, while you perhaps haven’t considered this, philosophy shapes the training sets and connections inside every large language model that you and your colleagues use. It shapes how you, as a leader, ask people to employ artificial intelligence or whether you default to the recommendations of vendors — which have their _own_ philosophical principles.

This discussion proved important to leaders — so much so that Schrage and Kiron followed it up with a [now-popular video](https://www.youtube.com/watch?v=59t62NHJFH4) to debate the issues further.

It is a fascinating, and frightening, time for leaders grappling with AI’s quick evolution. Leaders are watching AI tools save people time by automating mundane tasks. On the flip side, leaders are trying to figure out how not to destroy their companies’ talent pipelines as certain categories of entry-level jobs get replaced with AI tools. Without question, AI has created a new wave of people management and leadership challenges.

Here at MIT SMR, we work hard to bring you practical, evidence-based strategies to tackle a broad set of challenges and grow your organization. In a year that has already delivered a great deal of chaos to us all, consider these six articles for expert, novel advice and perspective.

#### 1\. Philosophy Eats AI

As AI and large language models evolve, leaders need to examine the philosophical foundations of how cognitive technologies are trained. Philosophy offers important perspectives on the goals of AI models, the definition of knowledge, and AI’s representations of reality. All of these perspectives shape how AI creates business value, and companies that seek business value from technology investment must look more deeply at their philosophical framework.

Read the full article “[Philosophy Eats AI](https://sloanreview.mit.edu/article/philosophy-eats-ai/),” by Michael Schrage and David Kiron.

#### 2\. Five Traits of Leaders Who Excel at Decision-Making

When we’re forced to make a decision in the heat of uncertainty, many of us tend toward one of two extremes: a hasty rush to action, or a complete avoidance of it. A new study conducted by HSBC and the author looked at what traits stood out among business leaders who effectively made decisions at their biggest personal and professional moments. The research found that viewing change positively, framing unexpected challenges as opportunities, and embracing grounded optimism were key.

Read the full article “[Five Traits of Leaders Who Excel at Decision-Making](https://sloanreview.mit.edu/article/five-traits-of-leaders-who-excel-at-decision-making/),” by David Tuckett.

#### 3\. Five Trends in AI and Data Science for 2025

In 2025, surveys reveal five big AI trends: a need to grapple with the promise and hype around agentic AI; the push to measure results from generative AI experiments; an emerging clearer vision of what a data-driven culture really means; a renewed focus on unstructured data; and a continued struggle over which C-suite role will oversee data and AI responsibilities.

Read the full article “[Five Trends in AI and Data Science for 2025](https://sloanreview.mit.edu/article/five-trends-in-ai-and-data-science-for-2025/),” by Thomas H. Davenport and Randy Bean.

#### 4\. Why AI Demands a New Breed of Leaders

Artificial intelligence is changing how humans and machines work together. But most organizations still focus on the technical aspect of AI implementation because their leadership structure does too. Companies need a new role, the chief innovation and transformation officer, to manage the profound cultural and organizational changes AI adoption brings. Here’s why forward-thinking organizations have already hired or plan to bring on such leaders.

Read the full article “[Why AI Demands a New Breed of Leaders](https://sloanreview.mit.edu/article/why-ai-demands-a-new-breed-of-leaders/),” by Faisal Hoque, Thomas H. Davenport, and Erik Nelson.

#### 5\. Why AI Will Not Provide Sustainable Competitive Advantage

Artificial intelligence does not change anything about the fundamental nature of sustained competitive advantage when its use is pervasive. Once AI’s use is ubiquitous, it will transform economies and lift markets as a whole, but it will not uniquely benefit any single company. Businesses seeking to gain an innovation edge over rivals will need to focus their efforts on cultivating creativity among their employees.

Read the full article “[Why AI Will Not Provide Sustainable Competitive Advantage](https://sloanreview.mit.edu/article/why-ai-will-not-provide-sustainable-competitive-advantage/),” by David Wingate, Barclay L. Burns, and Jay B. Barney.

#### 6\. When Team Accountability Is Low: Four Hard Questions for Leaders

Many leaders bemoan a lack of accountability on their team. But moaning about it — or scolding people — won’t fix the problem. A leader needs to understand what’s _stopping_ people from behaving accountably and then address those challenges. The bad news is that you may have to actively disrupt some of your own long-held behaviors as well. Ask these four questions and then use the related tips to break problematic behavioral patterns accordingly.

Read the full article, “[When Team Accountability Is Low: Four Hard Questions for Leaders](https://sloanreview.mit.edu/article/when-team-accountability-is-low-four-hard-questions-for-leaders/),” by Melissa Swift.",![](https://sloanreview.mit.edu/wp-content/uploads/2025/06/Top62025-tile-angleREV-1290x860-1.jpg),"[""artificial-intelligence"",""data-science"",""devops"",""natural-language-processing"",""python"",""transformers""]"
22eeec2c-998a-42eb-9791-521bd6b03e66,82126e1a9ef9b48d3695afe9366a61c5303147abe53c48537b2ef1c171a676aa,0d379404-a15d-4c5b-8a86-5f48b82e93a4,9 Mistakes Leaders Make With AI Strategy,https://sloanreview.mit.edu/video/9-mistakes-leaders-make-with-ai-strategy/,"At the 2025 MIT Sloan CIO Symposium, many tech and business leaders voiced the same frustrations — about AI initiatives that aren’t delivering business value, pilot projects that never made it to production, and their ongoing struggle to figure out what’s going wrong.

So we asked those leaders and AI experts in the room to share lessons learned by asking a key question: What’s a common mistake you see organizations still making when shaping AI strategy?

Their answers revealed a pattern. “The low-hanging fruit — it’s not really as low as we think it is,” said MIT Sloan senior lecturer George Westerman, warning about overestimating AI’s capabilities. But the challenges go much deeper than technical limitations.

Monica Caldas, executive vice president and CIO at Liberty Mutual Insurance, stressed the need for cross-functional teams and cultural change management, and to rethink how organizations operate.

McKinsey partner Hannah Mayer made a telling observation about speed of change: “Employees are three times more willing and excited to leverage AI in the workplace than their leaders expect.” She pointed to executive disagreement as a bottleneck that’s holding organizations back.

Watch the video to hear advice about avoiding these critical mistakes:

*   **Setting unrealistic expectations** by overestimating current AI tool capabilities.
*   **Missing the transformation opportunity** by treating AI like just another software tool.
*   **Getting stuck in pilot mode** and coming up short on production deployments.
*   **Encountering executive hesitation** and moving forward too slowly.
*   **Forgetting the human factor** and focusing too much on technology rather than people.
*   **Underestimating the security risks** and failing to build resilience.

These AI strategy errors are avoidable once you know what to look for. Watch this video to learn from your peers’ experiences and avoid the pitfalls that are costing organizations time, money, and competitive advantage.

##### Video Credits

**Laurianne McLaughlin** is senior editor, digital, at MIT Sloan Management Review.

**M. Shawn Read** is the multimedia editor at MIT Sloan Management Review.","At the 2025 MIT Sloan CIO Symposium, many tech and business leaders voiced the same frustrations — about AI initiatives that aren’t delivering business value, pilot projects that never made it to prod...","[""api"",""artificial-intelligence"",""devops"",""rust"",""tensorflow"",""transformers""]"
c02b69c7-bc47-4741-bd9f-9b130f41a4d7,09a430a64cc18ba677eaaa9c4e5a2d7c20d7446b,161e6a7a-f07a-4953-95ba-77ccee004a53,Harnessing the Power of Nested Materialized Views and exploring Cascading Refresh,https://aws.amazon.com/blogs/big-data/harnessing-the-power-of-nested-materialized-views-and-exploring-cascading-refresh/,"[Amazon Redshift materialized views](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html) enables you to significantly improve performance of complex queries. Materialized views store precomputed query results that future similar queries can utilize, offering a powerful solution for data warehouse environments where applications often need to execute resource-intensive queries against large tables. This optimization technique enhances query speed and efficiency by allowing many computation steps to be skipped, with precomputed results returned directly. Materialized views are particularly useful for speeding up predictable and repeated queries, such as those used to populate dashboards or generate reports. Instead of repeatedly performing resource-intensive operations, applications can query a materialized view and retrieve precomputed results, leading to significant performance gains and improved user experience. Additionally, materialized views can be incrementally refreshed, applying logic only to changed data when data manipulation language (DML) changes are made to the underlying base tables, further optimizing performance and maintaining data consistency.

This post demonstrates how to maximize your [Amazon Redshift](https://docs.aws.amazon.com/redshift/) query performance by effectively implementing materialized views. We’ll explore creating materialized views and implementing nested refresh strategies, where materialized views are defined in terms of other materialized views to expand their capabilities. This approach is particularly powerful for reusing precomputed joins with different aggregate options, significantly reducing processing time for complex ETL and BI workloads. Let’s explore how to implement this powerful feature in your data warehouse environment.

### Introduction to Nested Materialized Views

Nested materialized views in Amazon Redshift allow you to create materialized views based on other materialized views. This capability enables a hierarchical structure of precomputed results, significantly enhancing query performance and data processing efficiency. With nested materialized views, you can build multi-layered data abstractions, creating increasingly complex and specialized views tailored to specific business needs.This layered approach offers several advantages:

*   **Improved Query Performance**: Each level of the nested materialized view hierarchy serves as a cache, allowing queries to quickly access pre-computed data without the need to traverse the underlying base tables.
*   **Reduced Computational Load**: By offloading the computational work to the materialized view refresh process, you can significantly reduce the runtime and resource utilization of your day-to-day queries.
*   **Simplified Data Modeling**: Nested materialized views enable you to create a more modular and extensible data model, where each layer represents a specific business concept or use case.
*   **Incremental Refreshes**: The Redshift materialized views support incremental refreshes, allowing you to update only the changed data within the nested hierarchy, further optimizing the refresh process.
*   **Cascading Materialized Views**: The Redshift materialized views support automatic handling of Extract, Load, and Transform (ELT) style workloads, minimizing the need for manual creation and management of these processes.

You can implement nested materialized views using the [CREATE MATERIALIZED VIEW](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-create-sql-command.html) statement, which allows referencing other materialized views in the definition. Common use cases include:

*   Modular data transformation pipelines
*   Hierarchical aggregations for progressive analysis
*   Multi-level data validation pipelines
*   Historical data snapshot management
*   Optimized BI reporting with precomputed results

### Architecture

[![architecture](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/architecture-2.png)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/architecture-2.png)

Architectural diagram depicting Amazon Redshift’s nested materialized view structure. Shows multiple base tables (orange) connecting to materialized views (red), with connections to a nested view layer and data sharing table (green). Includes integration points for users and QuickSight visualization.

1.  **Base Table(s)**: These are the underlying base tables that contain the raw data for your data warehouse. It can be local tables or data sharing tables.
2.  **Base Materialized View(s)**: These are the first-level materialized views that are created directly on top of the base tables. These views encapsulate common data transformations and aggregations. This can serve as the base for the nested materialized view and also be accessed by users directly.
3.  **Nested Materialized View(s)**: These are the second level (or higher) materialized views that are created based on the base materialized views. The nested materialized view can further aggregate, filter, or transform the data from the base materialized views.
4.  **Application/Users/BI Reporting**: The application or business intelligence (BI) tools interact with the nested materialized views to generate reports and dashboards. The nested views provide a more optimized and precomputed data structure for efficient querying and reporting.

## Creating and using nested materialized views

To demonstrate how nested materialized views work in Amazon Redshift, we’ll use the TPC-DS dataset. We’ll create three queries using the STORE, STORE\_SALES, CUSTOMER, and CUSTOMER\_ADDRESS tables to simulate data warehouse reports. This example will illustrate how multiple reports can share result sets and how materialized views can improve both resource efficiency and query performance.Let’s consider the following queries as dashboard queries:

```
SELECT cust.c_customer_id,
cust.c_first_name, 
cust.c_last_name, 
sales.ss_item_sk, 
sales.ss_quantity, 
cust.c_current_addr_sk 
FROM store_sales sales INNER JOIN customer cust
ON sales.ss_customer_sk = cust.c_customer_sk;

SELECT cust.c_customer_id,
cust.c_first_name, 
cust.c_last_name, 
sales.ss_item_sk, 
sales.ss_quantity, 
cust.c_current_addr_sk, 
store.s_store_name
FROM store_sales sales INNER JOIN customer cust
ON sales.ss_customer_sk = cust.c_customer_sk
INNER JOIN store store
ON sales.ss_store_sk = store.s_store_sk;

SELECT cust.c_customer_id, 
cust.c_first_name, cust.c_last_name, 
sales.ss_item_sk, 
sales.ss_quantity, 
addr.ca_state
FROM store_sales sales INNER JOIN customer cust
ON sales.ss_customer_sk = cust.c_customer_sk
INNER JOIN store store
ON sales.ss_store_sk = store.s_store_sk
INNER JOIN customer_address addr
ON cust.c_current_addr_sk = addr.ca_address_sk;
```

Notice that the join between STORE\_SALES and CUSTOMER tables is present at all 3 queries (dashboards).

The second query adds a join with STORE table and the third query is the second one with an extra join with CUSTOMER\_ADDRESS table. This pattern is common in business intelligence scenarios. As mentioned earlier, using a materialized view can speed up queries because the result set is stored and ready to be delivered to the user, avoiding reprocessing of the same data. In cases like this, we can use nested materialized views to reuse already processed data.When transforming our queries into a set of nested materialized views, the result would be as below:

```
CREATE MATERIALIZED VIEW StoreSalesCust as
SELECT cust.c_customer_id, 
cust.c_first_name, 
cust.c_last_name, 
sales.ss_item_sk, 
sales.ss_store_sk, 
sales.ss_quantity, 
cust.c_current_addr_sk
FROM store_sales sales INNER JOIN customer cust
ON sales.ss_customer_sk = cust.c_customer_sk;

CREATE MATERIALIZED VIEW StoreSalesCustStore as
SELECT storesalescust.c_customer_id, 
storesalescust.c_first_name, 
storesalescust.c_last_name, 
storesalescust.ss_item_sk, 
storesalescust.ss_quantity, 
storesalescust.c_current_addr_sk, 
store.s_store_name
FROM StoreSalesCust storesalescust INNER JOIN store store
ON storesalescust.ss_store_sk = store.s_store_sk;

CREATE MATERIALIZED VIEW StoreSalesCustAddress as
SELECT storesalescuststore.c_customer_id, 
storesalescuststore.c_first_name, 
storesalescuststore.c_last_name, 
storesalescuststore.ss_item_sk, 
storesalescuststore.ss_quantity, 
addr.ca_state
FROM StoreSalesCustStore storesalescuststore INNER JOIN customer_address addr
ON storesalescuststore.c_current_addr_sk = addr.ca_address_sk;
```

Nested materialized views can improve performance and resource efficiency by reusing initial view results, minimizing redundant joins, and working with smaller result sets. This creates a hierarchical structure where materialized views depend on one another. Due to these dependencies, you must refresh the views in a specific order.

[![message](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/msg1-1.png)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/msg1-1.png)

SQL query result indicating dependency issue for REFRESH MATERIALIZED VIEW StoreSalesCustAddress.

With the new option “REFRESH MATERIALIZED VIEW _mv\_name_ CASCADE” you will be able to refresh the entire chain of dependencies for the materialized views you have. Note that in this example we are using the third materialized view, StoreSalesCustAddress, and this will refresh all 3 materialized views because they are dependent on each other.

[![message](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/msg2-1.png)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/msg2-1.png)

SQL query showing successful CASCADE refresh of StoreSalesCustAddress materialized view in Amazon Redshift.

If we use the second materialized view with the CASCADE option, we will refresh only the first and second materialized views, leaving the third unchanged. This may be useful when we need to keep some materialized views with less current data than others.

The [SVL\_MV\_REFRESH\_STATUS](https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_MV_REFRESH_STATUS.html) system view reveals the refresh sequence of materialized views. When triggering a cascade refresh on StoreSalesCustAddress, the system follows the dependency chain we established: StoreSalesCust refreshes first, followed by StoreSalesCustStore, and finally StoreSalesCustAddress. This demonstrates how the refresh operation respects the hierarchical structure of our materialized views.

[![result](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/result1-1.png)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/result1-1.png)

SQL query result from SVL\_MV\_REFRESH\_STATUS showing successful recomputation of three materialized views.

#### Considerations

Consider a dependency chain where StoreSalesCust (A) → StoreSalesCustStore (B) → StoreSalesCustAddress (C).

*   The CASCADE refresh behavior works as follows:
    *   When refreshing C with CASCADE: A, B, and C will all be refreshed.
    *   When refreshing B with CASCADE: Only A and B will be refreshed.
    *   When refreshing A with CASCADE: Only A will be refreshed.
    *   If you specifically need to refresh A and C but not B, you must perform separate refresh operations without using CASCADE—first refresh A, then refresh C directly.

## Best Practices for Materialized View

*   Improve the source query: Start with a well-optimized SELECT statement for your materialized view. This is especially important for views that need full rebuilds during each refresh.
*   Plan refresh strategies: When creating materialized views that depend on other materialized views, you cannot use AUTO REFRESH YES. Instead, implement orchestrated refresh mechanisms using Redshift Data API with Amazon EventBridge for scheduling and AWS Step Functions for workflow management.
*   Leverage distribution and sort keys_:_ Properly configure distribution and sort keys on materialized views based on their query patterns to optimize performance. Well-chosen keys improve query speed and reduce I/O operations.
*   Consider incremental refresh capability: When possible, design materialized views to support incremental refresh, which only updates changed data rather than rebuilding the entire view, greatly improving refresh performance.
*   To learn more about the Automated materialized view (auto-MV) feature to boost your workload performance, this intelligent system monitors your workload and automatically creates materialized views to enhance overall performance. For more detailed information on this feature, please refer to [Automated materialized views](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-auto-mv.html).

## Clean up

Complete the following steps to clean up your resources:

*   Delete the Redshift provisioned replica cluster or the Redshift serverless endpoints created for this exercise

or

*   Drop only the Materialized view which you have created for testing

## Conclusion

This post showed how to create nested Amazon Redshift materialized views and refresh the child materialized views using the new REFRESH CASCADE option. You can quickly build and maintain efficient data processing pipelines and seamlessly extend the low latency query execution benefits of materialized views to data analysis.

* * *

### About the authors

[![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/ritesh-100-1.png)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/ritesh-100-1.png)**Ritesh Kumar Sinha** is an Analytics Specialist Solutions Architect based out of San Francisco. He has helped customers build scalable data warehousing and big data solutions for over 16 years. He loves to design and build efficient end-to-end solutions on AWS. In his spare time, he loves reading, walking, and doing yoga.

[![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/raza_new_new_new-1.jpg)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/raza_new_new_new-1.jpg)**Raza Hafeez** is a Senior Product Manager at Amazon Redshift. He has over 13 years of professional experience building and optimizing enterprise data warehouses and is passionate about enabling customers to realize the power of their data. He specializes in migrating enterprise data warehouses to AWS Modern Data Architecture.

[![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/Ricardo.jpg)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/09/Ricardo.jpg)**Ricardo Serafim** is a Senior Analytics Specialist Solutions Architect at AWS. He has been helping companies with Data Warehouse solutions since 2007.",[Amazon Redshift materialized views](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html) enables you to significantly improve performance of complex queries. Materialized v...,"[""api"",""aws"",""data-analysis"",""database"",""imaging"",""python""]"
b5036752-857f-4fbf-a3ad-52c8b83b0112,b951ce0acb070729daab939fa0141fbc61a87120,161e6a7a-f07a-4953-95ba-77ccee004a53,Realizing ocean data democratization: Furuno Electric’s initiatives using Amazon DataZone,https://aws.amazon.com/blogs/big-data/realizing-ocean-data-democratization-furuno-electrics-initiatives-using-amazon-datazone/,"_This is a guest post authored by Akira Mikami, a technical expert at Furuno Electric. The content and opinions in this post are those of the third-party author and AWS is not responsible for the content or accuracy of this post._

Since successfully commercializing the world’s first fish finder in 1948, Furuno Electric has been developing unique ultrasonic and electronic technologies in the marine electronics field. Under the company motto of “making the invisible visible”, they’ve have expanded their business centered on marine sensing technology and are now extending into subscription-based data businesses using [Internet of Things](https://aws.amazon.com/what-is/iot/) (IoT) data. They’re are actively promoting the planning and development of data businesses to realize their new management vision outlined in FURUNO GLOBAL VISION NAVI NEXT 2030.

Like many manufacturing companies, Furuno Electric faced significant changes in revenue structure and technical architecture as they transitioned from traditional business to data-driven business. To succeed in this transformation, it was essential to build a foundation that promotes data utilization across the entire organization.

This post demonstrates how Furuno Electric built their system using [Amazon DataZone](https://aws.amazon.com/datazone/) and other [Amazon Web Services](https://aws.amazon.com/) (AWS) services to address technical infrastructure fragmentation, establish proper security governance, and develop an effective data business promotion system as part of their journey transitioning from a traditional manufacturing company to a data-driven business.

## Challenges

Furuno Electric faced three specific challenges in promoting their data business: technical infrastructure fragmentation and duplication, lack of security governance, and underdeveloped data business promotion system.

Project managers in the data business were independently designing and building data infrastructure, resulting in duplication of components for data collection, processing, and storage. This situation created wasteful development investments, hindered effective use of common data, and caused inefficient states that took time to launch businesses. Marine data services including fishing vessel data collection and sharing system, FWC, and Furuno Open Platform (FOP) had similar functions implemented separately for each project along the functional axes of data collection, processing, visualization, and analysis, resulting in unnecessary workload across the organization.

Security measures were considered and implemented separately by each department, and although checklists existed, they weren’t applied uniformly. This resulted in a lack of consistency in security measures, duplicate consideration costs for each department, and uncertainty in the comprehensiveness of measures. Integrated risk management was also difficult.

The organizational structure wasn’t prepared for the iterative development processes and long-term revenue models specific to data businesses, and there was a lack of mechanisms for cross-departmental data utilization and joint development. The distributed operational structure across departments made it difficult to rapidly deploy and continuously improve data businesses. In the process of creating data businesses, it became necessary to build entirely different customer relationships compared to traditional product sales businesses. In terms of organizational management and talent strategy, there was a need to transition from a top-down, risk-averse, specialized skill-focused structure to a bottom-up, challenge-oriented structure that emphasizes communication skills and diversity.

## Solution overview

Furuno Electric built a data management foundation centered on Amazon DataZone, [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3), [AWS Glue](https://aws.amazon.com/glue/), and [AWS Control Tower](https://aws.amazon.com/controltower/), a comprehensive solution designed to address each of the three challenges mentioned in the preceding section.

### Building the integrated data platform JuBuRaw

To address technical infrastructure fragmentation and duplication, they built Junction Architecture of Business Raw Data (JuBuRaw), a platform that consolidates common components for data collection, storage, management, and authentication. Using [AWS Cloud Development Kit](https://aws.amazon.com/cdk/) (AWS CDK) to code the infrastructure, they achieved standardization and automation of environment construction. This provides consistency and reproducibility, making it easier to add new systems and migrate existing systems to the common platform. Merely by executing CDK, a standard data pipeline (using [AWS IoT Core](https://aws.amazon.com/iot-core/), Amazon S3, AWS Glue, [Amazon Kinesis](https://aws.amazon.com/kinesis/), [Amazon API Gateway](https://aws.amazon.com/api-gateway/), and [AWS Lambda](https://aws.amazon.com/lambda/)) for a specific system is automatically built. This eliminates duplicate design and development within the organization, reducing business launch time and improving fixed cost management. By standardizing common functions, they reduced the management and operation costs of existing systems and enabled the launch of new systems in half the time compared to before.

The following diagram is the overall JuBuRaw architecture.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/Furuno-Picture1.png)

### Security control with AWS Control Tower

To address the lack of security governance, they implemented a comprehensive security framework centered on AWS Control Tower to apply consistent security policies across multiple accounts. With automated monitoring systems using [AWS Security Hub](https://aws.amazon.com/security-hub/), [AWS Config](https://aws.amazon.com/config/), and [AWS CloudTrail](https://aws.amazon.com/cloudtrail/) and an integrated authentication system using [AWS IAM Identity Center](https://aws.amazon.com/iam/identity-center/), they provide security consistency while reducing operational costs and management burden.

With the organization’s management account at the top, they placed AWS Control Tower, [AWS Organizations](https://aws.amazon.com/organizations/), and AWS IAM Identity Center to achieve hierarchical security management. By adopting a multi-layered defense structure consisting of account baselines with AWS CloudTrail and AWS Config enabled, log archive environments, and audit and security operation environments, consistent security policies are applied to all accounts, enabling early detection and response to security incidents. This integrated approach has reduced the workload for security responses. This configuration is shown in the following diagram.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/Furuno-Picture2.png)

### Establishing a data democratization foundation with Amazon DataZone

To address the underdeveloped data business promotion system, they introduced Amazon DataZone to streamline data discovery, sharing, and governance across the organization. They clarified the role division between the infrastructure management team and the data management team, centralizing data security policies, quality management, and metadata standardization. With a project-based collaboration environment, they promoted cross-departmental data utilization, establishing a foundation to support the creation and continuous monetization of data businesses.

## Organizational reform and operational structure establishment

In parallel with the introduction of technical solutions, they implemented organizational reforms to support medium- to long-term data utilization. The new organizational structure consists of three main roles: the infrastructure management team, the data management team, and the chief data officer. The following chart shows this organizational structure.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/Furuno-Picture3-1.png)

The infrastructure management team is responsible for maintaining and developing the technical foundation of the platform, managing multiple accounts using AWS Control Tower, applying and monitoring security baselines, and tracking infrastructure version management and changing history. By specializing in common technologies, they can provide a stable platform.

The data management team is responsible for data quality management and continual improvement using [AWS Glue Data Quality](https://aws.amazon.com/glue/features/data-quality/), standardization and maintenance of metadata, definition and application of data security policies, management of Amazon DataZone data Catalog, and providing data governance using Amazon DataZone. To maximize the value of data, they focus on deeply understanding business requirements and data characteristics and performing appropriate data management.

The chief data officer is responsible for formulating data business strategies and determining direction, promoting coordination and collaboration between teams, making decisions regarding the evolution of the data management foundation, and fostering a data utilization culture throughout the organization. From a strategic perspective, they oversee the whole and bridge business goals and technology.

This clear division of roles has established an operational structure for effective data utilization, accelerating the data business creation process. Additionally, clarifying data ownership has improved data quality and reliability, promoting data utilization across the organization. This structure is sustainable and can flexibly respond to technological changes and changes in the business environment.

## Benefits of the modernized platform

As a concrete application example of the integrated data platform JuBuRaw and organizational structure explained in the previous section, we introduce the migration project of the existing service SHIPS. This use case is a comprehensive migration case that uses all three solution elements of data collection, management, and utilization mentioned earlier.

Furuno Electric provides a system called SHIPS that plots ship position information and monitors the status of equipment installed on ships. By migrating this existing service to the JuBuRaw foundation, the several functional enhancements are expected.

In terms of data integration enhancement, by using the [data catalog function of Amazon DataZone](https://docs.aws.amazon.com/datazone/latest/userguide/working-with-business-catalog.html), it becomes easier to integrate not only ship position information but also various data sources such as internal systems, IoT devices, other company systems, automatic identification system (AIS) data, and weather and sea condition data. This enables swift data analysis and comprehensive ship management, which means operators can detect potential issues and implement preventive measures before they develop into serious problems. Particularly important is that by storing this data in a common data lake and retaining them as master data, they create an environment where the data can be easily used by other applications.

For security enhancement, organizations can use Amazon DataZone federated governance with [publish-subscribe (pubsub)](https://aws.amazon.com/what-is/pub-sub-messaging/) workflow mechanism and fine-grained access control capabilities. This means they can implement detailed permissions management specifically for data assets, rows, and columns while maintaining unified access control and data governance across multiple AWS accounts and organizational boundaries.

In this case, by using the new integrated data management foundation, it becomes possible to integrate individually designed and built data foundations, improving both efficiency and functionality. A consistent data flow from data sources to the data platform and then to individual applications is realized, enabling flexible data utilization centered on the data lake. Linkage with each application can also be easily realized from the data lake, providing expandability for future data utilization.

This SHIPS migration case is a comprehensive approach using the solution elements of the JuBuRaw foundation and is expected to serve as a reference model for future system migrations. It’s expected to achieve both service quality improvement and operational cost reduction.

## Future vision and next steps

Based on the data management foundation they’ve built, Furuno Electric aims to further expand and deepen data utilization. As part of their plan to continue and expand digital transformation, they’re currently starting with the migration of SHIPS, but plan to gradually migrate other IoT-related services (such as FOP, FWC, and Ichidake) to the new data management foundation in the future. This is expected to further strengthen the foundation for company-wide data utilization and enhance synergies between services.

Continuous enhancement of secure data sharing and access control is also essential. With the increase in data and expansion of utilization scope, the importance of security and access control will further increase. They’ll optimize the balance between data protection and utilization while incorporating practices accumulated through operations.

Additionally, Furuno Electric is exploring the expansion of their data management capabilities to [Amazon SageMaker](https://aws.amazon.com/sagemaker/), specifically using [Amazon SageMaker Catalog](https://aws.amazon.com/sagemaker/catalog/) integrated with Amazon DataZone. This integration will enable them to seamlessly extend their existing data analytics governance workflows into [artificial intelligence and machine learning](https://aws.amazon.com/training/learn-about/machine-learning/) (AI/ML) workloads. By applying the same data discovery, data sharing, and access control foundation across both data analytics and AI model development, they can accelerate the development of new AI-powered services. The unified governance framework will also provide secure and efficient AI adoption throughout the organization.

Through these initiatives, Furuno Electric is realizing their company motto of “making the invisible visible” in the field of data business as well. The integrated data platform JuBuRaw isn’t just an integration of technical foundations but serves as a foundation to support organizational culture transformation and the creation of new business models. As seen in the SHIPS migration case, using this foundation not only enhances existing services but also expands possibilities for new data utilization.

Through building a data foundation that can flexibly respond to business growth and changes while using a cloud-based environment, Furuno Electric has successfully led their digital transformation. They’ll continue to provide new value to customers through the democratization of marine data and accelerate the transition to data-driven business.

This case serves as a reference for many manufacturing companies promoting data utilization, showing that approaches from both technical and organizational perspectives are key to success. As Furuno Electric’s initiatives demonstrate, data democratization and effective utilization play an important role in the digital transformation of manufacturing.

* * *

### About the Authors

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/akira.png)**Akira Mikami** is a technical expert who played a central role in the FURUNO Data Platform (JuBuRaw) Construction Project at Furuno Electric Co., Ltd. Specializing in data platform construction and architecture, he led the implementation of cloud solutions utilizing AWS. He contributed to achieving efficient data management and strengthening team collaboration, leading the project to success.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/Furuno-Author2.png)**Junpei Ozono** is a Sr. Go-to-market (GTM) Data & AI solutions architect at Amazon Web Services (AWS) in Japan. He drives technical market creation for data and AI solutions while collaborating with global teams to develop scalable GTM motions. He guides organizations in designing and implementing innovative data-driven architectures powered by AWS services, helping customers accelerate their cloud transformation journey through modern data and AI solutions. His expertise spans across modern data architectures including data mesh, data lakehouse, and generative AI, so customers can build scalable and innovative solutions on Amazon Web Services (AWS).

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/nishida.png)**Mitsuhiko Nishida** is an Enterprise Solutions Architecture Automotive & Manufacturing Group Solutions Architect at Amazon Web Services (AWS) in Japan. He serves as a field Solutions Architect for manufacturing customers, helping them solve their business challenges. With expertise in generative AI and manufacturing IT, he guides the design and implementation of innovative solutions leveraging cutting-edge technologies. He supports manufacturing customers in building efficient architecture powered by AWS services to accelerate their cloud transformation journey and contribute to their digital transformation initiatives.","_This is a guest post authored by Akira Mikami, a technical expert at Furuno Electric. The content and opinions in this post are those of the third-party author and AWS is not responsible for the cont...","[""api"",""aws"",""data-analysis"",""python"",""sensors"",""tensorflow""]"
6425dcf3-15fc-425c-9638-8c6d9c67df63,6dc56fee7888f36366befd206bf3d23a3ff830fc,161e6a7a-f07a-4953-95ba-77ccee004a53,Geospatial data lakes with Amazon Redshift,https://aws.amazon.com/blogs/big-data/geospatial-data-lakes-with-amazon-redshift/,"Data lake architectures help organizations offload data from premium storage systems without losing the ability to query and analyze the data. This architecture can be useful for geospatial data, where builders might have terabytes of infrequently accessed data in their databases that they want to cost-effectively maintain. However, this requires for their data lake query engine to support geographic information systems (GIS) data types and functions.

[Amazon Redshift](https://aws.amazon.com/redshift/) supports [querying spatial data](https://docs.aws.amazon.com/redshift/latest/dg/geospatial-overview.html), including the GEOMETRY and GEOGRAPHY data types and functions that are used in querying GIS systems. Additionally, Amazon Redshift lets you query geospatial data both in your data lakes on Amazon S3 and your Redshift data warehouse, giving you the choice of how you can access your data. Additionally, [AWS Lake Formation](https://aws.amazon.com/lake-formation/) and support for [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM) in Esri’s [ArcGIS Pro](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview) gives you a way to securely bridge data between your geospatial data lakes and map visualization tools. You can set up, manage, and secure geospatial data lakes in the cloud with a few clicks.

In this post, we walk through how to set up a geospatial data lake using Lake Formation and query the data with ArcGIS Pro using [Amazon Redshift Serverless](https://aws.amazon.com/redshift/redshift-serverless/).

## Solution overview

In our example, a county public health department has used Lake Formation to secure their data lake that contains public health information (PHI) data. Epidemiologists within the county want to create a map for the clinics providing vaccination for their communities. The county’s GIS analysts need access to the data lake to create the required maps without being able to access the PHI data.

This solution uses Lake Formation tags to allow column-level access in the database to the public information that includes the clinic names, addresses, zip codes, and longitude/latitude coordinates without allowing access to the PHI data within the same tables. We use Redshift Serverless and [Amazon Redshift Spectrum](https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html) to access this data from ArcGIS Pro, a GIS mapping software from Esri, an AWS Partner.

The following diagram shows the architecture for this solution.

![End-to-end architecture showing ArcGIS Pro data integration with AWS analytics services through Redshift connector](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-01.png)

The following is a sample schema for this post.

`Description`

`Column Name`

`Geoproperty Tag`

`Patient ID`

`patient_id`

`No`

`Clinic ID`

`clinic_id`

`Yes`

`Address of Clinic`

`clinic_address`

`Yes`

`Clinic Zip Code`

`clinic_zip`

`Yes`

`Clinic City`

`clinic_city`

`Yes`

`First Name Patient`

`first_name`

`No`

`Last Name Patient`

`last_name`

`No`

`Patient Address`

`patient_address`

`No`

`Patient Zip Code`

`patient_zip`

`No`

`Vaccination Type`

`vaccination_type`

`No`

`Latitude of Clinic`

`clinic_lat`

`Yes`

`Longitude of Clinic`

`clinic_long`

`Yes`

In the following sections, we walk through the steps to set up the solution:

1.  Deploy the solution infrastructure using [AWS CloudFormation](http://aws.amazon.com/cloudformation).
2.  Upload a CSV with sample data to an [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3) bucket and run an [AWS Glue](https://aws.amazon.com/glue) crawler to crawl the data.
3.  Set up Lake Formation permissions.
4.  Configure the Amazon Redshift Query Editor v2.
5.  Set up the schemas in Amazon Redshift.
6.  Create a view in Amazon Redshift.
7.  Create a local database user in ArcGIS Pro.
8.  Connect ArcGIS Pro to the Redshift database.

## Prerequisites

You should have the following prerequisites:

*   An AWS account
*   [Lake Formation enabled in your target AWS Region](https://docs.aws.amazon.com/lake-formation/latest/dg/initial-lf-config.html)
*   Familiarity with Lake Formation and setting permissions on tables
*   [ArcGIS Pro](https://pro.arcgis.com/en/pro-app/latest/get-started/install-and-sign-in-to-arcgis-pro.htm)
*   Network connectivity from the ArcGIS Pro client to the virtual private cloud (VPC) where Amazon Redshift resources will be deployed using either [VPN](https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html) or [AWS Direct Connect](https://aws.amazon.com/directconnect/)

## Set up the infrastructure with AWS CloudFormation

To create the environment for the demo, complete the following steps:

1.  Log in to the [AWS Management Console](http://aws.amazon.com/console) as an AWS account administrator and a Lake Formation data lake administrator—this account needs to be both an account admin and a data lake admin for the template to complete.
2.  Open the AWS CloudFormation console
3.  Choose [Launch Stack](https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create?stackName=geo-sample&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/BDB-3975/infra_setup.yml).

The CloudFormation template creates the following components:

*   **S3 bucket** – `samp-clinic-db-{ACCOUNT_ID}`
*   **AWS Glue database** – `samp-clinical-glue-db`
*   **AWS Glue crawler** – `samp-glue-crawler`
*   **Redshift Serverless workgroup** – `samp-clinical-rs-wg`
*   **Redshift Serverless namespace** – `samp-clinical-rs-ns`
*   **IAM role for Amazon Redshift** – `demo-RedshiftIAMRole-{UNIQUE_ID}`
*   **IAM role for AWS Glue** – `samp-clinical-glue-role`
*   **Lake Formation tag** – `geoproperty`

## Upload a CSV to the S3 bucket and run the AWS Glue crawler

The next step is to create a data lake in our demo environment and then use an AWS Glue crawler to populate the AWS Glue database and update the schema and metadata in the [AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro).

The CloudFormation stack created the S3 bucket we will use as well as the AWS Glue database and crawler. We have provided a [fictious test dataset](https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/BDB-3975/data-with-geocode.csv) that will represent the patient and clinical information. Download the file and complete the following steps:

1.  On the AWS CloudFormation console, open the stack you just launched.
2.  On the **Resources** tab, choose the link to the S3 bucket.
3.  Choose **Upload** and add the CSV file (data-with-geocode.csv), then choose **Upload**.
4.  On the AWS Glue console, choose **Crawlers** in the navigation pane.
5.  Select the crawler you created with the CloudFormation stack and choose **Run**.

The crawler run should only take a minute to complete, and will populate a table named `clinic-sample-s3_ACCOUNT_ID` with a fictious dataset.

6.  Choose **Tables** in the navigation pane and open the table the crawler populated.

You will see that the dataset contains fields that contain PHI and personally identifiable information (PII).

![AWS Glue table 'clinic-sample_s3' schema definition with patient and clinic fields, input/output formats, and database properties](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-02.png)

We now have a database set up and the Data Catalog populated with the schema and metadata we will use for the rest of the demo.

## Set up Lake Formation permissions

In this next set of steps, we demonstrate how to secure PHI data to maintain compliance and empower GIS analysts to work effectively. To secure the data lake, we use AWS Lake Formation. In order to properly set up Lake Formation permissions, we need to gather details on how access to the data lake is established.

The Data Catalog provides metadata and schema information that enables services to access data within the data lake. To access the data lake from ArcGIS Pro, we use the [ArcGIS Pro Redshift connector](https://pro.arcgis.com/en/pro-app/latest/help/data/databases/connect-redshift.htm), which allows a connection from ArcGIS Pro to Amazon Redshift. Amazon Redshift can access the Data Catalog and provide connectivity to the data lake. The CloudFormation template created a Redshift Serverless instance and namespace and an IAM role that we will use to configure this connection. We still need to set up Lake Formation permissions so that GIS analysts can only access publicly available fields and not those containing PHI or PII. We will assign a Lake Formation tag on the columns containing the publicly available information and assign permissions to the GIS analysts to allow access to columns with this tag.

By default, the Lake Formation configuration allows Super access to `IAMAllowedPrinciples`; this is to maintain backward compatibility as detailed in [Changing the default settings for your data lake](https://docs.aws.amazon.com/lake-formation/latest/dg/change-settings.html). To demonstrate a more secure configuration, we will remove this default configuration.

1.  On the Lake Formation console, choose **Administration** in the navigation pane.
2.  In the **Data Catalog settings** section, make sure **Use only IAM access control for new databases** and **Use only IAM access control for new tables in new databases** are unchecked.

![AWS Data Catalog settings interface showing unchecked IAM-only access control options for new databases and tables](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-03.png)

3.  In the navigation pane, under **Permissions**, choose **Data permissions**.
4.  Select `IAMAllowedPrincipals` and choose **Revoke**.
5.  Choose **Tables** in the navigation pane.
6.  Open the table `clinic-sample-s3_ACCOUNT_ID` and choose **Edit schema**.
7.  Select the fields beginning with clinic\_ and choose **Edit LF-Tags**.
8.  The CloudFormation stack created a Lake Formation tag named `geoproperty`. Assign `geoproperty` as the key and true for the value on all the `clinic_` fields, then choose **Save**.

Next, we need to grant the Amazon Redshift IAM role permission to access fields tagged with `geoproperty = true`.

9.  Choose **Data lake permissions**, then choose **Grant**.
10.  For the IAM role, choose `demo-RedshiftIAMRole-UNIQUE_ID`.
11.  Select `geoproperty` for the key and true for the value.
12.  Under **Database permissions**, select **Describe**, and under **Table permissions**, select **Select** and **Describe**.

## Configure the Amazon Redshift Query Editor v2

Next, we need to perform the initial configuration of Amazon Redshift required for database operations. We use an [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) secret created by the template to make sure password access is managed securely in accordance with AWS best practices.

1.  On the Amazon Redshift console, choose **Query editor v2**.
2.  When you first start Amazon Redshift, a one-time configuration for the account appears. For this post, leave the options default and choose **Configure account**.

For more information about these options, refer to [Configuring your AWS account](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-getting-started.html).

![Redshift query editor configuration interface with AWS KMS encryption settings and optional S3 bucket path input](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-04.png)

The query editor will require credentials to connect to the serverless instance; these have been created by the template and stored in Secrets Manager.

3.  Select **Other ways to connect**, then select **AWS Secrets Manager**.
4.  For **Secret**, select (`Redshift-admin-credentials`).
5.  Choose **Save**.

![Redshift connection interface displaying IAM Identity Center and AWS Secrets Manager authentication methods with credential selector](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-05.png)

## Set up schemas in Amazon Redshift

An external schema in Amazon Redshift is a feature used to reference schemas that exist in external data sources. For information on creating external schemas, see [External schemas in Amazon Redshift Spectrum](https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-schemas.html). We use an external schema to provide access to the data lake in Amazon Redshift. From ArcGIS Pro, we will connect to Amazon Redshift to access the geospatial data.

The IAM role used in the creation of the external schema needs to be associated with the Redshift namespace. This has already been set up by the CloudFormation template, but it’s a good practice to verify that the role is set up correctly before proceeding.

1.  On the Redshift Serverless console, choose **Namespace configuration** in the navigation pane.
2.  Choose the namespace (`sample-rs-namespace`).

![Amazon Redshift Serverless console displaying namespace configuration with status, workgroup and creation details](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-06.png)

On the **Security and encryption** tab, you should see the IAM role created by CloudFormation. If this role or the namespace isn’t present, verify the stack in AWS CloudFormation before proceeding.

3.  Copy the ARN of the role for use in a later step.

![Redshift security configuration panel showing single synchronized IAM role with complete ARN and management options](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-07.png)

4.  Choose **Query data** to return to the query editor.

![Amazon Redshift Serverless interface displaying sample-rs-namespace configuration with management and query data controls](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-08.png)

5.  In the query editor, enter the following SQL command; be sure to replace the example role ARN with your own. This SQL command will create an external schema that uses the same Redshift role associated with our namespace to attach to the AWS Glue database.

```
CREATE EXTERNAL SCHEMA samp_clinic_sch_ext FROM DATA CATALOG
database 'sample-glue-database'
IAM_ROLE 'arn:aws:iam::{ACCOUNT_ID}:role/demo-RedshiftIAMRole-{UNIQUE_ID}';
```

6.  In the query editor, perform a select query on `sample-glue-database`:

`SELECT * FROM ""dev"".""samp_clinic_sch_ext"".""clinic-sample_s3_{ACCOUNT_ID}"";`

Because the associated role has been granted access to columns tagged with `geoproperty = true`, only those fields will be returned, as shown in the following screenshot (the data in this example is fictionalized).

![Query result displaying 20 medical clinics with details like name, address, and coordinates](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-09.png)

7.  Use the following command to create a local schema in Amazon Redshift. The external schema can’t be updated; we will use this local schema to add a geometry field with a Redshift function.

`CREATE SCHEMA samp_clinic_sch_local`

## Create a view in Amazon Redshift

For the data to be viewable from ArcGIS Pro, we will need to create a view. Now that the schemas have been established, we can create the view that can be accessed from ArcGIS Pro.

Amazon Redshift provides many [geospatial functions](https://docs.aws.amazon.com/redshift/latest/dg/geospatial-functions.html) that can be used to create views with fields used by ArcGIS Pro to add points onto a map. We will use one of these functions because the dataset contains latitude and longitude.

Use the following SQL code in the Amazon Redshift Query Editor to create a new view named `clinic_location_view`. Replace {ACCOUNT\_ID} with your own account ID.

```
CREATE
OR REPLACE VIEW ""samp_clinic_sch_local"".""clinic_location_view"" AS
SELECT
    clinic_id as id,
    clinic_lat as lat,
    clinic_long as long,
    ST_MAKEPOINT(long, lat) as geom
FROM
    “dev”.""samp_clinic_sch_ext"".""clinic-sample_s3_{ACCOUNT_ID}""
WITH NO SCHEMA BINDING;
```

The new view that is created under your local schema will have a column named geom containing map-based points that can be used by ArcGIS Pro to add points during map creation. The points in this example are for the clinics providing vaccines. In a real-world scenario, as new clinics are built and their data is added to the data lake, their locations would be added to the map created using this data.

## Create a local database user for ArcGIS Pro

For this demo, we use a database user and group to provide access for ArcGIS Pro clients. Enter the following SQL code into the Amazon Redshift Query Editor to create a database user and group:

```
CREATE USER dbuser with PASSWORD ‘SET_PASSWORD_HERE’;
CREATE GROUP esri_developer_group;
ALTER GROUP esri_developer_group ADD USER dbuser;
```

After the commands are complete, use the following code to grant permissions to the group:

```
GRANT USAGE ON SCHEMA samp_clinic_sch_local TO GROUP esri_developer_group;
ALTER DEFAULT PRIVILEGES IN SCHEMA samp_clinic_sch_local GRANT SELECT ON TABLES TO GROUP esri_developer_group;
GRANT SELECT ON ALL TABLES IN SCHEMA samp_clinic_sch_local TO GROUP esri_developer_group;
```

## Connect ArcGIS Pro to the Redshift database

In order to add the database connection to ArcGIS Pro, you need the endpoint for the Redshift Serverless workgroup. You can access the endpoint information on the `sample-rs-wg` workgroup details page on the Redshift Serverless console. The Redshift namespaces and workgroups are listed by default, as shown in the following screenshot.

![Amazon Redshift Serverless namespace and workgroup status dashboard with performance metrics](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-10.png)

You can copy the endpoint in the **General information** section. This endpoint will need to modified; the :5439/dev will need to be removed when configuring the connector in ArcGIS Pro.

![Amazon Redshift Serverless workgroup details showing configuration and connection information](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-11.png)

1.  Open ArcGIS Pro with the project file you want to add the Redshift connection to.

Make sure the [Amazon Redshift ODBC connector](https://docs.aws.amazon.com/redshift/latest/mgmt/odbc20-install-config-win.html) has already been installed; this is required in order to make the connection.

2.  On the menu, choose **Insert** and then **Connections**, **Database**, and **New Database Connection**.
3.  For **Database Platform**, choose **Amazon Redshift**.
4.  For **Server**, insert the endpoint you copied (remove everything following `.com` from the endpoint).
5.  For **Database**, choose your database.

![Amazon Redshift Serverless connection settings with server, authentication, and database fields](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-12.png)

If your ArcGIS Pro client doesn’t have access to the endpoint, you will receive an error during this step. A network path must exist between the ArcGIS Pro client and the Redshift Serverless endpoint. You can set up the network path with [Direct Connect](https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html), [AWS Site-to-Site VPN](https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html), or [AWS Client VPN](https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/what-is.html). Although it’s not recommended for security reasons, you can also configure Amazon Redshift with a [publicly available endpoint](https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-console-workgroups.html#serverless-workgroup-describe). Be sure you consult your security and network teams for best practices and policy guidance before allowing public access to your Redshift Serverless instance.

If a network path exists and you’re having issues connecting, verify the security group rules allow communication inbound from your ArcGIS Pro subnet over the port your Redshift Serverless instance is running on. The default port is 5439, but you can configure a range of ports depending on your environment; see [Connecting to Amazon Redshift Serverless](https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-connecting.html) for more information.

If connectivity is successful, ArcGIS Pro will add the Amazon Redshift connection under **Connection File Name**.

1.  Choose **OK**.
2.  Choose the connection to display the view that was created to include geometry (`clinic_location_view`).
3.  Choose (right-click) the view and choose **Add To Current Map**.

ArcGIS Pro will add the points from the view onto the map. The final map displayed has the symbology edited to use red crosses to represent the clinics instead of dots.

![Professional GIS interface showing Houston metropolitan vaccination clinics with topographic base map, toolbars, and database connectivity](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/bdb-3975-arc-red-13.png)

## Clean up

After you have finished the demo, complete the following steps to clean up your resources:

1.  On the Amazon S3 console, open the bucket created by the CloudFormation stack and delete the `data-with-geocode.csv` file.
2.  On the AWS CloudFormation console, delete the demo stack to remove the resources it created.

## Conclusion

In this post, we reviewed how to set up Redshift Serverless to use geospatial data contained within a data lake to enhance maps in ArcGIS Pro. This technique helps builders and GIS analysts use available datasets in data lakes and transform it in Amazon Redshift to further enrich the data before presenting it on a map. We also showed how to secure a data lake using Lake Formation, crawl a geospatial dataset with AWS Glue, and visualize the data in ArcGIS Pro.

For additional best practices for storing geospatial data in Amazon S3 and querying it with Amazon Redshift, see [How to partition your geospatial data lake for analysis with Amazon Redshift](https://aws.amazon.com/blogs/publicsector/how-partition-geospatial-data-lake-analysis-amazon-redshift/). We invite you to leave feedback in the comments section.

* * *

### About the authors

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/jeremy-spell-photo-100x133.jpg)**Jeremy Spell** is a Cloud Infrastructure Architect working with Amazon Web Services (AWS) Professional Services. He enjoys architecting and building solutions for customers. In his free time Jeremy makes Texas style BBQ, and spends time with his family and church community.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/13/jeff-demuth.jpg)**Jeff Demuth** is a solutions architect who joined Amazon Web Services (AWS) in 2016. He focuses on the geospatial community and is passionate about geographic information systems (GIS) and technology. Outside of work, Jeff enjoys traveling, building Internet of Things (IoT) applications, and tinkering with the latest gadgets.","Data lake architectures help organizations offload data from premium storage systems without losing the ability to query and analyze the data. This architecture can be useful for geospatial data, wher...","[""api"",""aws"",""data-analysis"",""database"",""python"",""tensorflow""]"
307ad8f9-e65d-4ddd-a8a5-0e75be9c7a55,4b81fcd695358ca9292f1e795168017194b8e7c4,161e6a7a-f07a-4953-95ba-77ccee004a53,Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio,https://aws.amazon.com/blogs/big-data/develop-and-monitor-a-spark-application-using-existing-data-in-amazon-s3-with-amazon-sagemaker-unified-studio/,"Organizations face significant challenges managing their big data analytics workloads. Data teams struggle with fragmented development environments, complex resource management, inconsistent monitoring, and cumbersome manual scheduling processes. These issues lead to lengthy development cycles, inefficient resource utilization, reactive troubleshooting, and difficult-to-maintain data pipelines.These challenges are especially critical for enterprises processing terabytes of data daily for business intelligence (BI), reporting, and machine learning (ML). Such organizations need unified solutions that streamline their entire analytics workflow.

The next generation of [Amazon SageMaker](https://aws.amazon.com/sagemaker/) with [Amazon EMR](https://aws.amazon.com/emr/) in [Amazon SageMaker Unified Studio](https://aws.amazon.com/sagemaker/unified-studio/) addresses these pain points through an integrated development environment (IDE) where data workers can develop, test, and refine Spark applications in one consistent environment. [Amazon EMR Serverless](https://aws.amazon.com/emr/serverless/) alleviates cluster management overhead by dynamically allocating resources based on workload requirements, and built-in monitoring tools help teams quickly identify performance bottlenecks. Integration with [Apache Airflow](https://airflow.apache.org/) through [Amazon Managed Workflows for Apache Airflow](https://aws.amazon.com/managed-workflows-for-apache-airflow/) (Amazon MWAA) provides robust scheduling capabilities, and the pay-only-for-resources-used model delivers significant cost savings.

In this post, we demonstrate how to develop and monitor a Spark application using existing data in [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3) using SageMaker Unified Studio.

## Solution overview

This solution uses SageMaker Unified Studio to execute and oversee a Spark application, highlighting its integrated capabilities. We cover the following key steps:

1.  Create an EMR Serverless compute environment for interactive applications using SageMaker Unified Studio.
2.  Create and configure a Spark application.
3.  Use [TPC-DS](https://www.tpc.org/tpcds/) data to build and run the Spark application using a [Jupyter](https://jupyter.org/) notebook in SageMaker Unified Studio.
4.  Monitor application performance and schedule recurring runs with Amazon MWAA integrated.
5.  Analyze results in SageMaker Unified Studio to optimize workflows.

## Prerequisites

For this walkthrough, you must have the following prerequisites:

*   **An AWS account** – If you don’t have an account, you can [create one](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/).
*   **A SageMaker Unified Studio domain** – For instructions, refer to [Create an Amazon SageMaker Unified Studio domain – quick setup](https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/create-domain-sagemaker-unified-studio-quick.html).
*   **A demo project** – Create a demo project in your SageMaker Unified Studio domain. For instructions, see [Create a project](https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/getting-started-create-a-project.html). For this example, we choose **All capabilities** in the project profile section.

## Add EMR Serverless as compute

Complete the following steps to create an EMR Serverless compute environment to build your Spark application:

1.  In SageMaker Unified Studio, open the project you created as a prerequisite and choose **Compute**.
2.  Choose **Data processing**, then choose **Add compute**.
3.  Choose **Create new compute resources**, then choose **Next**.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-EMR-Add-Compute-1-New.png)

4.  Choose **EMR Serverless**, then choose **Next**.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-EMR-Serverless-2-New.png)

5.  For **Compute name**, enter a name.
6.  For **Release label**, choose **emr-7.5.0**.
7.  For **Permission mode**, choose **Compatibility**.
8.  Choose **Add compute**.

It takes a few minutes to spin up the EMR Serverless application. After it’s created, you can view the compute in SageMaker Unified Studio.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/07/BDB-5127-Compute-3-1.png)

The preceding steps demonstrate how you can set up an Amazon EMR Serverless application in SageMaker Unified Studio to run interactive PySpark workloads. In subsequent steps, we build and monitor Spark applications in an interactive JupyterLab workspace.

## Develop, monitor, and debug a Spark application in a Jupyter notebook within SageMaker Unified Studio

In this section, we build a Spark application using the TPC-DS dataset within SageMaker Unified Studio. With [Amazon SageMaker Data Processing](https://aws.amazon.com/sagemaker/data-processing/), you can focus on transforming and analyzing your data without managing compute capacity or open source applications, saving you time and reducing costs. SageMaker Data Processing provides a unified developer experience from Amazon EMR, [AWS Glue](https://aws.amazon.com/glue), [Amazon Redshift](http://aws.amazon.com/redshift), [Amazon Athena](http://aws.amazon.com/athena), and Amazon MWAA in a single notebook and query interface. You can automatically provision your capacity on Amazon EMR on [Amazon Elastic Compute Cloud](http://aws.amazon.com/ec2) (Amazon EC2) or EMR Serverless. Scaling rules manage changes to your compute demand to optimize performance and runtimes. Integration with Amazon MWAA simplifies workflow orchestration by alleviating infrastructure management needs. For this post, we use EMR Serverless to read and query the TPC-DS dataset within a notebook and run it using Amazon MWAA.

Complete the following steps:

1.  Upon completion of the previous steps and prerequisites, navigate to SageMaker Studio and open your project.
2.  Choose **Build** and then **JupyterLab**.

The notebook takes about 30 seconds to initialize and connect to the space.

3.  Under **Notebook**, choose **Python 3 (ipykernel)**.
4.  In the first cell, next to **Local Python**, choose the dropdown menu and choose **PySpark**.
5.  Choose the dropdown menu next to **Project.Spark** and choose **EMR-S Compute**.
6.  Run the following code to develop your Spark application. This example reads a 3 TB TPC-DS dataset in Parquet format from a publicly accessible S3 bucket:

```
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store/"").createOrReplaceTempView(""store"")
```

After the Spark session starts and execution logs start to populate, you can explore the Spark UI and driver logs to further debug and troubleshoot Spark progra![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-UI-Driver-4.jpeg) The following screenshot shows an example of the Spark UI. ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-SparkUI-5.jpeg) The following screenshot shows an example of the driver logs. ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/03/BDB-5127-Spark-Driver-6.jpeg) The following screenshot shows the **Executors** tab, which provides access to the driver and executor logs. ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Executors-7.jpeg)

7.  Use the following code to read some more TPC-DS datasets. You can create temporary views and use the Spark UI to see the files being read. Refer to the appendix at the end of this for details on using the TPC-DS dataset within your buckets.

```
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/item/"").createOrReplaceTempView(""item"")
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store_sales/"").createOrReplaceTempView(""store_sales"")
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/date_dim/"").createOrReplaceTempView(""date_dim"")
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/customer/"").createOrReplaceTempView(""customer"")
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/catalog_sales/"").createOrReplaceTempView(""catalog_sales"")
spark.read.parquet(""s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/web_sales/"").createOrReplaceTempView(""web_sales"")
```

In each cell of your notebook, you can expand **Spark Job Progress** to view the stages of the job submitted to EMR Serverless for a specific cell. You can see the time taken to complete each stage. In addition, if a failure occurs, you can examine the logs, making troubleshooting a seamless experience.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Spark-Job-Progress-8.jpeg)

Because the files are partitioned based on date key column, you can observe that Spark runs parallel tasks for reads.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-SparkJobs-9.jpeg)

8.  Next, get the count across the date time keys on data that is partitioned based on the time key using the following code:

```
select count(1), ss_sold_date_sk from store_sales group by ss_sold_date_sk order by ss_sold_date_sk
```

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Notebook-Block-10.jpg)

## Monitor jobs in the Spark UI

On the **Jobs** tab of the Spark UI, you can see a list of complete or actively running jobs, with the following details:

*   The action that triggered the job
*   The time it took (for this example, 41 seconds, but timing will vary)
*   The number of stages (2) and tasks (3,428); these are for reference and specific to this specific example

You can choose the job to view more details, particularly around the stages. Our job has two stages; a new stage is created whenever there is a shuffle. We have one stage for the initial reading of each dataset, and one for the aggregation. ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-JobRun-11.gif) In the following example, we run some TPC-DS SQL statements that are used for performance and benchmarks:

```
 with frequent_ss_items as
 (select substr(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt
  from store_sales, date_dim, item
  where ss_sold_date_sk = d_date_sk
    and ss_item_sk = i_item_sk
    and d_year in (2000, 2000+1, 2000+2,2000+3)
  group by substr(i_item_desc,1,30),i_item_sk,d_date
  having count(*) >4),
 max_store_sales as
 (select max(csales) tpcds_cmax
  from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales
        from store_sales, customer, date_dim
        where ss_customer_sk = c_customer_sk
         and ss_sold_date_sk = d_date_sk
         and d_year in (2000, 2000+1, 2000+2,2000+3)
        group by c_customer_sk) x),
 best_ss_customer as
 (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales
  from store_sales, customer
  where ss_customer_sk = c_customer_sk
  group by c_customer_sk
  having sum(ss_quantity*ss_sales_price) > (95/100.0) *
    (select * from max_store_sales))
 select sum(sales)
 from (select cs_quantity*cs_list_price sales
       from catalog_sales, date_dim
       where d_year = 2000
         and d_moy = 2
         and cs_sold_date_sk = d_date_sk
         and cs_item_sk in (select item_sk from frequent_ss_items)
         and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer)
      union all
      (select ws_quantity*ws_list_price sales
       from web_sales, date_dim
       where d_year = 2000
         and d_moy = 2
         and ws_sold_date_sk = d_date_sk
         and ws_item_sk in (select item_sk from frequent_ss_items)
         and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer))) x
```

You can monitor your Spark job in SageMaker Unified Studio using two methods. Jupyter notebooks provide basic monitoring, showing real-time job status and execution progress. For more detailed analysis, use the Spark UI. You can examine specific stages, tasks, and execution plans. The Spark UI is particularly useful for troubleshooting performance issues and optimizing queries. You can track estimated stages, running tasks, and task timing details. This comprehensive view helps you understand resource utilization and track job progress in depth.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Spark-TPCDS-12.gif)

In this section, we explained how you can EMR Serverless compute in SageMaker Unified Studio to build an interactive Spark application. Through the Spark UI, the interactive application provides fine-grained task-level status, I/O, and shuffle details, as well as links to corresponding logs of the task for this stage directly from your notebook, enabling a seamless troubleshooting experience.

## Clean up

To avoid ongoing charges in your AWS account, delete the resources you created during this tutorial:

1.  Delete the connection.
2.  Delete the EMR job.
3.  Delete the EMR output S3 buckets.
4.  Delete the Amazon MWAA resources, such as workflows and environments.

## Conclusion

In this post, we demonstrated how the next generation of SageMaker, combined with EMR Serverless, provides a powerful solution for developing, monitoring, and scheduling Spark applications using data in Amazon S3. The integrated experience significantly reduces complexity by offering a unified development environment, automatic resource management, and comprehensive monitoring capabilities through Spark UI, while maintaining cost-efficiency through a pay-as-you-go model. For businesses, this means faster time-to-insight, improved team collaboration, and reduced operational overhead, so data teams can focus on analytics rather than infrastructure management.

To get started, explore the [Amazon SageMaker Unified Studio User Guide](https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/what-is-sagemaker-unified-studio.html), set up a project in your AWS environment, and discover how this solution can transform your organization’s data analytics capabilities.

## Appendix

In the following sections, we discuss how to run a workload on a schedule and provide details about the TPC-DS dataset for building the Spark application using EMR Serverless.

### Run a workload on a schedule

In this section, we deploy a JupyterLab notebook and create a workflow using Amazon MWAA. You can use workflows to orchestrate notebooks, querybooks, and more in your project repositories. With workflows, you can define a collection of tasks organized as a directed acyclic graph (DAG) that can run on a user-defined schedule.Complete the following steps:

1.  In SageMaker Unified Studio, choose **Build**, and under **Orchestration**, choose **Workflows**.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Workflow-13.jpg)

2.  Choose **Create Workflow in Editor**.

You will be redirected to the JupyterLab notebook with a new DAG called `untitled.py` created under the `/src/workflows/dag` folder.

3.  We rename this notebook to `tpcds_data_queries.py`.
4.  You can reuse the existing template with the following updates:
    1.  Update line 17 with the schedule you want your code to run.
    2.  Update line 26 with your `NOTEBOOK_PATH`. This should be in `src/`<notebook\_name>`.ipynb`. Note the name of the automatically generated `dag_id`; you can name it based on your requirements.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Spark-TPCDS-DagNotebk-14.jpg)

5.  Choose **File** and **Save notebook.**

To test, you can trigger a manual run of your workload.

6.  In SageMaker Unified Studio, choose **Build**, and under **Orchestration**, choose **Workflows**.
7.  Choose your workflow, then choose **Run**.

You can monitor the success of your job on the **Runs** tab.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-AirflowRun-15.jpeg)

To debug your notebook job by accessing the Spark UI within your Airflow job console, you must use [EMR Serverless Airflow Operators](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/using-airflow.html) to submit your job. The link is available on the **Details** tab of your query.

This option has the following key limitations: it’s not available for Amazon EMR on EC2, and SageMaker notebook job operators don’t work.

You can configure the operator to generate one-time links to the application UIs and Spark stdout logs by passing `enable_application_ui_links=True` as a parameter. After the job starts running, these links are available on the **Details** tab of the relevant task. If `enable_application_ui_links=False`, then the links will be present but grayed out.

Make sure you have the `emr-serverless:GetDashboardForJobRun` [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM) permissions to generate the dashboard link.

Open the Airflow UI for your job. The Spark UI and history server dashboard options are visible on the **Details** tab, as shown in the following screenshot.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-AirflowUI-16.jpeg)

The following screenshot shows the **Jobs** tab of the Spark UI.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/26/BDB-5127-Airflow-SparkUI-17.jpeg)

### Use the TPC-DS dataset to build the Spark application using EMR Serverless

To use the TPC-DS dataset to run the Spark application against a dataset in an S3 bucket, you need to copy the TPC-DS dataset into your S3 bucket:

1.  Create a new S3 bucket in your test account if needed. In the following code, replace `$YOUR_S3_BUCKET` with your S3 bucket name. We suggest you export `YOUR_S3_BUCKET` as an environment variable:

```
<Your bucket name>
```

2.  Copy the TPC-DS source data as input to your S3 bucket. If it’s not exported as an environment variable, replace `$YOUR_S3_BUCKET` with your S3 bucket name:

```
aws s3 sync s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/ s3://$YOUR_S3_BUCKET/blog/BLOG_TPCDS-TEST-3T-partitioned/
```

* * *

### About the Authors

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2023/10/11/Amit-Maindola.jpg)**Amit Maindola** is a Senior Data Architect focused on data engineering, analytics, and AI/ML at Amazon Web Services. He helps customers in their digital transformation journey and enables them to build highly scalable, robust, and secure cloud-based analytical solutions on AWS to gain timely insights and make critical business decisions.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2022/07/08/image045.jpg)**Abhilash** is a senior specialist solutions architect at Amazon Web Services (AWS), helping public sector customers on their cloud journey with a focus on AWS Data and AI services. Outside of work, Abhilash enjoys learning new technologies, watching movies, and visiting new places.","Organizations face significant challenges managing their big data analytics workloads. Data teams struggle with fragmented development environments, complex resource management, inconsistent monitorin...","[""api"",""aws"",""database"",""python"",""react"",""spark""]"
65e42edd-3176-4777-9a61-31e7d4797038,ba37ce5135c4fbcef966b9cbd19062bf01fefdb5,161e6a7a-f07a-4953-95ba-77ccee004a53,Perform per-project cost allocation in Amazon SageMaker Unified Studio,https://aws.amazon.com/blogs/big-data/perform-per-project-cost-allocation-in-amazon-sagemaker-unified-studio/,"[Amazon SageMaker Unified Studio](https://aws.amazon.com/sagemaker/unified-studio/?nc1=h_ls) is a single data and AI development environment where you can find and access your data and act on it using AWS resources for SQL analytics, data processing, model development, and generative AI application development.

SageMaker Unified Studio is part of the next generation of [Amazon SageMaker](https://aws.amazon.com/sagemaker/?nc1=h_ls). SageMaker brings together AWS artificial intelligence and machine learning (AI/ML) and analytics capabilities and delivers an integrated experience for analytics and AI with unified access to data.

With SageMaker Unified Studio, you can create domains and projects, providing a single interface to build, deploy, execute, and monitor end-to-end workflows. This approach helps drive collaboration across teams and facilitates agile development.

SageMaker Unified Studio implements [resource tagging](https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/what-are-tags.html) when AWS resources are provisioned. You can use these tags to track and allocate costs for the various resources created as part of the domains and projects within SageMaker Unified Studio.

This post demonstrates how to perform cost allocation using these resource tags, so finance analysts and business analysts can implement and follow Financial Operations (FinOps) best practices to control and track cloud infrastructure costs.

## Solution overview

The following diagram illustrates how tagging works within SageMaker domains.

![High level diagram that illustrates SageMaker Unified Studio entities (domains, projects and environments) are organized and how tags are applied to each of them](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-domain-arch-diagram.png)

Before reviewing the implementation details, let’s explore several key SageMaker concepts: domain, project, project profile, and environment blueprint. For more information, refer to the [SageMaker Unified Studio Administrator Guide](https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/what-is-sagemaker-unified-studio.html).

*   **Domain** – A domain is an organizing entity created by an administrator. Administrators assign users to domains to enable collaboration using similar tools, assets, and resources. A domain can represent a business organization or a business unit containing people who collaborate and share resources. After creating a domain, administrators share the URL with users to access the portal.
*   **Projects** – Projects exist within each domain. A project provides a boundary where users can collaborate on a business use case. Users can create and share data, computing, and other resources within projects.
*   **Project profile** – When you create a project, you must select a project profile. A project profile is a template that governs infrastructure for the project, simplifying project creation with preconfigured settings and resources ready for use.
*   **Environment blueprints** – Environment blueprints are reusable templates for creating environments. They define settings for resource deployment and provide information for provisioning. Each blueprint uses an [AWS CloudFormation](http://aws.amazon.com/cloudformation) template to create resources in a repeatable and scalable manner.

For effective cost tracking and allocation, make sure your SageMaker resources have proper tags. You can configure these as [cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html) to group and filter across [AWS Billing and Cost Management](https://aws.amazon.com/aws-cost-management/aws-billing/) tools (such as [AWS Cost Explorer](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/) and [AWS Data Exports](https://docs.aws.amazon.com/cur/latest/userguide/what-is-data-exports.html)).

As of this writing, SageMaker domains support tagging at the blueprint, domain, project, and environment level. When you create projects or add resources within an existing project, the following tags are automatically added to resources through [CloudFormation resource tags](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html), configured for each blueprint stack:

*   **AmazonDataZoneBlueprint** – Type of blueprint corresponding to this blueprint’s CloudFormation template (for example, Tooling)
*   **AmazonDataZoneDomain** – [Amazon DataZone](https://aws.amazon.com/datazone/) domain associated with this CloudFormation template
*   **AmazonDataZoneEnvironment** – Amazon DataZone environment ID associated with this CloudFormation template
*   **AmazonDataZoneProject** – Amazon DataZone project associated with this CloudFormation template

To track costs in SageMaker Unified Studio, you will perform the following steps:

1.  Create a SageMaker domain and project.
2.  Configure cost and billing settings by enabling cost allocation tags.
3.  (Optional) Generate costs for your project.
4.  Track costs using Cost Explorer and Data Exports.

## Prerequisites

This post requires the following configurations in your AWS account:

*   [AWS IAM Identity Center](https://aws.amazon.com/iam/identity-center/) enabled in your organization management account (preferred) or in the member account where you will use SageMaker Unified Studio. For instructions on enabling IAM Identity Center, refer to [Enable IAM Identity Center](https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html).
*   Cost Explorer enabled in your organization management account (preferred) or in the member account where you will use SageMaker Unified Studio. For configuration steps, refer to [Enabling Cost Explorer](https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html).

Either [legacy AWS Cost and Usage Reports (AWS CUR)](https://docs.aws.amazon.com/cur/latest/userguide/cur-query-athena.html) with [Amazon Athena](http://aws.amazon.com/athena) integration or Data Exports configured and integrated with Athena for queries. For setup instructions, refer to [creating Data Exports](https://docs.aws.amazon.com/cur/latest/userguide/dataexports-create.html).

## Create a SageMaker Unified Studio domain and project

Complete the following steps to set up your domain and project:

1.  [Create a SageMaker Unified Studio domain using the Quick setup option](https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/create-domain-sagemaker-unified-studio-quick.html) (recommended for new users) or manual setup.

After domain creation, you will be redirected to the domain overview page.

2.  Choose **Open Unified Studio**.
3.  On the SageMaker Unified Studio console, choose **Create project**.
4.  For **Project profile**, choose **SQL analytics**, then choose **Continue**.

![SageMaker Unified Studio create project wokflow (configuration page)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-create-project-screen.png)

5.  Choose **Continue** to keep the default blueprint parameters.
6.  Review the configuration summary, then choose **Create project**.

![SageMaker Unified Studio create project wokflow (confirmation page)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-create-project-conf.png)

After the project is created, you will be redirected to the project overview page. Record the project ID and domain ID.

![Project details page showing various details such as project id, project name and project IAM role ARN](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-project-details.png)

## Cost and billing configuration

As mentioned earlier, to track costs in SageMaker Unified Studio, you must configure cost allocation tags. Refer to [Organizing and tracking costs using AWS cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html) for more information about this feature.

Complete the following steps:

1.  On the AWS Billing and Cost Management console, under **Cost organization** in the navigation pane, choose **Cost allocation tags**.
2.  Select the following tags and choose **Activate**:
    1.  `AmazonDataZoneDomain`
    2.  `AmazonDataZoneProject`
    3.  `AmazonDataZoneEnvironment`
    4.  `AmazonDataZoneBlueprint`

The `AmazonDataZoneProject` and `AmazonDataZoneDomain` tags correspond to the project and domain ID values you recorded earlier.

![AWS cost allocation tags interface showing the AWS tags that are currently configured as cost allocation tags](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-allocation-tags.jpeg)

Cost allocation tags configuration doesn’t apply retroactively. If you want to monitor costs associated with these tags in the AWS Billing and Cost Management tools before the activation date, you must request a [cost allocation tag backfill](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-allocation-backfill.html). The backfill operation can take several hours to complete.

## Generate costs for the project

This section explains how to generate costs associated with the underlying data backend ([Amazon Redshift](https://aws.amazon.com/pm/redshift) in this case) to examine them using AWS billing tools. You can skip this section if you’re tracking costs on an active project.

To generate costs, we use the table structure used in the Redshift Immersion Labs. Refer to [Create Tables](https://catalog.us-east-1.prod.workshops.aws/workshops/9f29cdba-66c0-445e-8cbb-28a092cb5ba7/en-US/lab2#create-tables) for more details.

To run queries in SageMaker Unified Studio, follow these steps:

1.  In your project, choose **New** and then **Query**.

![Image that shows the query button within the SageMaker Unified Studio project overview page allowing users to open the query editor tool](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-project-query-button.jpeg)

2.  Use the [Amazon Redshift Serverless](https://aws.amazon.com/redshift/redshift-serverless/) compute configured for the project to generate the costs:
    1.  Choose the **Redshift (Lakehouse)** connection.
    2.  Choose the `dev` database.
    3.  Choose the `project` schema.
    4.  Choose **Choose**.

![Image that shows the conection selector available in SageMaker Unified Studio. In this case Redshift LakeHouse connection is selected with dev database and project schema selected underneath](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-query-editor-connection-selector.jpeg)

3.  Copy and execute the SQL statements provided in the following [GitHub repo](https://github.com/awslabs/amazon-redshift-utils/blob/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/10GB/ddl.sql) into the SageMaker Unified Studio query editor to create, load, and validate data on the tables.

![View of the Query editor within the SageMaker Unified Studio portal. Image contains two SQL queries (create tables and COPY data operation)](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-query-editor-query-notebook.png)

After running these steps, you will have generated some Amazon Redshift costs that will be present for further analysis in AWS Billing and Cost Management tools. However, these tools (Cost Explorer and Data Exports) are refreshed least one time every 24 hours, so you might need to wait up to 24 hours before proceeding to the next section.

## Tracking costs in AWS Billing and Cost Management tools

With the cost allocation tags enabled, you can use AWS Billing and Cost Management tools to analyze and track costs, including Cost Explorer and Data Exports. For more information about using these tools, refer to the [AWS Billing and Cost Management User Guide](https://docs.aws.amazon.com/cost-management/latest/userguide/billing-getting-started.html).

### Check costs in Cost Explorer

You can check your SageMaker Unified Studio costs using Cost Explorer. With this tool, you can view and analyze your costs and usage through an interface with pre-built filters and aggregation capabilities for various metrics. For more information, refer to the [Analyzing your costs and usage with AWS Cost Explorer](https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html).

To access Cost Explorer, complete the following steps:

1.  On the [AWS Management Console](http://aws.amazon.com/console), choose your account name in the top right corner and choose **Billing Dashboard**, or search for “Cost Explorer” in the console search bar.
2.  On the Billing Dashboard, choose **Cost Explorer** in the navigation pane.
3.  For first-time users, choose **Launch Cost Explorer** to enable the service.

AWS can take up to 24 hours to prepare your cost data.

4.  To view overall costs per project, configure the following report parameters:
    1.  For **Date Range**, enter your range.
    2.  For **Granularity**, choose **Monthly**.
    3.  For **Dimension**, choose **Tag**.
    4.  For **Tag**, enter your tag (`AmazonDataZoneProject`).

![Image that shows how to group by a particular dimension (tag) in cost explorer](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-explorer-parameters-group-by-tag.png)

The following screenshot shows a sample report.

![AWS cost explorer report showing costs by SageMaker Unified Studio project](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-explorer-cost-by-project.png)

1.  To view different service costs for a specific project, update the following parameters:
    1.  For **Dimension**, choose **Service**.![Image that shows how to group by a particular dimension (service) in cost explorer](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-explorer-parameters-group-by-service.jpeg)
    2.  For **Tag**¸ choose `AmazonDataZoneProject` and choose the value of the project you want to inspect (in this case, `4z9d694nbsnyqx`).

![Image that illustrates how to filter by a specific dimension (tag) and value in cost explorer](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-explorer-parameters-filter-by-tag-1.jpeg)

The results should look similar to the following screenshot.

![AWS cost explorer report showing service costs for a particular SageMaker Unified Studio project](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-cost-explorer-cost-for-project.png)

### Check costs using Data Exports

With Data Exports, you can query your cost and usage in AWS with the maximum flexibility degree compared to other tools such as Cost Explorer. It provides a [comprehensive set of measures and dimensions](https://docs.aws.amazon.com/cur/latest/userguide/table-dictionary-cur2.html#cur2-column-groups) that you can include in the export to create a personalized report. This report is then delivered to [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3) so you can configure it with Athena, so it can be queried using SQL or business intelligence (BI) tools such as [Amazon QuickSight](https://aws.amazon.com/quicksight/?nc1=h_ls).

This post assumes you have already [configured a data export](https://docs.aws.amazon.com/cur/latest/userguide/dataexports-create-standard.html) and you have it integrated with Athena (refer to [Processing data exports](https://docs.aws.amazon.com/cur/latest/userguide/dataexports-processing.html) for more information). For instructions on setting up CUR and Athena integration, refer to [Creating reports](https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html).

#### Check costs by project

Use the following query to check costs by project:

```
SELECT product_servicecode,
    product_product_family,
    resource_tags[ 'user_amazon_data_zone_project' ] as user_amazon_data_zone_project,
    round(sum(line_item_unblended_cost), 2) costs,
    line_item_line_item_description 
FROM ""data_exports"".""data_exportdata""
where resource_tags [ 'user_amazon_data_zone_project' ] != ''
group by product_product_family,
    product_servicecode,
    resource_tags[ 'user_amazon_data_zone_project' ],
    line_item_line_item_description
order by round(sum(line_item_unblended_cost), 2) DESC;
```

Results will look similar to the following screenshot on the Athena console.

![Athena SQL query results when querying cost and usage data from data exports](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-athena-query-data-exports.jpeg)

The preceding query shows your costs grouped by:

*   Project (using tags)
*   Service
*   Product family, which corresponds to the subtype for a given product usage charge (for example, ML Instance for SageMaker, or Managed Storage for Amazon Redshift)

#### Check costs for individual projects

To check costs for a specific SageMaker Unified Studio project (for example, the sample project `4z9d694nbsnyqx` created during this walkthrough), you can use the following query:

```
SELECT product_servicecode,
    product_product_family,
    resource_tags[ 'user_amazon_data_zone_project' ] as user_amazon_data_zone_project,
    round(sum(line_item_unblended_cost), 2) costs,
    line_item_line_item_description 
FROM ""data_exports"".""data_exportdata""
where resource_tags [ 'user_amazon_data_zone_project' ] != ''
and resource_tags [ 'user_amazon_data_zone_project' ] = <provide the project id here>
group by product_product_family,
    product_servicecode,
    resource_tags[ 'user_amazon_data_zone_project' ],
    line_item_line_item_description
order by round(sum(line_item_unblended_cost), 2) DESC;
```

### Monitor costs with Data Exports and QuickSight

If you enabled Athena to work with Data Exports, you can also configure QuickSight to query this data source. With QuickSight, you can create interactive dashboards to track SageMaker costs in SageMaker Unified Studio at scale.

#### Configure access and permissions

To create CUR dashboards in QuickSight, first complete the following steps:

1.  Subscribe to QuickSight and have an author user account. For instructions on subscribing to QuickSight, refer to [Signing up for an Amazon QuickSight subscription](https://docs.aws.amazon.com/quicksight/latest/user/signing-up.html).
2.  Enable access to Athena and your CUR S3 bucket in the **Security & permissions** section of the QuickSight administration console. You need QuickSight administrator permissions to access this console.

![Image shows QuickSight administration console where administrators can edit the AWS services (Athena in this case) that QuickSight is allowed to access](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-quicksight-manage-athena-access.jpeg)

3.  If you’re using [AWS Lake Formation](https://aws.amazon.com/lake-formation/), make sure your QuickSight user is authorized to query the CUR database and table. For more information about granting access in Lake Formation, refer to [Granting permissions on Data Catalog resources](https://docs.aws.amazon.com/lake-formation/latest/dg/granting-catalog-permissions.html).

#### Create a QuickSight dataset

The next step is to create a dataset in QuickSight using a SQL query. For instructions on creating a dataset with SQL, refer to [Using SQL to customize data](https://docs.aws.amazon.com/quicksight/latest/user/adding-a-SQL-query.html). Use the following SQL expression:

```
SELECT product_servicecode,
    product_product_family,
    resource_tags[ 'user_amazon_data_zone_environment' ] as user_amazon_data_zone_environment,
    resource_tags[ 'user_amazon_data_zone_project' ] as user_amazon_data_zone_project,
    resource_tags[ 'user_amazon_data_zone_domain' ] as user_amazon_data_zone_domain,
    line_item_unblended_cost,
    line_item_usage_start_date,
    line_item_line_item_description
FROM ""data_exports"".""data_exportdata""
where resource_tags [ 'user_amazon_data_zone_environment' ] != '' or resource_tags [ 'user_amazon_data_zone_project' ] != ''
```

![Image of QuickSight dataset preparation page. Shows a SQL query that is used to extract data from the data exports previously configured.](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-quicksight-create-sql-dataset.jpeg)

The preceding query includes only cost and usage data that’s tagged with either `user_amazon_data_zone_environment` or `user_amazon_data_zone_project` to focus on SageMaker associated costs. To include other AWS costs, you must modify these filters.

#### Create QuickSight dashboards

Using the authoring capabilities of QuickSight, you can create interactive dashboards where business stakeholders can explore and track costs associated with SageMaker Unified Studio projects. You can use these dashboards to review relevant cost metrics at a glance that are derived from the Data Exports dimensions and metrics included in your dataset, as shown in the following screenshot. For more information about adding visuals to analyses, refer to [Adding visuals to Amazon QuickSight analyses](https://docs.aws.amazon.com/quicksight/latest/user/creating-a-visual.html).

![Example of a QuickSight dashboard consuming data exports cost and usage data. Dashboard contains multiple visuals that illustrate SageMaker Unified Studio costs by project and service](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/BDB-4902-quicksight-sample-dashboard.png)

The preceding example shows a dashboard built using QuickSight connected to a Data Exports dataset. The dashboard contains the following visuals:

*   [KPI visual](https://docs.aws.amazon.com/quicksight/latest/user/kpi.html) showing the current monthly costs for SageMaker Unified Studio along with the month over month (MoM) variation and history
*   [Autonarrative visual](https://docs.aws.amazon.com/quicksight/latest/user/narratives-creating.html) analyzing SageMaker Unified Studio costs (highest) by month
*   [Vertical stacked bar chart](https://docs.aws.amazon.com/quicksight/latest/user/bar-charts.html#create-bar-chart-stacked) showing SageMaker Unified Studio costs by month (grouped by project)
*   [Donut chart](https://docs.aws.amazon.com/quicksight/latest/user/donut-chart.html) showing SageMaker Unified Studio cost by service
*   [Heat map visual](https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html) correlating costs by project ID and service

Using this approach (QuickSight and Data Exports), you can create highly customizable dashboards to explore and monitor your SageMaker Unified Studio costs. Furthermore, you can create automated reports using the [QuickSight reporting feature](https://docs.aws.amazon.com/quicksight/latest/user/sending-reports.html) to send these by email to the relevant stakeholders.

## Clean up

Delete the resources you created as part of this post when you’re done with them to avoid monthly charges. This includes SageMaker resources, created Data Export reports and the QuickSight subscription (in case it was created to visualize costs).

1.  Delete SageMaker resources
    1.  Log in to the SageMaker domain using an admin role.
    2.  Delete the project you created.
    3.  Delete the SageMaker domain.
2.  Delete Data Exports reports
    1.  On the AWS Billing console, in the navigation pane, choose **Cost & Usage Reports**.
    2.  Select the report you want to delete.
    3.  Choose **Delete**.
    4.  Confirm the deletion by choosing **Delete report**.

For more information about managing Data Exports, refer to [Deleting exports](https://docs.aws.amazon.com/cur/latest/userguide/dataexports-delete.html).

3.  Unsubscribe from QuickSight
    NaN.  On the QuickSight console, choose your profile name in the top right corner.
    NaN.  Choose **Manage QuickSight**.
    NaN.  Choose **Account settings**.
    NaN.  At the bottom of the page, choose **Delete your QuickSight account**.
    NaN.  Review the information about data deletion.
    NaN.  Enter `delete` to confirm.
    NaN.  Choose **Delete**.

**IMPORTANT NOTE:** Before unsubscribing, make sure you backed up any dashboards or analyses you want to keep. After deletion, you can’t recover your QuickSight assets. For more information about managing your QuickSight subscription, refer to [Deleting your Amazon QuickSight subscription and closing the account](https://docs.aws.amazon.com/quicksight/latest/user/closing-account.html).

## Conclusion

Managing costs on a unified platform like SageMaker can seem challenging because it aggregates many tools and services with different cost models. In this post, we showed how to use AWS Billing and Cost Management tools to aggregate and categorize costs across the various services used within SageMaker. With this approach, you can monitor and track respective service costs, either in aggregate or focusing on a particular project.

Start taking control of your analytics and ML costs today. With AWS Billing and Cost Management tools with SageMaker, you can:

*   Track and monitor your service costs
*   Break down expenses by project or service
*   Implement efficient back charging mechanisms to the different business units or organizations using SageMaker within your organization

For further reading, refer to [Analyzing your costs and usage with AWS Cost Explorer](https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html) and [Processing Data Exports (using Athena)](https://docs.aws.amazon.com/cur/latest/userguide/dataexports-processing.html).

* * *

### About the authors

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/enriquh_res.jpg)**Enrique Salgado Hernández** is a Senior Specialist Solutions Architect at AWS with more than 10 years of experience working in the cloud. He specializes in designing and implementing large-scale analytics architectures across various industry sectors. He is passionate about working with customers to solve their problems by supporting them during their cloud journey.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/acmanjon_res.jpg)**Angel Conde Manjon** is a Senior EMEA Data & AI PSA, based in Madrid. He previously worked on research related to data analytics and AI in diverse European research projects. In his current role, Angel helps partners develop businesses centered on data and AI.",[Amazon SageMaker Unified Studio](https://aws.amazon.com/sagemaker/unified-studio/?nc1=h_ls) is a single data and AI development environment where you can find and access your data and act on it using...,"[""api"",""aws"",""data-analysis"",""database"",""python"",""tensorflow""]"
dccc886b-a763-4768-846f-c89cc4714e71,84bb1eeebc36650864e7ab5617a430883ccde71d,161e6a7a-f07a-4953-95ba-77ccee004a53,Near real-time baggage operational insights for airlines using Amazon Kinesis Data Streams,https://aws.amazon.com/blogs/big-data/near-real-time-baggage-operational-insights-for-airlines-using-amazon-kinesis-data-streams/,"To provide a seamless travel experience, aviation enterprises must streamline baggage handling to be as efficient as possible. Traditional baggage analytics systems often struggle with adaptability, real-time insights, data integrity, operational costs, and security, limiting their effectiveness in dynamic environments. Real-time analytics can help in several aspects, such as improving staffing decisions, baggage rerouting, payload planning, and predictive maintenance of [Internet of Things](https://aws.amazon.com/what-is/iot/) (IoT) sensors and belt loaders.

In this post, we explore a framework developed by IBM to modernize baggage analytics using [Amazon Web Services](https://aws.amazon.com/) (AWS) managed services such as [Amazon Kinesis Data Streams](https://aws.amazon.com/kinesis/data-streams/), [Amazon DynamoDB Streams](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/streamsmain.html), [Amazon Managed Service for Apache Flink](https://aws.amazon.com/managed-service-apache-flink/), [Amazon QuickSight](https://aws.amazon.com/quicksight), [Amazon Q in QuickSight](https://aws.amazon.com/quicksight/q/), [AWS Glue](https://aws.amazon.com/glue/), [Amazon SageMaker](https://aws.amazon.com/sagemaker/), and [Amazon Aurora](https://aws.amazon.com/rds/aurora/) within a serverless architecture. This approach delivers significant cost savings, enhanced scalability, and improved performance while providing better security and operational efficiency to meet the evolving needs of airlines. Before diving into the solution’s architecture, we first examine the traditional baggage analytics process and the need for modernization.

## **Importance of baggage analytics**

Baggage management is a process that starts at baggage check-in and ends with the passenger claiming their baggage in a happy path scenario. The following figure explains the high-level baggage management process and respective key performance indicators (KPI). The illustration highlights the critical role of payload planning (part 1), baggage loading (part 2), and below wing payload closeout (part 3) in the flight departure process, all of which directly impact the flight on-time departure metric (part 4). Enhancing the KPIs associated with these essential steps is vital for airlines to optimize operations.

![Baggage analytics KPIs](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/bdb-4610-kpis-1.png)

Figure 1: Baggage analytics KPIs

Common KPIs for baggage loading include baggage handling time, turnaround time impact, mishandled baggage rate, baggage accuracy rate, and baggage loading error rate. Similarly, the baggage check-in process plays a crucial role in enhancing the passenger experience. Analyzing variations in this metric across different stations and time periods provides valuable insights for identifying potential bottlenecks and improving efficiency.Airlines can measure performance KPIs using the following business process metrics:

*   **Wait times** – Wait times are the duration that a process step is waiting on an upstream dependency and are an important factor affecting the overall wait time. Analytics can help identify the potential areas (for example, stations, bag rooms, pier locations, belt loaders, or baggage types) where the processes and system can be fine-tuned to improve the overall wait time.
*   **Error rate** – Error rate is the time spent on correcting errors or defects. Within these processes, error rate is usually a result of data inconsistencies across multiple systems, manual data entries because of system unavailability or limited aircraft turn-around time, and inconsistencies between payload planning rules and loading procedures. Analytics can help classify these errors among system availability issues, outdated rules, inconsistent data between systems, and other factors. The classification can help prioritize fine-tuning and removing redundancies across systems, rules, and data.
*   **Rework time** – Rework time is time spent on correcting errors or defects. It can be improved but can’t be avoided, considering last-minute baggage, wheelchairs, ski equipment, and ship or aircraft changes that result in a new payload plan. Analytics can help classify the type, time, and frequency of rework activities across stations, staff members, baggage types, and scenarios related to flight delays and ship changes.
*   **Cycle time** – Cycle time is the time it takes to complete the process. You can improve the payload planning process cycle time by automating the payload distribution process. To do so, you need to identify and improve the time taken by the payload planning, loading, and closeout processes to reduce the complete departure process cycle time. In many cases, you can improve cycle time by adjusting the processes and adding extra resources, such as workforce, or in other cases by introducing automation. Analytics can identify these time-consuming steps and can be extended to use predictive models to apply mitigation strategies.

## **Traditional baggage analytics**

As explained in the following figure, the traditional baggage handling solution uses monolithic databases with several upstream and downstream dependencies. Upstream dependencies include bags, flight and passenger event feeds to subscribe to the real-time changes in flight, checked bags, and passenger itinerary changes. Downstream dependencies include staffing and customer notifications. The core application interfaces include belt loaders, IoT devices, kiosks, handheld scanners, and web applications for monitoring and reporting. The airline typically stores the reports in the operational database referred to in the diagram as baggage handling (relational database), retaining historical data spanning multiple years, and makes them available to all personnel on the airline’s network. The traditional approach to baggage analytics entails nightly processing of data batches into an enterprise data warehouse (EDW) to generate performance metrics related to airlines’ baggage handling processes.

![Traditional baggage analytics](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/bdb-4610-traditional-1.png)

Figure 2: Traditional baggage analytics

## **Need for modernization**

Modernizing baggage analytics is crucial for airlines to achieve growth and enhance operational efficiency. Key factors influencing the modernization are as follows:

*   **Inefficiencies in near real-time decision-making** – Current systems can’t process and analyze data in real time, leading to delayed responses to operational issues. Integration and data silos hinder insights, preventing proactive decision-making on baggage handling, routing, and anomaly detection.
*   **Limitations of traditional ETL solutions** – Legacy extract, transform, and load (ETL) processes are batch-driven, slow, and resource-intensive, making them unsuitable for dynamic airline operations. High maintenance costs and frequent failures reduce system reliability and availability.
*   **Challenges in proactive anomaly detection and resolution during irregular operations** – Airlines struggle to anticipate baggage issues during irregular operations, such as flight delays and weather disruptions. Without predictive analytics, preemptive actions remain a challenge in optimizing staffing, reducing mishandled baggage, and enhancing operational efficiency.

## **Solution**

The modernization of baggage operations must include breaking down the monolithic database into distinct databases based on business capabilities to address performance bottlenecks. Business capabilities can be described as fundamental abilities or competencies that a business possesses and that enable it to achieve its objectives and deliver value to its customers.

As explained in the following figure, the business capabilities for baggage management can be defined as baggage acceptance (check-in), baggage loading, baggage offloading, baggage tracking, baggage mishandling and claims, baggage rerouting, and more. \[part 1\]. The solution proposes Amazon DynamoDB for an operational database across all baggage management capabilities. DynamoDB global tables provide 99.999% availability with near-zero Recovery Time Objective (RTO) and Recovery Point Objective (RPO), which is crucial for mission-critical baggage handling systems. More details related to baggage operational database modernization can be found at [Enhance the reliability of airlines’ mission-critical baggage handling using Amazon DynamoDB](https://aws.amazon.com/blogs/database/enhance-the-reliability-of-airlines-mission-critical-baggage-handling-using-amazon-dynamodb/) in the AWS Database Blog.

The proposed logical solution for baggage operational analytics suggests segregating operational data from historical data, referred to in the diagram as baggage analytics and historical reporting database, to enhance efficiency and alleviate the burden on the operational database \[part 3\].

![Modern baggage analytics](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/bdb-4610-modern-1.png)

Figure 3: Modern baggage analytics

The solution further uses streaming architecture for the ongoing transfer of data from the operational database to the baggage analytics and historical reporting database \[part 2\]. This approach aims to facilitate near real-time analytics.The key features for a robust streaming architecture include:

*   Low-latency processing to enable near real-time updates
*   Scalability and elasticity to handle dynamic workloads efficiently
*   Fault tolerance and durability to promote data reliability with replication
*   The ability for multiple consumers to process the same data in parallel at full speed without bottlenecks or interference
*   Exactly one-time processing to avoid duplication and maintain data integrity
*   Ability to replay messages

### Real-time streaming on AWS Cloud

The solution uses either Kinesis Data Streams or DynamoDB Streams as a viable streaming solution for processing for change data capture (CDC) within milliseconds. For further information, refer to [Streaming options for change data capture](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/streamsmain.html#streamsmain.choose) and [Choose the right change data capture strategy for your Amazon DynamoDB applications](https://aws.amazon.com/blogs/database/choose-the-right-change-data-capture-strategy-for-your-amazon-dynamodb-applications/).

In this architecture, Kinesis Data Streams is selected to enable fan-out to multiple downstream consumers, extended data retention, and integration with [Amazon Managed Service for Apache Flink](https://aws.amazon.com/managed-service-apache-flink/). Amazon Managed Service for Apache Flink performs stateful stream processing—such as windowed aggregation, filtering, and anomaly detection—before passing data to DynamoDB or Aurora for further analytical aggregation and reporting. Although DynamoDB Streams could also have been used, Kinesis Data Streams provides greater flexibility and throughput for the scale of event processing required here. Additionally, Kinesis Data Streams data retention allows message replays for improved reliability and analysis.

### Baggage analytics on AWS Cloud

The solution will use [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) for structured and unstructured data storage and [Amazon Aurora PostgreSQL-Compatible Edition](https://aws.amazon.com/rds/aurora/features/) for relational aggregations. Aurora is well-suited for handling complex aggregations across multiple dimensions (such as month, year, station, and shift) with efficient indexing and SQL functions optimized for reporting. Its relational capabilities support analytical queries needed for performance metrics while providing scalability and efficiency

The following figure explains the high-level cloud architecture for baggage analytics using AWS services.

![Baggage real-time analytic architecture on AWS](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/06/30/bdb-4610-architecture-1.png)

Figure 4: Near real-time baggage analytics architecture on AWS

The solution can support the following analytics:

*   **Interactive and investigative analytics** which can produce charts and graphs and discover patterns and anomalies in the baggage data used by product owners. The solution proposes using Amazon QuickSight, which is an interactive tool. Additionally, the solution proposes Amazon Q in QuickSight for natural language queries using a chat-based interface. Amazon QuickSight can be configured using an AWS Glue crawler to automatically discover and extract metadata from various data stores such as Amazon S3 and Amazon Aurora and catalog it in a centralized repository. Amazon QuickSight can be configured to use Amazon Athena to read the data catalog.
*   **Predictive analytics** used by data scientists involves analyzing historical data to predict future events or behaviors. It uses statistical algorithms and [machine learning](https://aws.amazon.com/ai/machine-learning/) (ML) techniques to forecast outcomes. The proposed solution is to use a SageMaker notebook to perform predictive analytics on baggage data.

## Conclusion

Cloud-based solutions such as Kinesis Data Streams, Athena, and QuickSight revolutionize baggage analytics with scalable, cost-effective infrastructure. By integrating real-time data streaming, analysis, and visualization, they eliminate data silos and enable data-driven decision-making.This modernization optimizes processes, proactively resolving issues to minimize passenger disruptions. Embracing cloud-powered analytics isn’t just a necessity but a strategic step toward greater efficiency, resilience, and customer satisfaction.With this solution, airlines can enhance preemptive issue resolution in baggage operations. Real-time analytics enables better workforce planning, allowing airlines to predict staffing needs at departure and arrival stations, reducing labor costs while ensuring smooth operations. Additionally, data-driven insights help identify inefficiencies during irregular operations, enabling informed decisions for traffic diversion and process optimization.

Check out more [AWS Partners](https://aws.amazon.com/travel-and-hospitality/partner-solutions/) or contact an [AWS Representative](https://pages.awscloud.com/TravelandHospitalityContactUs.html) to know how we can help accelerate your business.

## Further reading

*   [AWS for Travel and Hospitality](https://aws.amazon.com/travel-and-hospitality/)
*   [IBM Travel and Transportation](https://www.ibm.com/industries/travel-transportation)
*   [IBM Consulting on AWS](https://partners.amazonaws.com/partners/001E000001IlLnmIAF/IBM)
*   [Modernize Baggage Acceptance Messaging with Enhanced Efficiency and Security](https://aws.amazon.com/blogs/industries/modernize-baggage-acceptance-messaging-with-enhanced-efficiency-and-security/)
*   [Reliable Airline Baggage Tracking Solution using AWS IoT and Amazon MSK](https://aws.amazon.com/blogs/iot/reliable-airline-baggage-tracking-solution-using-aws-iot-and-amazon-msk/)
*   [Enhance the reliability of airlines’ mission-critical baggage handling using Amazon DynamoDB](https://aws.amazon.com/blogs/database/enhance-the-reliability-of-airlines-mission-critical-baggage-handling-using-amazon-dynamodb/)
*   [Streamlining Aircraft Payload Planning and Closeout with AI-Powered Chatbots on AWS](https://aws.amazon.com/blogs/apn/streamlining-aircraft-payload-planning-and-closeout-with-ai-powered-chatbots-on-aws/)

[IBM Consulting](https://www.ibm.com/consulting/aws) is an AWS Premier Tier Services Partner that helps customers who use AWS to harness the power of innovation and drive their business transformation. They are recognized as a Global Systems Integrator (GSI) for over 22 competencies, including travel and hospitality consulting. For more information, please contact an [IBM Representative](https://www.ibm.com/account/reg/us-en/signup?formid=MAIL-consult).

* * *

### About the authors

**![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/Neeraj_kaushik.png)Neeraj Kaushik** is an Open Group Certified Distinguish Architect at IBM with two decades of experience in client-facing delivery roles. His experience spans several industries, including travel and transportation, banking, retail, education, healthcare, and anti-human trafficking. As a trusted advisor, he works directly with the client executive and architects on business strategy to define a technology roadmap. As a hands-on Chief Architect AWS Professional Certified Solution Architect, AWS Certified Machine Learning Specialist and Natural Language Processing Expert, he has led multiple complex cloud modernization programs and AI initiatives.

**![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/bdb-4610-jay-1.png)Jay Pandya** is a Senior Partner Solutions Architect in the Global Systems Integrator (GSI) team at Amazon Web Services (AWS). He has over 30 years of IT experience and is helping and providing guidance to AWS GSI partners to build, design, and architect agile, scalable, highly available, and secure solutions on AWS. Outside of the office, Jay enjoys spending time with his family and traveling, and he is an aviation enthusiast and avid sports and Formula 1 fan.

**![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/bdb-4610-vijay-1-scaled-e1751313704709-100x100-1.jpeg)Vijay Gokarn** is a Senior Solution Architect at IBM with extensive experience across industries including financial services, healthcare, industrial, retail, and travel and hospitality. He leads complex AWS transformation initiatives, drawing on his hands-on expertise as an AWS Certified Solutions Architect Associate. Vijay specializes in serverless architectures, event-driven systems, and enterprise modernization. As a skilled architect and team leader, he has delivered impactful solutions in cloud modernization, digital banking, and intelligent automation. His passion lies in bridging business strategy with technical execution to drive scalable digital transformation.

**![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2025/07/02/subhash_sharma-e1726775506523-150x150-1.png)Subhash Sharma** is Sr. Partner Solutions Architect at AWS. He has more than 25 years of experience in delivering distributed, scalable, highly available, and secured software products using Microservices, AI/ML, the Internet of Things (IoT), and Blockchain using a DevSecOps approach. In his spare time, Subhash likes to spend time with family and friends, hike, walk on beach, and watch TV.","To provide a seamless travel experience, aviation enterprises must streamline baggage handling to be as efficient as possible. Traditional baggage analytics systems often struggle with adaptability, r...","[""api"",""aws"",""database"",""python"",""rust"",""tensorflow""]"
d289d511-8aa4-4c49-bf5c-52f0867d3c70,caa414a14988541fb42678e3d8273377576cf2e4,161e6a7a-f07a-4953-95ba-77ccee004a53,Overcome your Kafka Connect challenges with Amazon Data Firehose,https://aws.amazon.com/blogs/big-data/overcome-your-kafka-connect-challenges-with-amazon-data-firehose/,"Apache Kafka is a popular open source distributed streaming platform that is widely used in the AWS ecosystem. It’s designed to handle real-time, high-throughput data streams, making it well-suited for building real-time data pipelines to meet the streaming needs of modern cloud-based applications.

For AWS customers looking to run Apache Kafka, but don’t want to worry about the undifferentiated heavy lifting involved with self-managing their Kafka clusters, [Amazon Managed Streaming for Apache Kafka](https://aws.amazon.com/msk/) (Amazon MSK) offers fully managed Apache Kafka. This means Amazon MSK provisions your servers, configures your Kafka clusters, replaces servers when they fail, orchestrates server patches and upgrades, makes sure clusters are architected for high availability, makes sure data is durably stored and secured, sets up monitoring and alarms, and runs scaling to support load changes. With a managed service, you can spend your time developing and running streaming event applications.

For applications to use data sent to Kafka, you need to write, deploy, and manage application code that consumes data from Kafka.

Kafka Connect is an open-source component of the Kafka project that provides a framework for connecting with external systems such as databases, key-value stores, search indexes, and file systems from your Kafka clusters. On AWS, our customers commonly write and manage connectors using the Kafka Connect framework to move data out of their Kafka clusters into persistent storage, like [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3), for long-term storage and historical analysis.

At scale, customers need to programmatically manage their Kafka Connect infrastructure for consistent deployments when updates are required, as well as the code for error handling, retries, compression, or data transformation as it is delivered from your Kafka cluster. However, this introduces a need for investment into the [software development lifecycle](https://aws.amazon.com/what-is/sdlc/) (SDLC) of this management software. Although the SDLC is a cost-effective and time-efficient process to help development teams build high-quality software, for many customers, this process is not desirable for their data delivery use case, particularly when they could dedicate more resources towards innovating for other key business differentiators. Beyond SDLC challenges, many customers face fluctuating data streaming throughput. For instance:

*   Online gaming businesses experience throughput variations based on game usage
*   Video streaming applications see changes in throughput depending on viewership
*   Traditional businesses have throughput fluctuations tied to consumer activity

Striking the right balance between resources and workload can be challenging. Under-provisioning can lead to consumer lag, processing delays, and potential data loss during peak loads, hampering real-time data flows and business operations. On the other hand, over-provisioning results in underutilized resources and unnecessary high costs, making the setup economically inefficient for customers. Even the action of scaling up your infrastructure introduces additional delays because resources need to be provisioned and acquired for your Kafka Connect cluster.

Even when you can estimate aggregated throughput, predicting throughput per individual stream remains difficult. As a result, to achieve smooth operations, you might resort to over-provisioning your Kafka Connect resources (CPU) for your streams. This approach, though functional, might not be the most efficient or cost-effective solution.

Customers have been asking for a fully serverless solution that will not only handle managing resource allocation, but transition the cost model to only pay for the data they are delivering from the Kafka topic, instead of underlying resources that require constant monitoring and management.

In September 2023, we announced a new integration between Amazon MSK and [Amazon Data Firehose](https://aws.amazon.com/firehose/), allowing builders to deliver data from their MSK topics to their destination sinks with a fully managed, serverless solution. With this new integration, you no longer needed to develop and manage your own code to read, transform, and write your data to your sink using Kafka Connect. Firehose abstracts away the retry logic required when reading data from your MSK cluster and delivering it to the desired sink, as well as infrastructure provisioning, because it can scale out and scale in automatically to adjust to the volume of data to transfer. There are no provisioning or maintenance operations required on your side.

At release, the checkpoint time to start consuming data from the MSK topic was the creation time of the Firehose stream. Firehose couldn’t start reading from other points on the data stream. This caused challenges for several different use cases.

For customers that are setting up a mechanism to sink data from their cluster for the first time, all data in the topic older than the timestamp of Firehose stream creation would need another way to be persisted. For example, customers using Kafka Connect connectors, like These users were limited in using Firehose because they wanted to sink all the data from the topic to their sink, but Firehose couldn’t read data from earlier than the timestamp of Firehose stream creation.

For other customers that were running Kafka Connect and needed to migrate from their Kafka Connect infrastructure to Firehose, this required some extra coordination. The release functionality of Firehose means you can’t point your Firehose stream to a specific point on the source topic, so a migration requires stopping data ingest to the source MSK topic and waiting for Kafka Connect to sink all the data to the destination. Then you can create the Firehose stream and restart the producers such that the Firehose stream can then consume new messages from the topic. This adds additional, and non-trivial, overhead to the migration effort when attempting to cut over from an existing Kafka Connect infrastructure to a new Firehose stream.

To address these challenges, we’re happy to announce a new feature in the Firehose integration with Amazon MSK. You can now specify the Firehose stream to either read from the earliest position on the Kafka topic or from a custom timestamp to begin reading from your MSK topic.

In the [first post of this series](https://aws.amazon.com/blogs/aws/amazon-msk-introduces-managed-data-delivery-from-apache-kafka-to-your-data-lake/), we focused on managed data delivery from Kafka to your data lake. In this post, we extend the solution to choose a custom timestamp for your MSK topic to be synced to Amazon S3.

## Overview of Firehose integration with Amazon MSK

Firehose integrates with Amazon MSK to offer a fully managed solution that simplifies the processing and delivery of streaming data from Kafka clusters into data lakes stored on Amazon S3. With just a few clicks, you can continuously load data from your desired Kafka clusters to an S3 bucket in the same account, eliminating the need to develop or run your own connector applications. The following are some of the key benefits to this approach:

*   **Fully managed service** – Firehose is a fully managed service that handles the provisioning, scaling, and operational tasks, allowing you to focus on configuring the data delivery pipeline.
*   **Simplified configuration** – With Firehose, you can set up the data delivery pipeline from Amazon MSK to your sink with just a few clicks on the [AWS Management Console](http://aws.amazon.com/console).
*   **Automatic scaling** – Firehose automatically scales to match the throughput of your Amazon MSK data, without the need for ongoing administration.
*   **Data transformation and optimization** – Firehose offers features like JSON to Parquet/ORC conversion and batch aggregation to optimize the delivered file size, simplifying data analytical processing workflows.
*   **Error handling and retries** – Firehose automatically retries data delivery in case of failures, with configurable retry durations and backup options.
*   **Offset select option** – With Firehose, you can select the starting position for the MSK delivery stream to be delivered within a topic from three options:
    *   **Firehose stream creation time** – This allows you to deliver data starting from Firehose stream creation time. When migrating from to Firehose, if you have an option to pause the producer, you can consider this option.
    *   **Earliest** – This allows you to deliver data starting from MSK topic creation time. You can choose this option if you’re setting a new delivery pipeline with Firehose from Amazon MSK to Amazon S3.
    *   **At timestamp** – This option allows you to provide a specific start date and time in the topic from where you want the Firehose stream to read data. The time is in your local time zone. You can choose this option if you prefer not to stop your producer applications while migrating from Kafka Connect to Firehose. You can refer to the Python script and steps provided later in this post to derive the timestamp for the latest events in your topic that were consumed by Kafka Connect.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image001.png)

The following are benefits of the new timestamp selection feature with Firehose:

*   You can select the starting position of the MSK topic, not just from the point that the Firehose stream is created, but from any point from the earliest timestamp of the topic.
*   You can replay the MSK stream delivery if required, for example in the case of testing scenarios to select from different timestamps with the option to select from a specific timestamp.
*   When migrating from Kafka Connect to Firehose, gaps or duplicates can be managed by selecting the starting timestamp for Firehose delivery from the point where Kafka Connect delivery ended. Because the new custom timestamp feature isn’t monitoring Kafka consumer offsets per partition, the timestamp you select for your Kafka topic should be a few minutes before the timestamp at which you stopped Kafka Connect. The earlier the timestamp you select, the more duplicate records you will have downstream. The closer the timestamp to the time of Kafka Connect stopping, the higher the likelihood of data loss if certain partitions have fallen behind. Be sure to select a timestamp appropriate to your requirements.

## Overview of solution

We discuss two scenarios to stream data.

In Scenario 1, we migrate to Firehose from Kafka Connect with the following steps:

1.  Derive the latest timestamp from MSK events that Kafka Connect delivered to Amazon S3.
2.  Create a Firehose delivery stream with Amazon MSK as the source and Amazon S3 as the destination with the topic starting position as **Earliest**.
3.  Query Amazon S3 to validate the data loaded.

In Scenario 2, we create a new data pipeline from Amazon MSK to Amazon S3 with Firehose:

1.  Create a Firehose delivery stream with Amazon MSK as the source and Amazon S3 as the destination with the topic starting position as **At timestamp**.
2.  Query Amazon S3 to validate the data loaded.

The solution architecture is depicted in the following diagram.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image003.png)

## Prerequisites

You should have the following prerequisites:

*   An [AWS account](https://portal.aws.amazon.com/billing/signup/iam?nc2=h_ct&redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation&src=header_signup#/support) and access to the following AWS services:
    *   [Amazon Elastic Compute Cloud](https://aws.amazon.com/ec2/) (Amazon EC2)
    *   Amazon Data Firehose
    *   [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM)
    *   Amazon MSK
    *   Amazon S3
*   An MSK provisioned or MSK serverless cluster with topics created and data streaming to it. The sample topic used in this is `order`.
*   An EC2 instance configured to use as a Kafka admin client. Refer to [Create an IAM role](https://docs.aws.amazon.com/msk/latest/developerguide/create-iam-role.html) for instructions to create the client machine and IAM role that you will need to run commands against your MSK cluster.
*   An S3 bucket for delivering data from Amazon MSK using Firehose.
*   Kafka Connect to deliver data from Amazon MSK to Amazon S3 if you want to migrate from Kafka Connect (Scenario 1).

## Migrate to Firehose from Kafka Connect

To reduce duplicates and minimize data loss, you need to configure your custom timestamp for Firehose to read events as close to the timestamp of the oldest committed offset that Kafka Connect reported. You can follow the steps in this section to visualize how the timestamps of each committed offset will vary by partition across the topic you want to read from. This is for demonstration purposes and doesn’t scale as a solution for workloads with a large number of partitions.

Sample data was generated for demonstration purposes by following the instructions referenced in the following [GitHub repo](https://github.com/aws-samples/clickstream-producer-for-apache-kafka). We set up a sample producer application that generates clickstream events to simulate users browsing and performing actions on an imaginary ecommerce website.

To derive the latest timestamp from MSK events that Kafka Connect delivered to Amazon S3, complete the following steps:

1.  From your Kafka client, query Amazon MSK to retrieve the Kafka Connect consumer group ID:
    
    ```
    ./kafka-consumer-groups.sh --bootstrap-server $bs --list --command-config client.properties
    ```
    
    ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image007.png)
    
2.  Stop Kafka Connect.
3.  Query Amazon MSK for the latest offset and associated timestamp for the consumer group belonging to Kafka Connect.

You can use the `get_latest_offsets.py` Python script from the following [GitHub repo](https://github.com/aws-samples/How-to-Overcome-your-Kafka-Connect-Challenges-with-Amazon-Data-Firehose) as a reference to get the timestamp associated with the latest offsets for your Kafka Connect consumer group. To enable authentication and authorization for a non-Java client with an IAM authenticated MSK cluster, refer to the following [GitHub repo](https://github.com/aws/aws-msk-iam-sasl-signer-python/blob/main/README.rst) for instructions on installing the `aws-msk-iam-sasl-signer-python` package for your client.

```
python3 get_latest_offsets.py --broker-list $bs --topic-name “order” --consumer-group-id “connect-msk-serverless-connector-090224” --aws-region “eu-west-1”
```

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image009.png)

Note the earliest timestamp across all the partitions.

## Create a data pipeline from Amazon MSK to Amazon S3 with Firehose

The steps in this section are applicable to both scenarios. Complete the following steps to create your data pipeline:

1.  On the Firehose console, choose **Firehose streams** in the navigation pane.
2.  Choose **Create Firehose stream**.  
    ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image011.jpg)
3.  For **Source**, choose **Amazon MSK**.
4.  For **Destination**, choose **Amazon S3**.  
    ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image013.jpg)
5.  For **Source settings**, browse to the MSK cluster and enter the topic name you created as part of the prerequisites.
6.  Configure the Firehose stream starting position based on your scenario:
    1.  For Scenario 1, set **Topic starting position** as **At Timestamp** and enter the timestamp you noted in the previous section.  
        ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image015.png)
    2.  For Scenario 2, set **Topic starting position** as **Earliest**.  
        ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image017.png)
7.  For **Firehose stream name**, leave the default generated name or enter a name of your preference.
8.  For **Destination settings**, browse to the S3 bucket created as part of the prerequisites to stream data.

Within this S3 bucket, by default, a folder structure with `YYYY/MM/dd/HH` will be automatically created. Data will be delivered to subfolders pertaining to the HH subfolder according to the Firehose to Amazon S3 ingestion timestamp.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image019.jpg)

9.  Under **Advanced settings**, you can choose to create the default IAM role for all the permissions that Firehose needs or choose existing an IAM role that has the policies that Firehose needs.  
    ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image021.png)
10.  Choose **Create Firehose stream**.  
    ![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image023.jpg)

On the Amazon S3 console, you can verify the data streamed to the S3 folder according to your chosen offset settings.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/13/BDB-4358-image025.png)

## Clean up

To avoid incurring future charges, delete the resources you created as part of this exercise if you’re not planning to use them further.

## Conclusion

Firehose provides a straightforward way to deliver data from Amazon MSK to Amazon S3, enabling you to save costs and reduce latency to seconds. To try Firehose with Amazon S3, refer to the [Delivery to Amazon S3 using Amazon Data Firehose](https://catalog.workshops.aws/msk-labs/en-US/amazon-data-firehose-integration) lab.

* * *

### About the Authors

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/17/swapna.png)**Swapna Bandla** is a Senior Solutions Architect in the AWS Analytics Specialist SA Team. Swapna has a passion towards understanding customers data and analytics needs and empowering them to develop cloud-based well-architected solutions. Outside of work, she enjoys spending time with her family.

![](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2024/09/17/Austin.png)**Austin Groeneveld** is a Streaming Specialist Solutions Architect at Amazon Web Services (AWS), based in the San Francisco Bay Area. In this role, Austin is passionate about helping customers accelerate insights from their data using the AWS platform. He is particularly fascinated by the growing role that data streaming plays in driving innovation in the data analytics space. Outside of his work at AWS, Austin enjoys watching and playing soccer, traveling, and spending quality time with his family.","Apache Kafka is a popular open source distributed streaming platform that is widely used in the AWS ecosystem. It’s designed to handle real-time, high-throughput data streams, making it well-suited fo...","[""api"",""aws"",""database"",""javascript"",""python"",""tensorflow""]"
b40c8932-bea2-4ec6-9001-9ad3c5ebf5d9,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/a-new-header-and-footer-experience-in-word-for-the-web/ba-p/4432026,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,A new header and footer experience in Word for the web,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/a-new-header-and-footer-experience-in-word-for-the-web/ba-p/4432026,"Hi, Microsoft 365 Insiders! I’m Marko Panić, a Product Manager on the Word team. Today, I’m excited to share a new and simplified experience for editing headers and footers in Microsoft Word for the web, which is now aligned with the experience in Word for Windows and for Mac. It includes changes such as:

*   the ability to edit headers and footers directly on the canvas.
*   a new contextual ribbon tab.
*   the ability to remove headers and footers separately, set different headers and footers for the first page or odd and even pages, and customize page numbers.

You can try this update [here](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG).

### A new header and footer experience in Word for the web

This update is part of our ongoing effort to create a consistent experience across Windows, Mac, and the web. With a familiar workflow for editing headers and footers in documents, it becomes much easier to make changes quickly and collaborate with others who may prefer one platform over another. The new header and footer experience also makes working on the canvas more intuitive and gives you easier access to the options you use most frequently.

Now, when you double-click on the header or footer areas, you can insert or edit the content in-place rather than via an overlay. It’s now easier to see whether you’re editing a header or footer or working in the main body of the document because content that isn’t being edited appears slightly faded. The new contextual ribbon tab offers relevant options like adding page numbers and page count, or adjusting the position and cadence of the header and footer text. Finally, you can remove only headers or only footers.

### How it works

1.  Open a new or existing document in Word for the web, or navigate [here](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG).
2.  Double-click on the top of any page to access the header section, or the bottom of any page to access the footer section.

**NOTE**: The **Header & Footer** contextual ribbon tab will automatically appear at the top of the page.

3.  Write in your header or footer text, or select options under the **Header & Footer** tab to add page numbers or page count, change the position of page numbers, have a different header or footer for the first page or odd and even pages, or remove headers or footers.
    

4.  To exit, click outside the header or footer area, click **Close Preview** in the **Header & Footer** tab, or press **Esc** on your keyboard. 

### Availability

This update is available to all Word for the web users. Try it [here](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG).

### Feedback

We want to hear from you! Please submit your comments and feedback by selecting **Help** > **Feedback**.

We hope you enjoy the new header and footer experience in Word for the web. Happy editing!

Learn about the [Microsoft 365 Insider program](https://aka.ms/MSFT365InsiderProgram) and sign up for the [Microsoft 365 Insider newsletter](https://aka.ms/msft365insidernews) to get the latest information about Insider features in your inbox once a month!","Hi, Microsoft 365 Insiders! I’m Marko Panić, a Product Manager on the Word team. Today, I’m excited to share a new and simplified experience for editing headers and footers in Microsoft Word for the w...","[""artificial-intelligence"",""cloud"",""energy"",""python"",""reinforcement-learning"",""tensorflow""]"
3daa2f45-3db2-40f8-9547-573a8faebf75,https://techcommunity.microsoft.com/t5/excel-blog/what-s-new-in-excel-november-2024/ba-p/4274202,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,What's New in Excel (November 2024),https://techcommunity.microsoft.com/t5/excel-blog/what-s-new-in-excel-november-2024/ba-p/4274202,"Welcome to the November 2024 update. This month, we are pleased to introduce new features in Copilot in Excel that can transform your data analysis, assist with conditional formatting, and the Excel chat helper is here to kickstart your Excel journey. These features are now available on Windows, Mac and the web. In addition, the new regular expression (regex) functions are now rolling out to Windows and Mac users.  
  

**Excel for Windows, Mac, and web:**  
\- Copilot in Excel: Transforming Data Analysis  
\- New Regular Expression (REGEX) Functions  
\- Conditional formatting with Copilot in Excel  
\- Copilot in Excel: Excel Chat Helper  
  
**Excel for Mac:**  
\- Recent widgets for Word, Excel, and PowerPoint (Insiders)

### **Excel for Windows, Mac, and web**

**Copilot in Excel: Transforming Data Analysis  
**

We recently announced the [general availability of Copilot in Excel](https://techcommunity.microsoft.com/blog/excelblog/unlock-the-power-of-copilot-in-excel-now-generally-available/4242810). Now, we’re excited to share the following announcements: 

*   Copilot in Excel with Python is generally available in US (EN-US) for Windows 
*   You can create a table specific to your needs with Copilot 
*   Copilot helps you pull in data from your organization and search the web 
*   Copilot's enhanced text analysis capabilities provide new ways to reason over your data and derive insights

A GIF showing a person asking Copilot to insert a table, and to make adjustments to the table before inserting into the spreadsheet.  

**[Read the full blog to learn more >](https://techcommunity.microsoft.com/blog/excelblog/copilot-in-excel-transforming-data-analysis/4303611)  
  
**

**New Regular Expression (REGEX) Functions  
**Regular expressions, or ‘regex’, are sequences of characters that define search patterns, commonly used for string searching and text parsing. They are incredibly versatile and are often used to check if a string contains a certain pattern, extract substrings that match the pattern, or replace substrings that match the pattern.   
  
The new regex functions we are introducing are:

*   **REGEXTEST**: Checks if any part of supplied text matches a regex pattern.
*   **REGEXEXTRACT**: Extracts one or more parts of supplied text that match a regex pattern.
*   **REGEXREPLACE**: Searches for a regex pattern within supplied text and replaces it with different text.

Picture showing an example using REGEXEXTRACT to extract names from text using the pattern ""\[A-z\]+ \[A-z\]+"", which matches two groups of alphabet characters separated by a space.

These features are currently rolling out to all platforms.

[Read the full blog here >](https://techcommunity.microsoft.com/blog/microsoft365insiderblog/new-regular-expression-regex-functions-in-excel/4226334)

**Conditional formatting with Copilot in Excel  
**Conditional formatting in Excel is a versatile tool that can help you analyze and present your data more effectively. By exploring the various capabilities of conditional formatting in combination with Copilot you can unlock the full potential of this feature and make your spreadsheets more informative and visually engaging. [Read more here >](https://techcommunity.microsoft.com/blog/excelblog/color-conditions-and-copilot-how-to-save-time-using-conditional-formatting-with-/4284025)

Gif showing application of Copilot generated conditional format and then how editing data updates the coloring based on the rule.

**  
Copilot in Excel: Excel Chat Helper**  
Starting with Excel can be challenging, especially when consolidating data from various sources like emails and meetings, or when you're unfamiliar with the required analysis. The chat helper within Copilot is your go-to resource for mastering Excel, simplifying complex tasks, and enhancing your productivity and here to guide you every step of the way. [Read more here >](https://techcommunity.microsoft.com/blog/excelblog/copilot-in-excel-excel-chat-helper/4293950)

## **Excel for Mac**

**Recent widgets for Word, Excel, and PowerPoint (Insiders)**  
Quickly access your recently used files straight from your desktop with Recent widgets for Word, Excel, and PowerPoint for Mac. The widgets allow you to both view and open your most recently accessed files from the desktop. [Read more here >](https://techcommunity.microsoft.com/blog/microsoft365insiderblog/recent-widgets-for-word-excel-and-powerpoint-for-mac/4289476)

## **Check if a specific feature is in your version of Excel**

_Click [here](https://aka.ms/xlExcelFeaturesFlyer) to open in a new browser tab_

Many of these features are the result of your feedback. THANK YOU! Your continued Feedback in Action (#FIA) helps improve Excel for everyone. Please let us know how you like a particular feature and what we can improve upon—[""Give a compliment"" or ""Make a suggestion"".](https://support.office.com/en-us/article/how-do-i-give-feedback-on-microsoft-office-2b102d44-b43f-4dd2-9ff4-23cf144cfb11).  You can also submit new ideas or vote for other ideas via [Microsoft Feedback](https://feedbackportal.microsoft.com/feedback/forum/c23f3b77-f01b-ec11-b6e7-0022481f8472).

Subscribe to our [Excel Blog](https://aka.ms/xlblog) and the [Insiders Blog](https://insider.microsoft365.com/blog) to get the latest updates. Stay connected with us and other Excel fans around the world – join our [Excel Community](https://aka.ms/ExcelCommunity) and follow us on [X, formerly Twitter](https://twitter.com/msexcel).

Special thanks to our Excel MVPs [David Benaim,](https://mvp.microsoft.com/en-us/PublicProfile/5003684?fullName=David%20Benaim) [Bill Jelen](https://mvp.microsoft.com/en-us/PublicProfile/21505?fullName=Bill%20Jelen), [Alan Murray](https://mvp.microsoft.com/en-US/MVP/profile/791c111d-ed9f-ea11-a811-000d3a8dfe0d), and [John Michaloudis](https://mvp.microsoft.com/en-US/MVP/profile/39aa1284-2d2e-eb11-a813-000d3a8dfe0d) for their contribution to this month's What's New in Excel article. David publishes weekly [YouTube videos](https://www.youtube.com/@learnspreadsheets) and regular [LinkedIn posts](https://www.linkedin.com/in/davebenaim/) about the latest innovations in Excel and more. Bill is the founder and host of [MrExcel.com](https://mrexcel.com/) and the author of several books about Excel. Alan is an Excel trainer, author and speaker, best known for his blog [computergaga.com](https://www.computergaga.com/) and [YouTube channel](https://www.youtube.com/@Computergaga) with the same name. John is the Founder & Chief Inspirational Officer at [MyExcelOnline.com](https://www.myexcelonline.com/) where he passionately teaches thousands of professionals how to use Excel to stand out from the crowd.","Welcome to the November 2024 update. This month, we are pleased to introduce new features in Copilot in Excel that can transform your data analysis, assist with conditional formatting, and the Excel c...","[""artificial-intelligence"",""data-analysis"",""data-science"",""python"",""reinforcement-learning"",""tensorflow""]"
baf5df0a-be64-4f8b-96e0-6ac051514a3c,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/introducing-the-new-get-data-dialog-and-onelake-catalog-in-excel/ba-p/4430878,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,Introducing the new Get Data dialog and OneLake catalog in Excel for Windows,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/introducing-the-new-get-data-dialog-and-onelake-catalog-in-excel/ba-p/4430878,"Hi, Microsoft 365 Insiders! My name is Gal Zivoni, and I’m a Product Manager on the Excel team. I’m excited to share a new, modern experience for Power Query in Microsoft Excel for Windows that also introduces the OneLake catalog.

### Introducing the new Get Data dialog and OneLake catalog in Excel for Windows

We’re introducing a new way to connect to data in Microsoft Excel for Windows that will make finding and using external data sources faster, smarter, and more intuitive! The new Get Data dialog for Power Query brings together search and recommendations in a streamlined layout to help you quickly locate the right data source to bring data from. The new experience also gives you access to all the high-quality data from across your organization that’s stored in Fabric’s OneLake right within Excel.

### How it works

1.  Open Excel on your Windows device and sign into your Microsoft 365 account.
2.  Select the **Data** tab on the ribbon, then select **Get Data** \> **Get Data (Preview)** to open the new dialog.

3.  Browse through popular data sources on the **Home** tab, or use the search bar to find a specific source.

**NOTE**: To browse through all available data sources, select the **New** tab under the categories list on the left.

4.  To browse through OneLake data artifacts, navigate to the **OneLake catalog** section in the **Home** tab, or the **OneLake** tab on the left.
    

5.  Select your desired data source and follow the prompts to import the data into Excel as you normally would before this update.

### Tips and tricks

*   You can still import external data from the **Get Data** dropdown categories as well as from the new Get Data dialog.
*   Lakehouse and Warehouse are currently supported in the OneLake catalog, with more data source support coming soon!
*   For more information, check out [About Power Query in Excel](https://support.microsoft.com/office/about-power-query-in-excel-7104fbee-9e62-4cb9-a02e-5bfb1a6c536a?preview=true).

### Availability

This feature is available to Beta Channel users running Version 2505 (Build 18829.20000) or later in Excel for Windows.

### Feedback

We want to hear from you! Please submit your comments and feedback on this new feature by selecting **Help** \> **Feedback** in Excel.  

Learn about the [Microsoft 365 Insider program](https://aka.ms/MSFT365InsiderProgram) and sign up for the [Microsoft 365 Insider newsletter](https://aka.ms/msft365insidernews) to get the latest information about Insider features in your inbox once a month!","Hi, Microsoft 365 Insiders! My name is Gal Zivoni, and I’m a Product Manager on the Excel team. I’m excited to share a new, modern experience for Power Query in Microsoft Excel for Windows that also i...","[""announcement"",""artificial-intelligence"",""energy"",""machine-learning"",""news"",""review""]"
ef26d463-5a17-431a-99e4-849785662336,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/summarize-transferred-calls-in-teams-with-copilot/ba-p/4427247,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,Summarize transferred calls in Teams with Copilot,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/summarize-transferred-calls-in-teams-with-copilot/ba-p/4427247,"Hi, Microsoft 365 Insiders! We’re Prashi Badkur and Nitin Jaitely, Product Managers on the Teams team. We’re excited to announce that you can now access and send a summary created by Microsoft 365 Copilot for transferred calls in Microsoft Teams for Mac, for Windows, and for Phone devices.

### Summarize transferred calls in Teams with Copilot

We know that many of you experience a high volume of call transfers and want to create customer service experiences that are consistent, privacy-conscious, and scalable. We’re thrilled to provide a solution! Now in Teams, you can opt to send a brief, AI-generated summary of a call before transferring it to someone that will include essential information from the call transcript, such as the callers’ identities, topics discussed, and actions taken. This feature reduces the need to repeat yourself when someone new joins a conversation, and ensures smooth handoffs.

### How it works

##### On Windows and Mac

1.  On a Teams call on your Windows or Mac device, make sure transcription is enabled by selecting **Record and transcribe** under the **More** (**…**) menu, then clicking **Start transcription**. Or, you can enable Copilot by clicking on the **Copilot** button and selecting **Turn on**, then your preferred language to have Copilot generate transcriptions and notes.
2.  Select **Transfer** to transfer the call.
3.  In the Transfer the call menu, check **Generate call transfer context for the recipient**. This will create an AI summary of your discussion up until now.
4.  If you’d like to send the summary to the incoming caller, select **Keep it**, then **Transfer** to transfer the call. The transferee will see the summary of the call before answering in the call notification. More context can be provided by selecting the **Call context** button when joining the call.

##### On Phone devices

1.  Make sure call transcription is turned on by selecting **More**, then the **Copilot** button and following the directions to choose a preferred language and set up transcription.
2.  While on a call on your Phone device, select **Transfer**, then check **Generate and send call summary with transfer**.
3.  Select **Transfer now** to transfer the call. The transferee will see an AI-generated Call summary after they accept the call under **More**.

### Availability

This feature is available to all users with a Microsoft 365 Copilot license and [Phone license](https://learn.microsoft.com/en-us/microsoftteams/teams-phone-licensing) running Teams on a Windows, Mac, or Phone device.

### Feedback

We’d love to hear your thoughts on how this feature is working! Select **Settings and more** > **Feedback** in the top-right corner of the Teams app, and then select either **Report a problem**, **Give a compliment**, or **Suggest a feature** to share your thoughts.

Learn about the [Microsoft 365 Insider program](https://aka.ms/MSFT365InsiderProgram) and sign up for the [Microsoft 365 Insider newsletter](https://aka.ms/msft365insidernews) to get the latest information about Insider features in your inbox once a month!","Hi, Microsoft 365 Insiders! We’re Prashi Badkur and Nitin Jaitely, Product Managers on the Teams team. We’re excited to announce that you can now access and send a summary created by Microsoft 365 Cop...","[""announcement"",""artificial-intelligence"",""computer-science"",""energy"",""news"",""privacy""]"
deb752bd-243d-4931-8afa-f6e27d3d8032,https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/request-more-access-in-word-excel-and-powerpoint-for-the-web/ba-p/4429019,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,"Request more access in Word, Excel, and PowerPoint for the web",https://techcommunity.microsoft.com/t5/microsoft-365-insider-blog/request-more-access-in-word-excel-and-powerpoint-for-the-web/ba-p/4429019,"Hi, Insiders! My name is Sonia Kulkarni, and I’m a Principal Product Manager on the Office Shared Services and Experiences team. I’m excited to share with you a new feature in [Word](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG), [Excel](https://excel.cloud.microsoft/?wdOrigin=COMMUNITY.EXCEL.1PBLOG), and [PowerPoint](https://powerpoint.cloud.microsoft/?wdOrigin=COMMUNITY.POWERPOINT.1PBLOG) for the web that allows enterprise customers to request elevated permissions on files directly in the application.

### Request more access in Word, Excel, and PowerPoint for the web

Many of you told us that getting edit access to important documents, spreadsheets, and presentations was time-consuming and disruptive to your workflows. This was because if you lacked certain permissions, you had to either download a copy or reach out to the file owner via channels like Outlook or Teams to ask for new permissions. These workarounds were suboptimal, and we’re thrilled to announce a better solution! Microsoft Word, Excel, and PowerPoint for the web will now include the ability for users with view-only permission to request more access while in a file, unlocking productivity and collaboration with just a few clicks.

### How it works

1.  Open a file you want to change your access to in [Word](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG), [Excel](https://excel.cloud.microsoft/?wdOrigin=COMMUNITY.EXCEL.1PBLOG), or [PowerPoint](https://powerpoint.cloud.microsoft/?wdOrigin=COMMUNITY.POWERPOINT.1PBLOG) for the web.
2.  Select **Viewing** in the upper right-hand corner, then select **Request more access**.
3.  Select the permission you need (**Ask to edit** or **Ask to review**), then write a note to the file owner (this is not mandatory to request more access).
4.  Select **Send**, and you’ll be notified that you will receive an email when the file owner responds to your request. Once your request is approved, refresh the file and notice that your permissions have changed.

As the file owner, you will receive an email showing the person’s current permissions, the access they requested, their note, and an option to **Change access** or **Decline** the request.

### Tips and tricks

*   If you’re zoomed in in your web browser, the Request more access feature will be hidden behind ellipsis (**…**). 
*   To update a pending request or leave a new note, follow the steps above, then select **Resend request**, and you will be notified that your request has been updated.

### Known issues

*   Permission changes may take longer to load with large or complex documents with several co-authors. Your permissions will reflect only after the document has finished loading.
*   This feature is not supported in Classic View mode in Word for the web.

### Requirements

To request different permissions in Word, Excel, and PowerPoint for the web, you must:

*   have a Microsoft 365 Enterprise account.
*   be viewing a file that’s stored in your organization’s OneDrive or SharePoint.
*   have a good internet connection.

### Availability

This feature is available to all Enterprise users running [Word](https://word.cloud.microsoft/?wdOrigin=COMMUNITY.WORD.1PBLOG), [Excel](https://excel.cloud.microsoft/?wdOrigin=COMMUNITY.EXCEL.1PBLOG), and [PowerPoint](https://powerpoint.cloud.microsoft/?wdOrigin=COMMUNITY.POWERPOINT.1PBLOG) for the web.

### Feedback

Let us know your thoughts or suggestions on this feature by selecting **Help** \> **Feedback** in Word, Excel, or PowerPoint for the web.

Learn about the [Microsoft 365 Insider program](https://aka.ms/MSFT365InsiderProgram) and sign up for the [Microsoft 365 Insider newsletter](https://aka.ms/msft365insidernews) to get the latest information about Insider features in your inbox once a month!","Hi, Insiders! My name is Sonia Kulkarni, and I’m a Principal Product Manager on the Office Shared Services and Experiences team. I’m excited to share with you a new feature in [Word](https://word.clou...","[""announcement"",""artificial-intelligence"",""cloud"",""energy"",""networking"",""python""]"
e7ef4296-12d0-4733-8784-efef12175357,updates-to-copilot-in-power-bi-more-ways-to-see-learn-from-and-ask-about-your-report-data,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,"Updates to Copilot in Power BI: More ways to see, learn from, and ask about your report data",https://powerbi.microsoft.com/en-us/blog/updates-to-copilot-in-power-bi-more-ways-to-see-learn-from-and-ask-about-your-report-data/,This blog draft gives an overview of updates to access and learn from report data via Copilot.,This blog draft gives an overview of updates to access and learn from report data via Copilot.,"[""energy"",""microsoft"",""improvement"",""announcement"",""data-science"",""databricks""]"
c7b96064-857f-457c-bd67-cf1fb160892f,https://techcommunity.microsoft.com/t5/excel-blog/text-analysis-in-excel/ba-p/4356131,41d67f31-e18d-46ca-b365-4bcbb7e23cfd,Text Analysis in Excel,https://techcommunity.microsoft.com/t5/excel-blog/text-analysis-in-excel/ba-p/4356131,"Not all data is numerical. There are important insights to be found through surveys, reviews, and other textual data. However, it can be time-consuming and difficult to comb through text to uncover those insights. We’re changing that with a new suite of text analysis capabilities that make Excel a one-stop shop for analyzing your text data, whether you want quick and easy insights or deep analysis. Combined with Excel’s power for numerical analysis and data visualization, you’ll be able to get actionable insights from text like never before. Here are three new features that you can use to analyze your text data.

### **Copilot in Excel**

Copilot can help you analyze a column of text data with ease. You can do all of the following simply by typing a prompt.

**1\. Get a high-level summary of the text including themes**

Within a few minutes, Copilot can scan your text data and return a summary of the text and a list of themes. You can ask for a specific length, tone, or focus area in your prompt. 

Copilot response showing feedback topics

**2\. Run sentiment analysis**

Copilot can identify sentiments in your text data, summarizing insights for each sentiment.

Copilot response with sentiment analysis

**3\. See sources for text analysis**

Text analysis results include sources from your data so you can validate results and don’t have to spend extra time searching for examples.

Source information from Copilot

**4\. Add a column with themes or sentiment**

Once you have a topic list or sentiment analysis, Copilot can insert a column to your dataset with a label for each of your text items. 

Inserting a topic column with Copilot

Then, you can ask for charts or PivotTables to help you find deeper insights about top issues or trends.

Text analysis is currently available for datasets with 30k+ rows. Copilot understands multiple languages without requiring translation.

### **Python in Excel** 

Perform customized text analysis using Python in Excel. With access to the [NLTK](https://www.nltk.org/) library, the possibilities for manipulation and analysis of text are endless.

Text visualizations created using NLTK

You can create a word cloud from your text data to visualize patterns in word frequency.

Python code using the WordCloud libraryWord cloud visualizing word frequencies from a text dataset

All of this is made easy using Copilot in Excel with Python. Just ask Copilot to analyze your text with Python, and Copilot will write the code for you.

Sentiment analysis using Copilot in Excel with Python

Copilot in Excel with Python is currently available in limited capacity, starting with Excel for Windows in the United States. Learn more: [Copilot in Excel with Python | Microsoft Support](https://support.microsoft.com/office/364e4ae9-9343-4d56-952a-5f62b0f70db6).

### **New Text Functions**

Finally, we have added new functions that allow you to manipulate text directly.

Regex functions unlock powerful pattern matching capabilities. [REGEXTEST](https://support.microsoft.com/en-us/office/regextest-function-7d38200b-5e5c-4196-b4e6-9bff73afbd31) identifies whether text matches a particular pattern.

Checking whether the strings in column C contain numerical digits, using the regex pattern “\[0-9\]”

Use the [REGEXEXTRACT](https://support.microsoft.com/en-us/office/regexextract-function-4b96c140-9205-4b6e-9fbe-6aa9e783ff57) function to extract strings from your text according to a pattern.

Extracting names from text using the pattern ""\[A-z\]+ \[A-z\]+"", which matches two groups of alphabet characters separated by a space

[REGEXREPLACE](https://support.microsoft.com/en-us/office/regexreplace-function-9c030bb2-5e47-4efc-bad5-4582d7100897) can replace text that matches a pattern with a string of your choice.

Replacing the first three digits of each phone number with \*\*\*, using the pattern “\[0-9\]{3}-”, which matches against three numerical digits followed by “-“

Have text data in multiple languages? You can now translate text directly in Excel. With the [TRANSLATE](https://support.microsoft.com/en-us/office/translate-function-d34f71c7-2ffe-409a-9a63-5eb5e91aa3dd#:~:text=TRANSLATE%20function%201%20Description%20The%20TRANSLATE%20%28%29%20function,...%204%20Common%20Errors%20Text%20Too%20Long%20) function, you can use [Microsoft Translation Services](https://www.microsoft.com/translator/) on your data without leaving the Excel grid.

Using the TRANSLATE function to translate Japanese reviews to English

\*Disclaimer: If you try these types of prompts and they do not work as expected, it is most likely due to our gradual feature rollout process. Please try again in a few weeks.

Your feedback helps shape the future of Excel. Please leave a comment below with thoughts or questions on content related to this blog post. Additionally, please let us know how you like a particular feature and what we can improve upon—[""Give a compliment"" or ""Make a suggestion""](https://support.office.com/en-us/article/how-do-i-give-feedback-on-microsoft-office-2b102d44-b43f-4dd2-9ff4-23cf144cfb11).  You can also submit new ideas or vote for other ideas via [Microsoft Feedback](https://feedbackportal.microsoft.com/feedback/forum/c23f3b77-f01b-ec11-b6e7-0022481f8472).

Subscribe to our [Excel Blog](https://aka.ms/xlblog) and the [Insiders Blog](https://insider.microsoft365.com/blog) to get the latest updates. Stay connected with us and other Excel fans around the world – join our [Excel Community](https://aka.ms/ExcelCommunity) and follow us on [X, formerly Twitter](https://twitter.com/msexcel).","Not all data is numerical. There are important insights to be found through surveys, reviews, and other textual data. However, it can be time-consuming and difficult to comb through text to uncover th...","[""artificial-intelligence"",""computer-science"",""imaging"",""python"",""reinforcement-learning"",""robotics""]"
09ec376f-987b-4b8b-bf09-d3064cc7bc0b,https://arstechnica.com/tech-policy/2025/07/why-gov-greg-abbott-wont-release-his-emails-with-elon-musk/,4eec5340-aa04-486a-b6a3-7319b40934a1,Why Gov. Greg Abbott won’t release his emails with Elon Musk,https://arstechnica.com/tech-policy/2025/07/why-gov-greg-abbott-wont-release-his-emails-with-elon-musk/,"_This story was originally published by [ProPublica](https://www.propublica.org/article/texas-governor-greg-abbott-elon-musk-emails-foia). This article is co-published with [The Texas Newsroom](https://www.kut.org/texasnewsroom) and [The Texas Tribune](https://www.texastribune.org/newsletters/briefweekly/) as part of an initiative to report on how power is wielded in Texas._

Texas Gov. [Greg Abbott](https://www.kut.org/tags/greg-abbott) doesn’t want [to reveal months of communications](https://www.documentcloud.org/documents/25975648-2025-06-03-abbott-musk-comms-appeal/) with [Elon Musk](https://www.kut.org/tags/elon-musk) or representatives from the tech mogul’s companies, arguing in part that they are of a private nature, not of public interest, and potentially embarrassing.

[Musk had an eventful legislative session in Texas](https://www.propublica.org/article/elon-musk-texas-legislature-laws-spacex-tesla) this year. In addition to his lobbyists successfully advocating for several new laws, Abbott cited the [Tesla](https://www.kut.org/tags/tesla) and [SpaceX](https://www.kut.org/tags/spacex) CEO as the inspiration for the state creating its own efficiency office and has praised him for moving the headquarters for many of his businesses to the state in recent years.

[Read full article](https://arstechnica.com/tech-policy/2025/07/why-gov-greg-abbott-wont-release-his-emails-with-elon-musk/)

[Comments](https://arstechnica.com/tech-policy/2025/07/why-gov-greg-abbott-wont-release-his-emails-with-elon-musk/#comments)",_This story was originally published by [ProPublica](https://www.propublica.org/article/texas-governor-greg-abbott-elon-musk-emails-foia). This article is co-published with [The Texas Newsroom](https:...,"[""api"",""artificial-intelligence"",""aws"",""cloud"",""networking"",""tensorflow""]"
32d39f4e-fbbe-4d7b-8586-7b8bd5b89c13,https://arstechnica.com/gadgets/2025/07/windows-10-will-stop-getting-new-office-features-in-august-of-2026/,4eec5340-aa04-486a-b6a3-7319b40934a1,Office problems on Windows 10? Microsoft’s response will soon be “upgrade to 11.”,https://arstechnica.com/gadgets/2025/07/windows-10-will-stop-getting-new-office-features-in-august-of-2026/,"Microsoft’s advertised end-of-support date for Windows 10 is October 14, 2025. But in reality, the company will gradually wind down support for the enduring popular operating system over the next three years. Microsoft [would really like you to upgrade](https://arstechnica.com/gadgets/2025/01/as-it-buries-windows-10-microsoft-declares-2025-year-of-the-windows-11-pc-refresh/) to Windows 11, [especially if it also means upgrading to a new PC](https://arstechnica.com/gadgets/2024/11/microsoft-pushes-full-screen-ads-for-copilot-pcs-on-windows-10-users/), but it also doesn’t want to leave hundreds of millions of home and business PCs totally unprotected.

Those competing goals have led to lots of announcements and re-announcements and clarifications about updates for both Windows 10 itself and the Office/Microsoft 365 productivity apps that many Windows users run on their PCs.

Today’s addition to the pile [comes via The Verge](https://www.theverge.com/news/706586/microsoft-365-office-app-features-windows-10-end-of-life-2026), which noticed [an update to a support document](https://learn.microsoft.com/en-us/microsoft-365-apps/end-of-support/windows-10-support?source=recommendations) that outlined when Windows 10 PCs would stop receiving new features for the continuously updated Microsoft 365 apps. Most home users will stop getting new features in August 2026, while business users running the Enterprise versions can expect to stop seeing new features in either October 2026 or January 2027, depending on the product they’re using.

[Read full article](https://arstechnica.com/gadgets/2025/07/windows-10-will-stop-getting-new-office-features-in-august-of-2026/)

[Comments](https://arstechnica.com/gadgets/2025/07/windows-10-will-stop-getting-new-office-features-in-august-of-2026/#comments)","Microsoft’s advertised end-of-support date for Windows 10 is October 14, 2025. But in reality, the company will gradually wind down support for the enduring popular operating system over the next thre...","[""announcement"",""computer-science"",""energy"",""enterprise"",""microsoft"",""news""]"
512ef719-7597-478c-b307-495f71cc3bb9,https://arstechnica.com/gadgets/2025/07/googles-android-head-confirms-chrome-os-and-android-are-merging/,4eec5340-aa04-486a-b6a3-7319b40934a1,"Chrome OS is “combining” with Android, but what does that mean?",https://arstechnica.com/gadgets/2025/07/googles-android-head-confirms-chrome-os-and-android-are-merging/,"Android and Chrome OS have been developed in parallel for years, but Google is planning to streamline its operating systems. In a recent interview, Android Ecosystem President Sameer Samat stated bluntly that Android and Chrome OS are merging. This shift, a long time in the making, could give Google more room to maneuver as it plans for new mobile computing experiences.

In the interview, TechRadar's Lance Ulanoff had other things on his mind, but Samat peppered him with questions about how he uses his Apple devices. ""I asked because we’re going to be combining ChromeOS and Android into a single platform, and I am very interested in how people are using their laptops these days and what they’re getting done,"" [said Samat](https://www.techradar.com/phones/android/i-think-you-see-the-future-first-on-android-googles-android-leader-sameer-samat).

We don't get back to this point in the remainder of the interview, but it's probably the most interesting thing Samat said. ""Combining"" can mean many things, but we can certainly speculate. In this case, it might mean the writing is on the wall for Chrome OS as it currently exists.

[Read full article](https://arstechnica.com/gadgets/2025/07/googles-android-head-confirms-chrome-os-and-android-are-merging/)

[Comments](https://arstechnica.com/gadgets/2025/07/googles-android-head-confirms-chrome-os-and-android-are-merging/#comments)","Android and Chrome OS have been developed in parallel for years, but Google is planning to streamline its operating systems. In a recent interview, Android Ecosystem President Sameer Samat stated blun...","[""api"",""artificial-intelligence"",""interview"",""machine-learning"",""mobile-development"",""tensorflow""]"
0ba3b171-afca-4c24-a48d-6a0266c2b469,https://arstechnica.com/information-technology/2025/07/new-grok-ai-model-surprises-experts-by-checking-elon-musks-views-before-answering/,4eec5340-aa04-486a-b6a3-7319b40934a1,New Grok AI model surprises experts by checking Elon Musk’s views before answering,https://arstechnica.com/information-technology/2025/07/new-grok-ai-model-surprises-experts-by-checking-elon-musks-views-before-answering/,"An AI model launched last week appears to have shipped with an unexpected occasional behavior: checking what its owner thinks first.

On Friday, independent AI researcher Simon Willison [documented](https://simonwillison.net/2025/Jul/11/grok-musk/) that xAI's new [Grok 4](https://arstechnica.com/ai/2025/07/musks-grok-4-launches-one-day-after-chatbot-generated-hitler-praise-on-x/) model searches for Elon Musk's opinions on X (formerly Twitter) when asked about controversial topics. The discovery comes just days after xAI launched Grok 4 amid controversy over an earlier version of the chatbot [generating antisemitic outputs](https://arstechnica.com/tech-policy/2025/07/grok-praises-hitler-gives-credit-to-musk-for-removing-woke-filters/), including labeling itself as ""MechaHitler.""

""That is ludicrous,"" Willison told Ars Technica upon initially hearing about the Musk-seeking behavior last week from AI researcher [Jeremy Howard](https://x.com/jeremyphoward/status/1943436621556466171), who traced the discovery through various users on X. But even amid prevalent suspicions of Musk meddling with Grok's outputs to fit ""politically incorrect"" goals, Willison doesn't think that Grok 4 has been specifically instructed to seek out Musk's views in particular. ""I think there is a good chance this behavior is unintended,"" he [wrote](https://simonwillison.net/2025/Jul/11/grok-musk/) in a detailed blog post on the topic.

[Read full article](https://arstechnica.com/information-technology/2025/07/new-grok-ai-model-surprises-experts-by-checking-elon-musks-views-before-answering/)

[Comments](https://arstechnica.com/information-technology/2025/07/new-grok-ai-model-surprises-experts-by-checking-elon-musks-views-before-answering/#comments)",An AI model launched last week appears to have shipped with an unexpected occasional behavior: checking what its owner thinks first.,"[""announcement"",""artificial-intelligence"",""computer-science"",""governance"",""reinforcement-learning"",""social-impact""]"
30193204-1b7a-4ddf-bd82-c7a180ea4d50,https://arstechnica.com/cars/2025/07/hyundais-ioniq-6-n-offers-more-sound-more-shifts-more-smiles/,4eec5340-aa04-486a-b6a3-7319b40934a1,"Hyundai’s Ioniq 6 N offers more sound, more shifts, more smiles",https://arstechnica.com/cars/2025/07/hyundais-ioniq-6-n-offers-more-sound-more-shifts-more-smiles/,"Hyundai provided flights from Albany to London and accommodation so Ars could attend the Goodwood Festival of Speed and the Ioniq 6 N reveal. Ars does not accept paid editorial content.

Hyundai's N division is celebrating its 10th anniversary in 2025. That's not long enough to earn the cachet of other performance sub-brands, like [BMW's M](https://arstechnica.com/cars/2024/03/testing-the-2024-bmw-m2-maybe-the-last-m-car-with-a-manual-transmission/) or [Mercedes-Benz's AMG](https://arstechnica.com/tag/mercedes-amg/), but N has certainly developed a reputation for fun, attainable, and powder-blue cars. With the [Ioniq 5 N](https://arstechnica.com/cars/2024/09/the-2025-hyundai-ioniq-5-n-is-the-antidote-to-boring-electric-cars/), Hyundai's skunkworks for speed proved that it can do the impossible: make an EV that's ridiculously, irrationally fun. Now, it's trying to reproduce that magic.

Meet the Ioniq 6 N, a new big-winged, high-powered, pastel-dipped machine designed for those who cross the spectrum between a love of EVs and a need for speed. It shares much of the performance architecture of the Ioniq 5 N, including stronger motors and cooling to match, but it has some significant updates that should make it even more engaging and fun.

Let's start with what's the same: The Ioniq 6 N uses a pair of uprated electric motors to deliver 641 hp (478 kW) and 568 lb-ft (779 nm) of torque, directed to all four wheels. Not only can those motors put out big power, but they can regen it as well, with the Ioniq 6 N able to brake at up to 0.6 G before it calls in support from its physical brake. (That's about twice as much as most EVs, which usually switch over to friction brakes at 0.3 G.)

[Read full article](https://arstechnica.com/cars/2025/07/hyundais-ioniq-6-n-offers-more-sound-more-shifts-more-smiles/)

[Comments](https://arstechnica.com/cars/2025/07/hyundais-ioniq-6-n-offers-more-sound-more-shifts-more-smiles/#comments)",Hyundai provided flights from Albany to London and accommodation so Ars could attend the Goodwood Festival of Speed and the Ioniq 6 N reveal. Ars does not accept paid editorial content.,"[""artificial-intelligence"",""documentation"",""energy"",""experiment"",""open-source"",""performance""]"
985b4a36-7c85-4b49-bd8f-2fd2f92b6d54,https://arstechnica.com/tech-policy/2025/07/two-guys-hated-using-comcast-so-they-built-their-own-fiber-isp/,4eec5340-aa04-486a-b6a3-7319b40934a1,"Two guys hated using Comcast, so they built their own fiber ISP",https://arstechnica.com/tech-policy/2025/07/two-guys-hated-using-comcast-so-they-built-their-own-fiber-isp/,"Samuel Herman and Alexander Baciu never liked using Comcast's cable broadband. Now, the residents of Saline, Michigan, operate a fiber Internet service provider that competes against Comcast in their neighborhoods and has ambitions to expand.

""All throughout my life pretty much, I've had to deal with Xfinity's bullcrap, them not being able to handle the speeds that we need,"" Herman told Ars. ""I lived in a house of 10. I have seven other brothers and sisters, and there's 10 of us in total with my parents.""

With all those kids using the Internet for school and other needs, ""it just doesn't work out,"" he said. Herman was particularly frustrated with Comcast upload speeds, which are much slower than the cable service's download speeds.

[Read full article](https://arstechnica.com/tech-policy/2025/07/two-guys-hated-using-comcast-so-they-built-their-own-fiber-isp/)

[Comments](https://arstechnica.com/tech-policy/2025/07/two-guys-hated-using-comcast-so-they-built-their-own-fiber-isp/#comments)","Samuel Herman and Alexander Baciu never liked using Comcast's cable broadband. Now, the residents of Saline, Michigan, operate a fiber Internet service provider that competes against Comcast in their...","[""artificial-intelligence"",""networking"",""performance"",""reinforcement-learning"",""rust"",""social-impact""]"
102c4d97-f36b-42a6-9136-58123ad19c14,https://arstechnica.com/culture/2025/07/species-at-30-makes-for-a-great-guilty-pleasure/,4eec5340-aa04-486a-b6a3-7319b40934a1,Species at 30 makes for a great guilty pleasure,https://arstechnica.com/culture/2025/07/species-at-30-makes-for-a-great-guilty-pleasure/,"Earlier this month, Hollywood [mourned the passing](https://deadline.com/2025/07/michael-madsen-dead-1236448967/#:~:text=Michael%20Madsen%2C%20the%20actor%20whose,apparent%20cardiac%20arrest%20at%20his) of Michael Madsen, a gifted actor best known for his critically acclaimed roles in _Reservoir Dogs_, _Kill Bill_, and _Donnie Brasco_, among others. Few obituaries have mentioned one of his lesser-known roles: a black ops mercenary hired to help hunt down an escaped human/alien hybrid in 1995's [_Species_](https://en.wikipedia.org/wiki/Species_\(film\)). The sci-fi thriller turns 30 this year, and while it garnered decidedly mixed reviews upon release, the film holds up quite well as a not-quite-campy B monster movie that makes for a great guilty pleasure.

**(Many spoilers below.)**

Screenwriter Dennis Feldman (_The Golden Child_) was partially inspired by an Arthur C. Clarke article discussing how the odds were slim that an extraterrestrial craft would ever visit Earth, given the great distances that would need to be traversed (assuming that traveling faster than the speed of light would be highly unlikely). Feldman was intrigued by the prospect of making extraterrestrial contact via information— specifically, alien instructions on how to build an instrument that could talk to terrestrial humans.

[Read full article](https://arstechnica.com/culture/2025/07/species-at-30-makes-for-a-great-guilty-pleasure/)

[Comments](https://arstechnica.com/culture/2025/07/species-at-30-makes-for-a-great-guilty-pleasure/#comments)","Earlier this month, Hollywood [mourned the passing](https://deadline.com/2025/07/michael-madsen-dead-1236448967/#:~:text=Michael%20Madsen%2C%20the%20actor%20whose,apparent%20cardiac%20arrest%20at%20hi...","[""announcement"",""api"",""artificial-intelligence"",""performance"",""python"",""reinforcement-learning""]"
ac538379-2537-4761-bb9f-cf89c4a68fe7,tldr-dfeae297,79a35b89-9d54-4aa7-b229-501b354c079d,"Google hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal",https://www.cnbc.com/2025/07/11/google-windsurf-ceo-varun-mohan-latest-ai-talent-deal-.html?utm_source=tldrnewsletter,"# Google hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal
**Category:** Big Tech & Startups
**Read Time:** 3 minute read
Google hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal
**Source:** [Google hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal](https://www.cnbc.com/2025/07/11/google-windsurf-ceo-varun-mohan-latest-ai-talent-deal-.html?utm_source=tldrnewsletter)","Google hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal","[""artificial-intelligence"",""commercialization"",""energy"",""google"",""machine-learning"",""news""]"
dc063f7b-e32c-40d1-8b98-109858715b2f,tldr-66db5941,79a35b89-9d54-4aa7-b229-501b354c079d,SpaceX to Invest $2 Billion Into Elon Musk's xAI,https://www.wsj.com/tech/spacex-to-invest-2-billion-into-elon-musks-xai-413934de?st=RGF5bY&amp;reflink=desktopwebshare_permalink&amp;utm_source=tldrnewsletter,"# SpaceX to Invest $2 Billion Into Elon Musk's xAI
**Category:** Big Tech & Startups
**Read Time:** 3 minute read
SpaceX to Invest $2 Billion Into Elon Musk's xAI
**Source:** [SpaceX to Invest $2 Billion Into Elon Musk's xAI](https://www.wsj.com/tech/spacex-to-invest-2-billion-into-elon-musks-xai-413934de?st=RGF5bY&amp;reflink=desktopwebshare_permalink&amp;utm_source=tldrnewsletter)",SpaceX to Invest $2 Billion Into Elon Musk's xAI,"[""artificial-intelligence"",""commercialization"",""funding"",""news"",""startup"",""openai""]"
8f4be995-fb07-424e-9435-9b241d6451e2,tldr-d3f1234e,79a35b89-9d54-4aa7-b229-501b354c079d,A Never-Ending Supply of Drones Has Frozen the Front Lines in Ukraine,https://www.wsj.com/world/europe/a-never-ending-supply-of-drones-has-frozen-the-front-lines-in-ukraine-ae29c581?mod=tech_lead_story&amp;utm_source=tldrnewsletter,"# A Never-Ending Supply of Drones Has Frozen the Front Lines in Ukraine
**Category:** Big Tech & Startups
**Read Time:** 7 minute read
A Never-Ending Supply of Drones Has Frozen the Front Lines in Ukraine
**Source:** [A Never-Ending Supply of Drones Has Frozen the Front Lines in Ukraine](https://www.wsj.com/world/europe/a-never-ending-supply-of-drones-has-frozen-the-front-lines-in-ukraine-ae29c581?mod=tech_lead_story&amp;utm_source=tldrnewsletter)",A Never-Ending Supply of Drones Has Frozen the Front Lines in Ukraine,"[""artificial-intelligence"",""commercialization"",""news"",""reinforcement-learning"",""startup"",""agriculture""]"
252a8ee5-14a8-4cfa-9ba8-6d271d11c293,tldr-2496947a,79a35b89-9d54-4aa7-b229-501b354c079d,Lucid absolutely smashes Guinness World Record for the longest EV drive on a single charge,https://newatlas.com/automotive/lucid-air-electric-vehicle-distance-single-charge-record/?utm_source=tldrnewsletter,"# Lucid absolutely smashes Guinness World Record for the longest EV drive on a single charge
**Category:** Big Tech & Startups
**Read Time:** 4 minute read
Lucid absolutely smashes Guinness World Record for the longest EV drive on a single charge
**Source:** [Lucid absolutely smashes Guinness World Record for the longest EV drive on a single charge](https://newatlas.com/automotive/lucid-air-electric-vehicle-distance-single-charge-record/?utm_source=tldrnewsletter)",Lucid absolutely smashes Guinness World Record for the longest EV drive on a single charge,"[""artificial-intelligence"",""autonomous-vehicles"",""commercialization"",""news"",""reinforcement-learning"",""startup""]"
9b6bd640-33a2-4380-84ad-fbcb4ae931b1,tldr-4a45200b,79a35b89-9d54-4aa7-b229-501b354c079d,Announcing GenAI Processors: Build powerful and flexible Gemini applications,https://developers.googleblog.com/en/genai-processors/?utm_source=tldrnewsletter,"# Announcing GenAI Processors: Build powerful and flexible Gemini applications
**Category:** Big Tech & Startups
**Read Time:** 6 minute read
Announcing GenAI Processors: Build powerful and flexible Gemini applications
**Source:** [Announcing GenAI Processors: Build powerful and flexible Gemini applications](https://developers.googleblog.com/en/genai-processors/?utm_source=tldrnewsletter)",Announcing GenAI Processors: Build powerful and flexible Gemini applications,"[""announcement"",""artificial-intelligence"",""energy"",""google"",""hardware"",""news""]"
92a26e8d-e334-414f-8a82-6fc2274b18cd,tldr-3cf57350,79a35b89-9d54-4aa7-b229-501b354c079d,I messed up my Google PM Vibe Coding Interview,https://old.reddit.com/r/ProductManagement/comments/1lw9r9h/i_messed_up_my_google_pm_vibe_coding_interview/?utm_source=tldrnewsletter,"# I messed up my Google PM Vibe Coding Interview
**Category:** Science & Futuristic Technology
**Read Time:** 2 minute read
I messed up my Google PM Vibe Coding Interview
**Source:** [I messed up my Google PM Vibe Coding Interview](https://old.reddit.com/r/ProductManagement/comments/1lw9r9h/i_messed_up_my_google_pm_vibe_coding_interview/?utm_source=tldrnewsletter)",I messed up my Google PM Vibe Coding Interview,"[""google"",""interview"",""news"",""angular"",""go"",""hackathon""]"
cd6f79d8-2401-4eec-b363-74124c40e3fb,tldr-911bf77c,79a35b89-9d54-4aa7-b229-501b354c079d,"Tim Cook Isn't Going Anywhere Soon, But an Apple Shake-Up Looms",https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1MjQ2MTMyNiwiZXhwIjoxNzUzMDY2MTI2LCJhcnRpY2xlSWQiOiJTWkM2T09UMEFGQjQwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.xBWdzATM2ZYU-In71SWhi2xJVS3zYJ1t2wmrlEdWxjY&amp;utm_source=tldrnewsletter,"# Tim Cook Isn't Going Anywhere Soon, But an Apple Shake-Up Looms
**Category:** Science & Futuristic Technology
**Read Time:** 16 minute read
Tim Cook Isn't Going Anywhere Soon, But an Apple Shake-Up Looms
**Source:** [Tim Cook Isn't Going Anywhere Soon, But an Apple Shake-Up Looms](https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1MjQ2MTMyNiwiZXhwIjoxNzUzMDY2MTI2LCJhcnRpY2xlSWQiOiJTWkM2T09UMEFGQjQwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.xBWdzATM2ZYU-In71SWhi2xJVS3zYJ1t2wmrlEdWxjY&amp;utm_source=tldrnewsletter)","Tim Cook Isn't Going Anywhere Soon, But an Apple Shake-Up Looms","[""apple"",""aws"",""machine-learning"",""news"",""python"",""reinforcement-learning""]"
405f757a-09d2-4eae-ae0e-8efa32b48878,86d1e26b29f660c3,a16788a4-2d53-4844-8096-4ba1aa7e5386,Robust Multimodal Large Language Models Against Modality Conflict,https://huggingface.co/papers/2025.07.14/robust-multimodal-large-language-models-against-mo,"# Robust Multimodal Large Language Models Against Modality Conflict

**Authors:** ·
								4 authors
**Submitted by:** Submitted by
		
			ustc-zhangzm

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/robust-multimodal-large-language-models-against-mo)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","Robust Multimodal Large Language Models Against Modality Conflict by ·
								4 authors","[""api"",""artificial-intelligence"",""education"",""machine-learning"",""natural-language-processing"",""transformers""]"
502ef7e0-4e9b-49cb-99d8-5041bb7e9831,cfdabba292b5e37a,a16788a4-2d53-4844-8096-4ba1aa7e5386,"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity",https://huggingface.co/papers/2025.07.14/blockffn-towards-end-side-acceleration-friendly-mi,"# BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity

**Authors:** ·
								8 authors
**Submitted by:** Submitted by
		
			Raincleared

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/blockffn-towards-end-side-acceleration-friendly-mi)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with
  Chunk-Level Activation Sparsity by ·
								8 authors","[""api"",""artificial-intelligence"",""education"",""huggingface"",""machine-learning"",""research""]"
f30818a7-7d1d-486b-8ca7-02b8c885918e,e6bff1f2d6009f34,a16788a4-2d53-4844-8096-4ba1aa7e5386,"Vision Foundation Models as Effective Visual Tokenizers for
  Autoregressive Image Generation",https://huggingface.co/papers/2025.07.14/vision-foundation-models-as-effective-visual-token,"# Vision Foundation Models as Effective Visual Tokenizers for
  Autoregressive Image Generation

**Authors:** ·
								8 authors
**Submitted by:** Submitted by
		
			xwen99

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/vision-foundation-models-as-effective-visual-token)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","Vision Foundation Models as Effective Visual Tokenizers for
  Autoregressive Image Generation by ·
								8 authors","[""api"",""artificial-intelligence"",""education"",""huggingface"",""machine-learning"",""research""]"
0e6f4530-fe0a-46f8-9f81-cd04e90ce743,1b24068df779dd05,a16788a4-2d53-4844-8096-4ba1aa7e5386,"What Has a Foundation Model Found? Using Inductive Bias to Probe for
  World Models",https://huggingface.co/papers/2025.07.14/what-has-a-foundation-model-found-using-inductive-,"# What Has a Foundation Model Found? Using Inductive Bias to Probe for
  World Models

**Authors:** ·
								4 authors
**Submitted by:** Submitted by
		
			Ksgk-fy

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/what-has-a-foundation-model-found-using-inductive-)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","What Has a Foundation Model Found? Using Inductive Bias to Probe for
  World Models by ·
								4 authors","[""api"",""artificial-intelligence"",""education"",""machine-learning"",""reinforcement-learning"",""research""]"
ba02ee6e-9e8e-4415-bd5c-3ad5e8067cdf,8735eb72cd39eeb7,a16788a4-2d53-4844-8096-4ba1aa7e5386,From One to More: Contextual Part Latents for 3D Generation,https://huggingface.co/papers/2025.07.14/from-one-to-more-contextual-part-latents-for-3d-ge,"# From One to More: Contextual Part Latents for 3D Generation

**Authors:** ·
								13 authors
**Submitted by:** Submitted by
		
			dscdyc

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/from-one-to-more-contextual-part-latents-for-3d-ge)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","From One to More: Contextual Part Latents for 3D Generation by ·
								13 authors","[""api"",""artificial-intelligence"",""education"",""huggingface"",""machine-learning"",""research""]"
f4959e20-87bc-436f-bfda-c5a7e5b3a02c,2aa2f484eea6c9b3,a16788a4-2d53-4844-8096-4ba1aa7e5386,One Token to Fool LLM-as-a-Judge,https://huggingface.co/papers/2025.07.14/one-token-to-fool-llm-as-a-judge,"# One Token to Fool LLM-as-a-Judge

**Authors:** ·
								6 authors
**Submitted by:** Submitted by
		
			yudian

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/one-token-to-fool-llm-as-a-judge)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","One Token to Fool LLM-as-a-Judge by ·
								6 authors","[""api"",""artificial-intelligence"",""education"",""machine-learning"",""research"",""transformers""]"
ac4061a7-ed73-4faa-b62d-c101de6e29b0,ef2baa4e5e14ab39,a16788a4-2d53-4844-8096-4ba1aa7e5386,"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities",https://huggingface.co/papers/2025.07.14/gemini-25-pushing-the-frontier-with-advanced-reaso,"# Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities

**Authors:** ·
								3303 authors
**Submitted by:** Submitted by
		
			iliashum

This research paper is currently trending on HuggingFace Papers, indicating high interest from the AI research community.

**Source:** HuggingFace Papers - Daily trending research
**Category:** AI/ML Research

[View Paper](https://huggingface.co/papers/2025.07.14/gemini-25-pushing-the-frontier-with-advanced-reaso)

---
*This paper was selected as trending research on HuggingFace Papers, representing cutting-edge developments in artificial intelligence and machine learning.*","Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,
  Long Context, and Next Generation Agentic Capabilities by ·
								3303 authors","[""api"",""artificial-intelligence"",""education"",""huggingface"",""machine-learning"",""research""]"
10406c02-abe4-40cc-838c-2fcf080a0f99,https://techcrunch.com/?p=3027600,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,Malaysia will require trade permits for U.S. AI chips,https://techcrunch.com/2025/07/14/malaysia-will-require-trade-permits-for-u-s-ai-chips/,"# Malaysia will require trade permits for U.S. AI chips
**Author:** Rebecca Szkutak
**Categories:** AI, Enterprise, AI chips, artificial intelligence, chip exports, nvidia
**Published:** 2025-07-14
Malaysia is introducing new export restrictions on U.S. AI chips in an attempt to prevent these chips from being smuggled into China.
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/malaysia-will-require-trade-permits-for-u-s-ai-chips/)",Malaysia is introducing new export restrictions on U.S. AI chips in an attempt to prevent these chips from being smuggled into China.,"[""announcement"",""api"",""artificial-intelligence"",""hardware"",""mit"",""nvidia""]"
df999336-e57e-4a05-bce9-29b6af9de8de,https://techcrunch.com/?p=3027596,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,"NotebookLM adds featured notebooks from The Economist, The Atlantic and others",https://techcrunch.com/2025/07/14/notebooklm-adds-featured-notebooks-from-the-economist-the-atlantic-and-others/,"# NotebookLM adds featured notebooks from The Economist, The Atlantic and others
**Author:** Sarah Perez
**Categories:** AI, Apps, notebooklm
**Published:** 2025-07-14
Google is adding notebooks from various authors, publications, researchers, and nonprofits to its popular AI-powered research and note-taking assistant NotebookLM.
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/notebooklm-adds-featured-notebooks-from-the-economist-the-atlantic-and-others/)","Google is adding notebooks from various authors, publications, researchers, and nonprofits to its popular AI-powered research and note-taking assistant NotebookLM.","[""artificial-intelligence"",""energy"",""google"",""publication"",""research"",""openai""]"
7f01687a-0473-4552-8550-94b9755c1312,https://techcrunch.com/?p=3027557,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,"Prime Day event drove over $24B in U.S. e-commerce sales, gen AI traffic was up 3,300%",https://techcrunch.com/2025/07/14/prime-day-event-drove-over-24b-in-u-s-e-commerce-sales-gen-ai-traffic-was-up-3300/,"# Prime Day event drove over $24B in U.S. e-commerce sales, gen AI traffic was up 3,300%
**Author:** Sarah Perez
**Categories:** AI, Commerce, TC, Amazon, commerce, gen ai
**Published:** 2025-07-14
Amazon's Prime Day, which leads to an overall boost to U.S. e-commerce thanks to competitive sales, saw a significant increase in retail traffic driven by generative AI products, including chatbots and browsers. According to a post-Prime Day analysis by Adobe Analytics, gen AI traffic to U.S. retail sites increased by 3,300% year-over-year &#8212; which was \[...\]
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/prime-day-event-drove-over-24b-in-u-s-e-commerce-sales-gen-ai-traffic-was-up-3300/)","Amazon's Prime Day, which leads to an overall boost to U.S. e-commerce thanks to competitive sales, saw a significant increase in retail traffic driven by generative AI products, including chatbots and browsers. According to a post-Prime Day analysis...","[""amazon"",""artificial-intelligence"",""computer-science"",""data-analysis"",""data-science"",""research""]"
740c5ded-6eb9-461d-84c3-3872a2edf27f,https://techcrunch.com/?p=3027534,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,Mark Zuckerberg says Meta is building a 5GW AI data center,https://techcrunch.com/2025/07/14/mark-zuckerberg-says-meta-is-building-a-5gw-ai-data-center/,"# Mark Zuckerberg says Meta is building a 5GW AI data center
**Author:** Maxwell Zeff
**Categories:** AI, Climate, AI data center, climate, Meta
**Published:** 2025-07-14
Meta is currently building out a data center, called Hyperion, which the company expects to supply its new AI lab with five gigawatts (GW) of computational power, CEO Mark Zuckerberg said in a Monday post on Threads. The announcement marks Meta's latest move to get ahead of OpenAI and Google in the AI race. After \[...\]
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/mark-zuckerberg-says-meta-is-building-a-5gw-ai-data-center/)","Meta is currently building out a data center, called Hyperion, which the company expects to supply its new AI lab with five gigawatts (GW) of computational power, CEO Mark Zuckerberg said in a Monday post on Threads. The announcement marks Meta's lat...","[""announcement"",""artificial-intelligence"",""climate-science"",""computer-science"",""energy"",""telecommunications""]"
11c0c8d8-ab2a-4a35-a2ea-152657d4c7a2,https://techcrunch.com/?p=3027474,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,"Weekly subscriptions dominate iOS app revenue, report finds",https://techcrunch.com/2025/07/14/weekly-subscriptions-dominate-ios-app-revenue-report-finds/,"# Weekly subscriptions dominate iOS app revenue, report finds
**Author:** Ivan Mehta
**Categories:** Apps, app store, in app purchases, iOS, subscriptions
**Published:** 2025-07-14
Weekly subscriptions have now become one of the most popular ways iOS apps are earning revenue, with these plans contributing 46% to the bottom line, according to a new report by app revenue management platform Adapty. The study, which observed $1.9 billion in revenue across more than 11,000 apps, noted that weekly plans have grown \[...\]
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/weekly-subscriptions-dominate-ios-app-revenue-report-finds/)","Weekly subscriptions have now become one of the most popular ways iOS apps are earning revenue, with these plans contributing 46% to the bottom line, according to a new report by app revenue management platform Adapty. The study, which observed $1.9...","[""apple"",""mobile-development"",""open-source"",""release"",""research"",""tensorflow""]"
12c08bc4-f29a-4b9c-a280-9b933ab00d56,https://techcrunch.com/?p=3027537,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,GM to challenge China’s LFP monopoly with upgraded battery factory,https://techcrunch.com/2025/07/14/gm-to-challenge-chinas-lfp-monopoly-with-upgraded-battery-factory/,"# GM to challenge China’s LFP monopoly with upgraded battery factory
**Author:** Tim De Chant
**Categories:** Climate, Transportation, electric vehicles, GM, lfp, LG, lg energy solution, lithium ion batteries, Ultium
**Published:** 2025-07-14
The factory’s overhaul will give U.S. LFP production a significant boost.
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/gm-to-challenge-chinas-lfp-monopoly-with-upgraded-battery-factory/)",The factory’s overhaul will give U.S. LFP production a significant boost.,"[""autonomous-vehicles"",""climate-science"",""energy"",""manufacturing"",""google"",""nvidia""]"
9b05d90e-2340-46db-afa2-80e2c475a514,https://techcrunch.com/?p=3027512,aecfa44d-8a9f-49f4-9af4-c0b916e9b95e,Rainmaker partners with Atmo to squeeze more rain from clouds,https://techcrunch.com/2025/07/14/rainmaker-partners-with-atmo-to-squeeze-more-rain-from-clouds/,"# Rainmaker partners with Atmo to squeeze more rain from clouds
**Author:** Tim De Chant
**Categories:** Startups, Climate, AI, Exclusive, meteorology, cloud seeding, clouds, Rain
**Published:** 2025-07-14
Atmo will use its deep learning models to help Rainmaker identify clouds that have potential for seeding.
**Source:** [TechCrunch](https://techcrunch.com/2025/07/14/rainmaker-partners-with-atmo-to-squeeze-more-rain-from-clouds/)",Atmo will use its deep learning models to help Rainmaker identify clouds that have potential for seeding.,"[""artificial-intelligence"",""climate-science"",""cloud"",""commercialization"",""deep-learning"",""education""]"
39361316-3d7e-46c1-978f-9c94b5edc419,https://venturebeat.com/?p=3014169,c83204e7-d3ca-4e54-a064-1399036f5e7d,AI’s fourth wave is here &#8212; are enterprises ready for what’s next?,https://venturebeat.com/ai/ais-fourth-wave-is-here-are-enterprises-ready-for-whats-next/,"![Yaad Oren, Emma Brunskill, Susan Etlinger](https://venturebeat.com/wp-content/uploads/2025/07/VBTRANSFORM25-0526-X2.jpg?w=578)

* * *

To maintain competitive advantage through the next five years, which innovations must forward-thinking companies prioritize right now?[Read More](https://venturebeat.com/ai/ais-fourth-wave-is-here-are-enterprises-ready-for-whats-next/)","![Yaad Oren, Emma Brunskill, Susan Etlinger](https://venturebeat.com/wp-content/uploads/2025/07/VBTRANSFORM25-0526-X2.jpg?w=578)","[""artificial-intelligence"",""breakthrough"",""enterprise"",""openai"",""anthropic"",""huggingface""]"
0859de0b-c545-42fb-ba0f-4d0ceaa4a4cd,https://venturebeat.com/?p=3014143,c83204e7-d3ca-4e54-a064-1399036f5e7d,The human harbor: Navigating identity and meaning in the AI age,https://venturebeat.com/ai/the-human-harbor-navigating-identity-and-meaning-in-the-ai-age/,"![Grossman/ChatGPT](https://venturebeat.com/wp-content/uploads/2025/07/Human-Harbor-Cover-Image.jpg?w=578)

* * *

The future is marked by deepening uncertainty about our place in it, and by growing ambiguity about the nature of human purpose itself.[Read More](https://venturebeat.com/ai/the-human-harbor-navigating-identity-and-meaning-in-the-ai-age/)",![Grossman/ChatGPT](https://venturebeat.com/wp-content/uploads/2025/07/Human-Harbor-Cover-Image.jpg?w=578),"[""artificial-intelligence"",""open-source"",""openai"",""transformers"",""anthropic"",""huggingface""]"
a61213c6-e1f9-4e11-9299-ff44df19e5ed,https://venturebeat.com/?p=3014139,c83204e7-d3ca-4e54-a064-1399036f5e7d,Stop vetting engineers like it’s 2021 — the AI-native workforce has arrived,https://venturebeat.com/programming-development/stop-vetting-engineers-like-its-2021-the-ai-native-workforce-has-arrived/,"![VentureBeat/Ideogram](https://venturebeat.com/wp-content/uploads/2025/07/Devs.jpeg?w=578)

* * *

Jobs will fade and rise due to AI. But those who learn to screen, train and build dev teams around AI-enabled talent will write the future.[Read More](https://venturebeat.com/programming-development/stop-vetting-engineers-like-its-2021-the-ai-native-workforce-has-arrived/)",![VentureBeat/Ideogram](https://venturebeat.com/wp-content/uploads/2025/07/Devs.jpeg?w=578),"[""artificial-intelligence"",""collaboration"",""openai"",""anthropic"",""huggingface"",""stanford""]"
641eb8dc-4551-4240-829e-0beda68b1d3c,https://venturebeat.com/?p=3014130,c83204e7-d3ca-4e54-a064-1399036f5e7d,Building voice AI that listens to everyone: Transfer learning and synthetic speech in action,https://venturebeat.com/ai/building-voice-ai-that-listens-to-everyone-transfer-learning-and-synthetic-speech-in-action/,"![illustration of outlined faces with blank speech bubbles representing speaking or speech](https://venturebeat.com/wp-content/uploads/2018/07/conversation.jpg?w=578)

* * *

Enterprises adopting voice AI must consider not just usability, but inclusion. Supporting users with disabilities is a market opportunity.[Read More](https://venturebeat.com/ai/building-voice-ai-that-listens-to-everyone-transfer-learning-and-synthetic-speech-in-action/)",![illustration of outlined faces with blank speech bubbles representing speaking or speech](https://venturebeat.com/wp-content/uploads/2018/07/conversation.jpg?w=578),"[""artificial-intelligence"",""education"",""enterprise"",""interview"",""openai"",""huggingface""]"
6f822e2c-4b0e-4417-8979-9d4a88aa8efa,https://venturebeat.com/?p=3014114,c83204e7-d3ca-4e54-a064-1399036f5e7d,Moonshot AI’s Kimi K2 outperforms GPT-4 in key benchmarks — and it’s free,https://venturebeat.com/ai/moonshot-ais-kimi-k2-outperforms-gpt-4-in-key-benchmarks-and-its-free/,"![Credit: VentureBeat made with Midjourney](https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_moonshot_rocket_launch_56741232-1790-42b9-a82d-854c8a8ee05f.webp?w=578)

* * *

Chinese AI startup Moonshot releases open-source Kimi K2 model that outperforms OpenAI and Anthropic on coding tasks with breakthrough agentic capabilities and competitive pricing.[Read More](https://venturebeat.com/ai/moonshot-ais-kimi-k2-outperforms-gpt-4-in-key-benchmarks-and-its-free/)",![Credit: VentureBeat made with Midjourney](https://venturebeat.com/wp-content/uploads/2025/07/nuneybits_Vector_art_of_moonshot_rocket_launch_56741232-1790-42b9-a82d-854c8a8ee05f.webp?w=578),"[""announcement"",""anthropic"",""artificial-intelligence"",""openai"",""review"",""transformers""]"
b450e61f-9a4b-45a4-9b20-cbb50d95cec6,https://venturebeat.com/?p=3014098,c83204e7-d3ca-4e54-a064-1399036f5e7d,A new paradigm for AI: How &#8216;thinking as optimization&#8217; leads to better general-purpose models,https://venturebeat.com/ai/a-new-paradigm-for-ai-how-thinking-as-optimization-leads-to-better-general-purpose-models/,"![Image credit: VentureBeat with Imagen 4](https://venturebeat.com/wp-content/uploads/2025/07/Energy-based-transformer.png?w=578)

* * *

A new AI model learns to ""think"" longer on hard problems, achieving more robust reasoning and better generalization to novel, unseen tasks.[Read More](https://venturebeat.com/ai/a-new-paradigm-for-ai-how-thinking-as-optimization-leads-to-better-general-purpose-models/)",![Image credit: VentureBeat with Imagen 4](https://venturebeat.com/wp-content/uploads/2025/07/Energy-based-transformer.png?w=578),"[""artificial-intelligence"",""breakthrough"",""energy"",""mathematics"",""performance"",""transformers""]"
33600d72-5c55-4e61-a319-9adad1994d5b,https://venturebeat.com/?p=3014091,c83204e7-d3ca-4e54-a064-1399036f5e7d,Solo.io wins ‘most likely to succeed’ award at VB Transform 2025 innovation showcase,https://venturebeat.com/ai/solo-io-wins-most-likely-to-succeed-award-at-vb-transform-2025-innovation-showcase/,"![Solo.io CEO and founder Idit Levine and Chief Product Officer Keith Babo after winning most likely to succeed at VB Transform&#039;s Innovation Showcase in SF on June 25, 2025. Photo:](https://venturebeat.com/wp-content/uploads/2025/07/VBTRANSFORM25-0743.jpg?w=578)

* * *

Solo.io's Kagent Studio framework allows enterprises to build, secure, run and manage their AI agents in Kubernetes. [Read More](https://venturebeat.com/ai/solo-io-wins-most-likely-to-succeed-award-at-vb-transform-2025-innovation-showcase/)","![Solo.io CEO and founder Idit Levine and Chief Product Officer Keith Babo after winning most likely to succeed at VB Transform&#039;s Innovation Showcase in SF on June 25, 2025. Photo:](https://ventu...","[""artificial-intelligence"",""award"",""breakthrough"",""enterprise"",""kubernetes"",""microsoft""]"
5d50cabd-33ad-4338-86e5-989536a64b98,2507.08802,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?,http://arxiv.org/abs/2507.08802v1,"# The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?

**Authors:** Denis Sutter, Julian Minder, Thomas Hofmann, Tiago Pimentel
**Categories:** cs.LG
**Published:** July 11, 2025

**Abstract:**
The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work.

**Comments:** 42 pages, 17 figures, code available in
  github.com/densutter/non-linear-representation-dilemma

**arXiv ID:** 2507.08802

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08802v1)
- [PDF](http://arxiv.org/pdf/2507.08802v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretabi...","[""algorithms"",""api"",""artificial-intelligence"",""machine-learning"",""natural-language-processing"",""reinforcement-learning""]"
56ee3a25-4a70-434d-a19f-b46dbde3552e,2507.08801,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective,http://arxiv.org/abs/2507.08801v1,"# Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective

**Authors:** Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang
**Categories:** cs.CV, cs.AI, cs.MM
**Published:** July 11, 2025

**Abstract:**
Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.

**Comments:** Code and Models: https://github.com/alibaba-damo-academy/Lumos

**arXiv ID:** 2507.08801

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08801v1)
- [PDF](http://arxiv.org/pdf/2507.08801v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibi...","[""artificial-intelligence"",""computer-science"",""natural-language-processing"",""physics"",""reinforcement-learning"",""transformers""]"
d294322a-f979-4f63-adcc-ae8a57a997a7,2507.08800,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,NeuralOS: Towards Simulating Operating Systems via Neural Generative Models,http://arxiv.org/abs/2507.08800v1,"# NeuralOS: Towards Simulating Operating Systems via Neural Generative Models

**Authors:** Luke Rivard, Sun Sun, Hongyu Guo, Wenhu Chen, Yuntian Deng
**Categories:** cs.CV, cs.AI, cs.CL, cs.HC, cs.LG
**Published:** July 11, 2025

**Abstract:**
We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.

**arXiv ID:** 2507.08800

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08800v1)
- [PDF](http://arxiv.org/pdf/2507.08800v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks comput...","[""artificial-intelligence"",""computer-science"",""deep-learning"",""machine-learning"",""neuroscience"",""physics""]"
cb48c9dc-e840-4426-ac86-73d1cb2cc0e7,2507.08799,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,KV Cache Steering for Inducing Reasoning in Small Language Models,http://arxiv.org/abs/2507.08799v1,"# KV Cache Steering for Inducing Reasoning in Small Language Models

**Authors:** Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano
**Categories:** cs.CL, cs.AI
**Published:** July 11, 2025

**Abstract:**
We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.

**arXiv ID:** 2507.08799

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08799v1)
- [PDF](http://arxiv.org/pdf/2507.08799v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages G...","[""api"",""artificial-intelligence"",""computer-science"",""natural-language-processing"",""physics"",""transformers""]"
c2f4fea2-4075-4372-9b1b-c189d3cd0363,2507.08796,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists,http://arxiv.org/abs/2507.08796v1,"# Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists

**Authors:** Owen Lewis, Neil Ghani, Andrew Dudzik, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković
**Categories:** cs.PL, cs.LG
**Published:** July 11, 2025

**Abstract:**
What should a function that extrapolates beyond known input/output examples look like? This is a tricky question to answer in general, as any function matching the outputs on those examples can in principle be a correct extrapolant. We argue that a ""good"" extrapolant should follow certain kinds of rules, and here we study a particularly appealing criterion for rule-following in list functions: that the function should behave predictably even when certain elements are removed. In functional programming, a standard way to express such removal operations is by using a filter function. Accordingly, our paper introduces a new semantic class of functions -- the filter equivariant functions. We show that this class contains interesting examples, prove some basic theorems about it, and relate it to the well-known class of map equivariant functions. We also present a geometric account of filter equivariants, showing how they correspond naturally to certain simplicial structures. Our highlight result is the amalgamation algorithm, which constructs any filter-equivariant function's output by first studying how it behaves on sublists of the input, in a way that extrapolates perfectly.

**Comments:** 18 pages, 2 figures

**arXiv ID:** 2507.08796

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08796v1)
- [PDF](http://arxiv.org/pdf/2507.08796v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","What should a function that extrapolates beyond known input/output examples look like? This is a tricky question to answer in general, as any function matching the outputs on those examples can in principle be a correct extrapolant. We argue that a ""good"" extrapolant should follow certain kinds of r...","[""algorithms"",""api"",""artificial-intelligence"",""computer-science"",""physics"",""reinforcement-learning""]"
b4abc1f9-3807-4ef2-8692-4b00ad5661b3,2507.08794,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,One Token to Fool LLM-as-a-Judge,http://arxiv.org/abs/2507.08794v1,"# One Token to Fool LLM-as-a-Judge

**Authors:** Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu
**Categories:** cs.LG, cs.CL
**Published:** July 11, 2025

**Abstract:**
Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., "":"" or ""."") or reasoning openers like ""Thought process:"" and ""Let's solve this problem step by step."" can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.

**arXiv ID:** 2507.08794

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08794v1)
- [PDF](http://arxiv.org/pdf/2507.08794v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning task...","[""algorithms"",""artificial-intelligence"",""natural-language-processing"",""reinforcement-learning"",""robotics"",""transformers""]"
06748d22-e815-49dc-a309-b0791659454f,2507.08793,cadbd294-fa19-4bbb-9c0e-127c8f589b4a,Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning,http://arxiv.org/abs/2507.08793v1,"# Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning

**Authors:** James McCarthy, Radu Marinescu, Elizabeth Daly, Ivana Dusparic
**Categories:** cs.LG, cs.AI
**Published:** July 11, 2025

**Abstract:**
Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies that minimise the likelihood of rare and catastrophic constraint violations caused by an environment's inherent randomness. In general, risk-aversion leads to conservative exploration of the environment which typically results in converging to sub-optimal policies that fail to adequately maximise reward or, in some cases, fail to achieve the goal. In this paper, we propose an exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic (ORAC), which constructs an exploratory policy by maximising a local upper confidence bound of the state-action reward value function whilst minimising a local lower confidence bound of the risk-averse state-action cost value function. Specifically, at each step, the weighting assigned to the cost value is increased or decreased if it exceeds or falls below the safety constraint value. This way the policy is encouraged to explore uncertain regions of the environment to discover high reward states whilst still satisfying the safety constraints. Our experimental results demonstrate that the ORAC approach prevents convergence to sub-optimal policies and improves significantly the reward-cost trade-off in various continuous control tasks such as Safety-Gymnasium and a complex building energy management environment CityLearn.

**arXiv ID:** 2507.08793

**Links:**
- [arXiv Page](http://arxiv.org/abs/2507.08793v1)
- [PDF](http://arxiv.org/pdf/2507.08793v1)

---
*This paper was published on arXiv, a preprint repository for scientific papers in physics, mathematics, computer science, and related fields.*","Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies that minimise the likelihood of rare and catastrophic constraint violations caused by an environment's inherent randomness. In general, risk-aversion leads to conservative exploration of the environment which typically res...","[""artificial-intelligence"",""computer-science"",""energy"",""mathematics"",""physics"",""reinforcement-learning""]"
05c4fd04-9569-49c0-81d5-e9a9ce8102b3,fastapi@0.116.1,e0ec7b85-25a1-4fac-b427-33bddd886cb1,"fastapi 0.116.1 - FastAPI framework, high performance, easy to learn, fast to code, ready for production",https://pypi.org/project/fastapi/0.116.1/,"# fastapi 0.116.1

FastAPI framework, high performance, easy to learn, fast to code, ready for production

**Version:** 0.116.1
**Author:** Unknown
**License:** Not specified

**Python Version:** >=3.8

**Description:**
<p align=""center"">
  <a href=""https://fastapi.tiangolo.com""><img src=""https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png"" alt=""FastAPI""></a>
</p>
<p align=""center"">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster"" target=""_blank"">
    <img src=""https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&branch=master"" alt=""Test"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg"" alt=""Coverage"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package"" alt=""Package version"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img sr

**Key Dependencies:**
- starlette<0.48.0,>=0.40.0
- pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4
- typing-extensions>=4.8.0
- fastapi-cli[standard]>=0.0.8; extra == ""standard""
- httpx>=0.23.0; extra == ""standard""

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Changelog](https://fastapi.tiangolo.com/release-notes/)
- [Documentation](https://fastapi.tiangolo.com/)
- [Homepage](https://github.com/fastapi/fastapi)
","FastAPI framework, high performance, easy to learn, fast to code, ready for production","[""api"",""machine-learning"",""manufacturing"",""performance"",""python"",""reinforcement-learning""]"
ec52a2f5-6152-4912-9750-2a90c9e7d763,pandas@2.3.1,e0ec7b85-25a1-4fac-b427-33bddd886cb1,"pandas 2.3.1 - Powerful data structures for data analysis, time series, and statistics",https://pypi.org/project/pandas/2.3.1/,"# pandas 2.3.1

Powerful data structures for data analysis, time series, and statistics

**Version:** 2.3.1
**Author:** Unknown
**License:** BSD 3-Clause License
         
         Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team
         All rights reserved.
         
         Copyright (c) 2011-2023, Open source contributors.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
         * Redistributions of source code must retain the above copyright notice, this
           list of conditions and the following disclaimer.
         
         * Redistributions in binary form must reproduce the above copyright notice,
           this list of conditions and the following disclaimer in the documentation
           and/or other materials provided with the distribution.
         
         * Neither the name of the copyright holder nor the names of its
           contributors may be used to endorse or promote products derived from
           this software without specific prior written permission.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
         Copyright (c) 2010-2019 Keith Goodman
         Copyright (c) 2019 Bottleneck Developers
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
             * Redistributions of source code must retain the above copyright notice,
               this list of conditions and the following disclaimer.
         
             * Redistributions in binary form must reproduce the above copyright
               notice, this list of conditions and the following disclaimer in the
               documentation and/or other materials provided with the distribution.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
         ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
         LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
         CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
         SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
         INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
         CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
         ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
         POSSIBILITY OF SUCH DAMAGE.Copyright 2017- Paul Ganssle <paul@ganssle.io>
         Copyright 2017- dateutil contributors (see AUTHORS file)
         
            Licensed under the Apache License, Version 2.0 (the ""License"");
            you may not use this file except in compliance with the License.
            You may obtain a copy of the License at
         
                http://www.apache.org/licenses/LICENSE-2.0
         
            Unless required by applicable law or agreed to in writing, software
            distributed under the License is distributed on an ""AS IS"" BASIS,
            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            See the License for the specific language governing permissions and
            limitations under the License.
         
         The above license applies to all contributions after 2017-12-01, as well as
         all contributions that have been re-licensed (see AUTHORS file for the list of
         contributors who have re-licensed their code).
         --------------------------------------------------------------------------------
         dateutil - Extensions to the standard Python datetime module.
         
         Copyright (c) 2003-2011 - Gustavo Niemeyer <gustavo@niemeyer.net>
         Copyright (c) 2012-2014 - Tomi Pieviläinen <tomi.pievilainen@iki.fi>
         Copyright (c) 2014-2016 - Yaron de Leeuw <me@jarondl.net>
         Copyright (c) 2015-     - Paul Ganssle <paul@ganssle.io>
         Copyright (c) 2015-     - dateutil contributors (see AUTHORS file)
         
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
             * Redistributions of source code must retain the above copyright notice,
               this list of conditions and the following disclaimer.
             * Redistributions in binary form must reproduce the above copyright notice,
               this list of conditions and the following disclaimer in the documentation
               and/or other materials provided with the distribution.
             * Neither the name of the copyright holder nor the names of its
               contributors may be used to endorse or promote products derived from
               this software without specific prior written permission.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
         ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
         LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
         A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
         CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
         EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
         PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
         PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
         LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
         NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
         SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
         
         The above BSD License Applies to all code, even that also covered by Apache 2.0.# MIT License
         
         Copyright (c) 2019 Hadley Wickham; RStudio; and Evan Miller
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
         of this software and associated documentation files (the ""Software""), to deal
         in the Software without restriction, including without limitation the rights
         to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
         copies of the Software, and to permit persons to whom the Software is
         furnished to do so, subject to the following conditions:
         
         The above copyright notice and this permission notice shall be included in all
         copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
         IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
         FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
         AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
         LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
         OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
         SOFTWARE.
         Based on http://opensource.org/licenses/MIT
         
         This is a template. Complete and ship as file LICENSE the following 2
         lines (only)
         
         YEAR:
         COPYRIGHT HOLDER:
         
         and specify as
         
         License: MIT + file LICENSE
         
         Copyright (c) <YEAR>, <COPYRIGHT HOLDER>
         
         Permission is hereby granted, free of charge, to any person obtaining
         a copy of this software and associated documentation files (the
         ""Software""), to deal in the Software without restriction, including
         without limitation the rights to use, copy, modify, merge, publish,
         distribute, sublicense, and/or sell copies of the Software, and to
         permit persons to whom the Software is furnished to do so, subject to
         the following conditions:
         
         The above copyright notice and this permission notice shall be
         included in all copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
         EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
         MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
         NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
         LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
         OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
         WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
         The MIT License
         
         Copyright (c) 2008-     Attractive Chaos <attractor@live.co.uk>
         
         Permission is hereby granted, free of charge, to any person obtaining
         a copy of this software and associated documentation files (the
         ""Software""), to deal in the Software without restriction, including
         without limitation the rights to use, copy, modify, merge, publish,
         distribute, sublicense, and/or sell copies of the Software, and to
         permit persons to whom the Software is furnished to do so, subject to
         the following conditions:
         
         The above copyright notice and this permission notice shall be
         included in all copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
         EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
         MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
         NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
         BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
         ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
         CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
         SOFTWARE.musl as a whole is licensed under the following standard MIT license:
         
         ----------------------------------------------------------------------
         Copyright © 2005-2020 Rich Felker, et al.
         
         Permission is hereby granted, free of charge, to any person obtaining
         a copy of this software and associated documentation files (the
         ""Software""), to deal in the Software without restriction, including
         without limitation the rights to use, copy, modify, merge, publish,
         distribute, sublicense, and/or sell copies of the Software, and to
         permit persons to whom the Software is furnished to do so, subject to
         the following conditions:
         
         The above copyright notice and this permission notice shall be
         included in all copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
         EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
         MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
         IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
         CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
         TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
         SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
         ----------------------------------------------------------------------
         
         Authors/contributors include:
         
         A. Wilcox
         Ada Worcester
         Alex Dowad
         Alex Suykov
         Alexander Monakov
         Andre McCurdy
         Andrew Kelley
         Anthony G. Basile
         Aric Belsito
         Arvid Picciani
         Bartosz Brachaczek
         Benjamin Peterson
         Bobby Bingham
         Boris Brezillon
         Brent Cook
         Chris Spiegel
         Clément Vasseur
         Daniel Micay
         Daniel Sabogal
         Daurnimator
         David Carlier
         David Edelsohn
         Denys Vlasenko
         Dmitry Ivanov
         Dmitry V. Levin
         Drew DeVault
         Emil Renner Berthing
         Fangrui Song
         Felix Fietkau
         Felix Janda
         Gianluca Anzolin
         Hauke Mehrtens
         He X
         Hiltjo Posthuma
         Isaac Dunham
         Jaydeep Patil
         Jens Gustedt
         Jeremy Huntwork
         Jo-Philipp Wich
         Joakim Sindholt
         John Spencer
         Julien Ramseier
         Justin Cormack
         Kaarle Ritvanen
         Khem Raj
         Kylie McClain
         Leah Neukirchen
         Luca Barbato
         Luka Perkov
         M Farkas-Dyck (Strake)
         Mahesh Bodapati
         Markus Wichmann
         Masanori Ogino
         Michael Clark
         Michael Forney
         Mikhail Kremnyov
         Natanael Copa
         Nicholas J. Kain
         orc
         Pascal Cuoq
         Patrick Oppenlander
         Petr Hosek
         Petr Skocik
         Pierre Carrier
         Reini Urban
         Rich Felker
         Richard Pennington
         Ryan Fairfax
         Samuel Holland
         Segev Finer
         Shiz
         sin
         Solar Designer
         Stefan Kristiansson
         Stefan O'Rear
         Szabolcs Nagy
         Timo Teräs
         Trutz Behn
         Valentin Ochs
         Will Dietz
         William Haddon
         William Pitcock
         
         Portions of this software are derived from third-party works licensed
         under terms compatible with the above MIT license:
         
         The TRE regular expression implementation (src/regex/reg* and
         src/regex/tre*) is Copyright © 2001-2008 Ville Laurikari and licensed
         under a 2-clause BSD license (license text in the source files). The
         included version has been heavily modified by Rich Felker in 2012, in
         the interests of size, simplicity, and namespace cleanliness.
         
         Much of the math library code (src/math/* and src/complex/*) is
         Copyright © 1993,2004 Sun Microsystems or
         Copyright © 2003-2011 David Schultz or
         Copyright © 2003-2009 Steven G. Kargl or
         Copyright © 2003-2009 Bruce D. Evans or
         Copyright © 2008 Stephen L. Moshier or
         Copyright © 2017-2018 Arm Limited
         and labelled as such in comments in the individual source files. All
         have been licensed under extremely permissive terms.
         
         The ARM memcpy code (src/string/arm/memcpy.S) is Copyright © 2008
         The Android Open Source Project and is licensed under a two-clause BSD
         license. It was taken from Bionic libc, used on Android.
         
         The AArch64 memcpy and memset code (src/string/aarch64/*) are
         Copyright © 1999-2019, Arm Limited.
         
         The implementation of DES for crypt (src/crypt/crypt_des.c) is
         Copyright © 1994 David Burren. It is licensed under a BSD license.
         
         The implementation of blowfish crypt (src/crypt/crypt_blowfish.c) was
         originally written by Solar Designer and placed into the public
         domain. The code also comes with a fallback permissive license for use
         in jurisdictions that may not recognize the public domain.
         
         The smoothsort implementation (src/stdlib/qsort.c) is Copyright © 2011
         Valentin Ochs and is licensed under an MIT-style license.
         
         The x86_64 port was written by Nicholas J. Kain and is licensed under
         the standard MIT terms.
         
         The mips and microblaze ports were originally written by Richard
         Pennington for use in the ellcc project. The original code was adapted
         by Rich Felker for build system and code conventions during upstream
         integration. It is licensed under the standard MIT terms.
         
         The mips64 port was contributed by Imagination Technologies and is
         licensed under the standard MIT terms.
         
         The powerpc port was also originally written by Richard Pennington,
         and later supplemented and integrated by John Spencer. It is licensed
         under the standard MIT terms.
         
         All other files which have no copyright comments are original works
         produced specifically for use as part of this library, written either
         by Rich Felker, the main author of the library, or by one or more
         contibutors listed above. Details on authorship of individual files
         can be found in the git version control history of the project. The
         omission of copyright and license comments in each file is in the
         interest of source tree size.
         
         In addition, permission is hereby granted for all public header files
         (include/* and arch/*/bits/*) and crt files intended to be linked into
         applications (crt/*, ldso/dlstart.c, and arch/*/crt_arch.h) to omit
         the copyright notice and permission notice otherwise required by the
         license, and to use these files without any requirement of
         attribution. These files include substantial contributions from:
         
         Bobby Bingham
         John Spencer
         Nicholas J. Kain
         Rich Felker
         Richard Pennington
         Stefan Kristiansson
         Szabolcs Nagy
         
         all of whom have explicitly granted such permission.
         
         This file previously contained text expressing a belief that most of
         the files covered by the above exception were sufficiently trivial not
         to be subject to copyright, resulting in confusion over whether it
         negated the permissions granted in the license. In the spirit of
         permissive licensing, and of not having licensing issues being an
         obstacle to adoption, that text has been removed.Copyright (c) 2005-2023, NumPy Developers.
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are
         met:
         
             * Redistributions of source code must retain the above copyright
                notice, this list of conditions and the following disclaimer.
         
             * Redistributions in binary form must reproduce the above
                copyright notice, this list of conditions and the following
                disclaimer in the documentation and/or other materials provided
                with the distribution.
         
             * Neither the name of the NumPy Developers nor the names of any
                contributors may be used to endorse or promote products derived
                from this software without specific prior written permission.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
         ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
         LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
         A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
         OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
         SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
         LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
         DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
         THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
         (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
                                          Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
         
            1. Definitions.
         
               ""License"" shall mean the terms and conditions for use, reproduction,
               and distribution as defined by Sections 1 through 9 of this document.
         
               ""Licensor"" shall mean the copyright owner or entity authorized by
               the copyright owner that is granting the License.
         
               ""Legal Entity"" shall mean the union of the acting entity and all
               other entities that control, are controlled by, or are under common
               control with that entity. For the purposes of this definition,
               ""control"" means (i) the power, direct or indirect, to cause the
               direction or management of such entity, whether by contract or
               otherwise, or (ii) ownership of fifty percent (50%) or more of the
               outstanding shares, or (iii) beneficial ownership of such entity.
         
               ""You"" (or ""Your"") shall mean an individual or Legal Entity
               exercising permissions granted by this License.
         
               ""Source"" form shall mean the preferred form for making modifications,
               including but not limited to software source code, documentation
               source, and configuration files.
         
               ""Object"" form shall mean any form resulting from mechanical
               transformation or translation of a Source form, including but
               not limited to compiled object code, generated documentation,
               and conversions to other media types.
         
               ""Work"" shall mean the work of authorship, whether in Source or
               Object form, made available under the License, as indicated by a
               copyright notice that is included in or attached to the work
               (an example is provided in the Appendix below).
         
               ""Derivative Works"" shall mean any work, whether in Source or Object
               form, that is based on (or derived from) the Work and for which the
               editorial revisions, annotations, elaborations, or other modifications
               represent, as a whole, an original work of authorship. For the purposes
               of this License, Derivative Works shall not include works that remain
               separable from, or merely link (or bind by name) to the interfaces of,
               the Work and Derivative Works thereof.
         
               ""Contribution"" shall mean any work of authorship, including
               the original version of the Work and any modifications or additions
               to that Work or Derivative Works thereof, that is intentionally
               submitted to Licensor for inclusion in the Work by the copyright owner
               or by an individual or Legal Entity authorized to submit on behalf of
               the copyright owner. For the purposes of this definition, ""submitted""
               means any form of electronic, verbal, or written communication sent
               to the Licensor or its representatives, including but not limited to
               communication on electronic mailing lists, source code control systems,
               and issue tracking systems that are managed by, or on behalf of, the
               Licensor for the purpose of discussing and improving the Work, but
               excluding communication that is conspicuously marked or otherwise
               designated in writing by the copyright owner as ""Not a Contribution.""
         
               ""Contributor"" shall mean Licensor and any individual or Legal Entity
               on behalf of whom a Contribution has been received by Licensor and
               subsequently incorporated within the Work.
         
            2. Grant of Copyright License. Subject to the terms and conditions of
               this License, each Contributor hereby grants to You a perpetual,
               worldwide, non-exclusive, no-charge, royalty-free, irrevocable
               copyright license to reproduce, prepare Derivative Works of,
               publicly display, publicly perform, sublicense, and distribute the
               Work and such Derivative Works in Source or Object form.
         
            3. Grant of Patent License. Subject to the terms and conditions of
               this License, each Contributor hereby grants to You a perpetual,
               worldwide, non-exclusive, no-charge, royalty-free, irrevocable
               (except as stated in this section) patent license to make, have made,
               use, offer to sell, sell, import, and otherwise transfer the Work,
               where such license applies only to those patent claims licensable
               by such Contributor that are necessarily infringed by their
               Contribution(s) alone or by combination of their Contribution(s)
               with the Work to which such Contribution(s) was submitted. If You
               institute patent litigation against any entity (including a
               cross-claim or counterclaim in a lawsuit) alleging that the Work
               or a Contribution incorporated within the Work constitutes direct
               or contributory patent infringement, then any patent licenses
               granted to You under this License for that Work shall terminate
               as of the date such litigation is filed.
         
            4. Redistribution. You may reproduce and distribute copies of the
               Work or Derivative Works thereof in any medium, with or without
               modifications, and in Source or Object form, provided that You
               meet the following conditions:
         
               (a) You must give any other recipients of the Work or
                   Derivative Works a copy of this License; and
         
               (b) You must cause any modified files to carry prominent notices
                   stating that You changed the files; and
         
               (c) You must retain, in the Source form of any Derivative Works
                   that You distribute, all copyright, patent, trademark, and
                   attribution notices from the Source form of the Work,
                   excluding those notices that do not pertain to any part of
                   the Derivative Works; and
         
               (d) If the Work includes a ""NOTICE"" text file as part of its
                   distribution, then any Derivative Works that You distribute must
                   include a readable copy of the attribution notices contained
                   within such NOTICE file, excluding those notices that do not
                   pertain to any part of the Derivative Works, in at least one
                   of the following places: within a NOTICE text file distributed
                   as part of the Derivative Works; within the Source form or
                   documentation, if provided along with the Derivative Works; or,
                   within a display generated by the Derivative Works, if and
                   wherever such third-party notices normally appear. The contents
                   of the NOTICE file are for informational purposes only and
                   do not modify the License. You may add Your own attribution
                   notices within Derivative Works that You distribute, alongside
                   or as an addendum to the NOTICE text from the Work, provided
                   that such additional attribution notices cannot be construed
                   as modifying the License.
         
               You may add Your own copyright statement to Your modifications and
               may provide additional or different license terms and conditions
               for use, reproduction, or distribution of Your modifications, or
               for any such Derivative Works as a whole, provided Your use,
               reproduction, and distribution of the Work otherwise complies with
               the conditions stated in this License.
         
            5. Submission of Contributions. Unless You explicitly state otherwise,
               any Contribution intentionally submitted for inclusion in the Work
               by You to the Licensor shall be under the terms and conditions of
               this License, without any additional terms or conditions.
               Notwithstanding the above, nothing herein shall supersede or modify
               the terms of any separate license agreement you may have executed
               with Licensor regarding such Contributions.
         
            6. Trademarks. This License does not grant permission to use the trade
               names, trademarks, service marks, or product names of the Licensor,
               except as required for reasonable and customary use in describing the
               origin of the Work and reproducing the content of the NOTICE file.
         
            7. Disclaimer of Warranty. Unless required by applicable law or
               agreed to in writing, Licensor provides the Work (and each
               Contributor provides its Contributions) on an ""AS IS"" BASIS,
               WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
               implied, including, without limitation, any warranties or conditions
               of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
               PARTICULAR PURPOSE. You are solely responsible for determining the
               appropriateness of using or redistributing the Work and assume any
               risks associated with Your exercise of permissions under this License.
         
            8. Limitation of Liability. In no event and under no legal theory,
               whether in tort (including negligence), contract, or otherwise,
               unless required by applicable law (such as deliberate and grossly
               negligent acts) or agreed to in writing, shall any Contributor be
               liable to You for damages, including any direct, indirect, special,
               incidental, or consequential damages of any character arising as a
               result of this License or out of the use or inability to use the
               Work (including but not limited to damages for loss of goodwill,
               work stoppage, computer failure or malfunction, or any and all
               other commercial damages or losses), even if such Contributor
               has been advised of the possibility of such damages.
         
            9. Accepting Warranty or Additional Liability. While redistributing
               the Work or Derivative Works thereof, You may choose to offer,
               and charge a fee for, acceptance of support, warranty, indemnity,
               or other liability obligations and/or rights consistent with this
               License. However, in accepting such obligations, You may act only
               on Your own behalf and on Your sole responsibility, not on behalf
               of any other Contributor, and only if You agree to indemnify,
               defend, and hold each Contributor harmless for any liability
               incurred by, or claims asserted against, such Contributor by reason
               of your accepting any such warranty or additional liability.
         
            END OF TERMS AND CONDITIONS
         
         
         Copyright (c) Donald Stufft and individual contributors.
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
             1. Redistributions of source code must retain the above copyright notice,
                this list of conditions and the following disclaimer.
         
             2. Redistributions in binary form must reproduce the above copyright
                notice, this list of conditions and the following disclaimer in the
                documentation and/or other materials provided with the distribution.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
         ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
         WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.A. HISTORY OF THE SOFTWARE
         ==========================
         
         Python was created in the early 1990s by Guido van Rossum at Stichting
         Mathematisch Centrum (CWI, see https://www.cwi.nl) in the Netherlands
         as a successor of a language called ABC.  Guido remains Python's
         principal author, although it includes many contributions from others.
         
         In 1995, Guido continued his work on Python at the Corporation for
         National Research Initiatives (CNRI, see https://www.cnri.reston.va.us)
         in Reston, Virginia where he released several versions of the
         software.
         
         In May 2000, Guido and the Python core development team moved to
         BeOpen.com to form the BeOpen PythonLabs team.  In October of the same
         year, the PythonLabs team moved to Digital Creations, which became
         Zope Corporation.  In 2001, the Python Software Foundation (PSF, see
         https://www.python.org/psf/) was formed, a non-profit organization
         created specifically to own Python-related Intellectual Property.
         Zope Corporation was a sponsoring member of the PSF.
         
         All Python releases are Open Source (see https://opensource.org for
         the Open Source Definition).  Historically, most, but not all, Python
         releases have also been GPL-compatible; the table below summarizes
         the various releases.
         
             Release         Derived     Year        Owner       GPL-
                             from                                compatible? (1)
         
             0.9.0 thru 1.2              1991-1995   CWI         yes
             1.3 thru 1.5.2  1.2         1995-1999   CNRI        yes
             1.6             1.5.2       2000        CNRI        no
             2.0             1.6         2000        BeOpen.com  no
             1.6.1           1.6         2001        CNRI        yes (2)
             2.1             2.0+1.6.1   2001        PSF         no
             2.0.1           2.0+1.6.1   2001        PSF         yes
             2.1.1           2.1+2.0.1   2001        PSF         yes
             2.1.2           2.1.1       2002        PSF         yes
             2.1.3           2.1.2       2002        PSF         yes
             2.2 and above   2.1.1       2001-now    PSF         yes
         
         Footnotes:
         
         (1) GPL-compatible doesn't mean that we're distributing Python under
             the GPL.  All Python licenses, unlike the GPL, let you distribute
             a modified version without making your changes open source.  The
             GPL-compatible licenses make it possible to combine Python with
             other software that is released under the GPL; the others don't.
         
         (2) According to Richard Stallman, 1.6.1 is not GPL-compatible,
             because its license has a choice of law clause.  According to
             CNRI, however, Stallman's lawyer has told CNRI's lawyer that 1.6.1
             is ""not incompatible"" with the GPL.
         
         Thanks to the many outside volunteers who have worked under Guido's
         direction to make these releases possible.
         
         
         B. TERMS AND CONDITIONS FOR ACCESSING OR OTHERWISE USING PYTHON
         ===============================================================
         
         Python software and documentation are licensed under the
         Python Software Foundation License Version 2.
         
         Starting with Python 3.8.6, examples, recipes, and other code in
         the documentation are dual licensed under the PSF License Version 2
         and the Zero-Clause BSD license.
         
         Some software incorporated into Python is under different licenses.
         The licenses are listed with code falling under that license.
         
         
         PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2
         --------------------------------------------
         
         1. This LICENSE AGREEMENT is between the Python Software Foundation
         (""PSF""), and the Individual or Organization (""Licensee"") accessing and
         otherwise using this software (""Python"") in source or binary form and
         its associated documentation.
         
         2. Subject to the terms and conditions of this License Agreement, PSF hereby
         grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,
         analyze, test, perform and/or display publicly, prepare derivative works,
         distribute, and otherwise use Python alone or in any derivative version,
         provided, however, that PSF's License Agreement and PSF's notice of copyright,
         i.e., ""Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
         2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023 Python Software Foundation;
         All Rights Reserved"" are retained in Python alone or in any derivative version
         prepared by Licensee.
         
         3. In the event Licensee prepares a derivative work that is based on
         or incorporates Python or any part thereof, and wants to make
         the derivative work available to others as provided herein, then
         Licensee hereby agrees to include in any such work a brief summary of
         the changes made to Python.
         
         4. PSF is making Python available to Licensee on an ""AS IS""
         basis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
         IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND
         DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS
         FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT
         INFRINGE ANY THIRD PARTY RIGHTS.
         
         5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON
         FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS
         A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON,
         OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.
         
         6. This License Agreement will automatically terminate upon a material
         breach of its terms and conditions.
         
         7. Nothing in this License Agreement shall be deemed to create any
         relationship of agency, partnership, or joint venture between PSF and
         Licensee.  This License Agreement does not grant permission to use PSF
         trademarks or trade name in a trademark sense to endorse or promote
         products or services of Licensee, or any third party.
         
         8. By copying, installing or otherwise using Python, Licensee
         agrees to be bound by the terms and conditions of this License
         Agreement.
         
         
         BEOPEN.COM LICENSE AGREEMENT FOR PYTHON 2.0
         -------------------------------------------
         
         BEOPEN PYTHON OPEN SOURCE LICENSE AGREEMENT VERSION 1
         
         1. This LICENSE AGREEMENT is between BeOpen.com (""BeOpen""), having an
         office at 160 Saratoga Avenue, Santa Clara, CA 95051, and the
         Individual or Organization (""Licensee"") accessing and otherwise using
         this software in source or binary form and its associated
         documentation (""the Software"").
         
         2. Subject to the terms and conditions of this BeOpen Python License
         Agreement, BeOpen hereby grants Licensee a non-exclusive,
         royalty-free, world-wide license to reproduce, analyze, test, perform
         and/or display publicly, prepare derivative works, distribute, and
         otherwise use the Software alone or in any derivative version,
         provided, however, that the BeOpen Python License is retained in the
         Software, alone or in any derivative version prepared by Licensee.
         
         3. BeOpen is making the Software available to Licensee on an ""AS IS""
         basis.  BEOPEN MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
         IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, BEOPEN MAKES NO AND
         DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS
         FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE SOFTWARE WILL NOT
         INFRINGE ANY THIRD PARTY RIGHTS.
         
         4. BEOPEN SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF THE
         SOFTWARE FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS
         AS A RESULT OF USING, MODIFYING OR DISTRIBUTING THE SOFTWARE, OR ANY
         DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.
         
         5. This License Agreement will automatically terminate upon a material
         breach of its terms and conditions.
         
         6. This License Agreement shall be governed by and interpreted in all
         respects by the law of the State of California, excluding conflict of
         law provisions.  Nothing in this License Agreement shall be deemed to
         create any relationship of agency, partnership, or joint venture
         between BeOpen and Licensee.  This License Agreement does not grant
         permission to use BeOpen trademarks or trade names in a trademark
         sense to endorse or promote products or services of Licensee, or any
         third party.  As an exception, the ""BeOpen Python"" logos available at
         http://www.pythonlabs.com/logos.html may be used according to the
         permissions granted on that web page.
         
         7. By copying, installing or otherwise using the software, Licensee
         agrees to be bound by the terms and conditions of this License
         Agreement.
         
         
         CNRI LICENSE AGREEMENT FOR PYTHON 1.6.1
         ---------------------------------------
         
         1. This LICENSE AGREEMENT is between the Corporation for National
         Research Initiatives, having an office at 1895 Preston White Drive,
         Reston, VA 20191 (""CNRI""), and the Individual or Organization
         (""Licensee"") accessing and otherwise using Python 1.6.1 software in
         source or binary form and its associated documentation.
         
         2. Subject to the terms and conditions of this License Agreement, CNRI
         hereby grants Licensee a nonexclusive, royalty-free, world-wide
         license to reproduce, analyze, test, perform and/or display publicly,
         prepare derivative works, distribute, and otherwise use Python 1.6.1
         alone or in any derivative version, provided, however, that CNRI's
         License Agreement and CNRI's notice of copyright, i.e., ""Copyright (c)
         1995-2001 Corporation for National Research Initiatives; All Rights
         Reserved"" are retained in Python 1.6.1 alone or in any derivative
         version prepared by Licensee.  Alternately, in lieu of CNRI's License
         Agreement, Licensee may substitute the following text (omitting the
         quotes): ""Python 1.6.1 is made available subject to the terms and
         conditions in CNRI's License Agreement.  This Agreement together with
         Python 1.6.1 may be located on the internet using the following
         unique, persistent identifier (known as a handle): 1895.22/1013.  This
         Agreement may also be obtained from a proxy server on the internet
         using the following URL: http://hdl.handle.net/1895.22/1013"".
         
         3. In the event Licensee prepares a derivative work that is based on
         or incorporates Python 1.6.1 or any part thereof, and wants to make
         the derivative work available to others as provided herein, then
         Licensee hereby agrees to include in any such work a brief summary of
         the changes made to Python 1.6.1.
         
         4. CNRI is making Python 1.6.1 available to Licensee on an ""AS IS""
         basis.  CNRI MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
         IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, CNRI MAKES NO AND
         DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS
         FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON 1.6.1 WILL NOT
         INFRINGE ANY THIRD PARTY RIGHTS.
         
         5. CNRI SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON
         1.6.1 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS
         A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 1.6.1,
         OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.
         
         6. This License Agreement will automatically terminate upon a material
         breach of its terms and conditions.
         
         7. This License Agreement shall be governed by the federal
         intellectual property law of the United States, including without
         limitation the federal copyright law, and, to the extent such
         U.S. federal law does not apply, by the law of the Commonwealth of
         Virginia, excluding Virginia's conflict of law provisions.
         Notwithstanding the foregoing, with regard to derivative works based
         on Python 1.6.1 that incorporate non-separable material that was
         previously distributed under the GNU General Public License (GPL), the
         law of the Commonwealth of Virginia shall govern this License
         Agreement only as to issues arising under or with respect to
         Paragraphs 4, 5, and 7 of this License Agreement.  Nothing in this
         License Agreement shall be deemed to create any relationship of
         agency, partnership, or joint venture between CNRI and Licensee.  This
         License Agreement does not grant permission to use CNRI trademarks or
         trade name in a trademark sense to endorse or promote products or
         services of Licensee, or any third party.
         
         8. By clicking on the ""ACCEPT"" button where indicated, or by copying,
         installing or otherwise using Python 1.6.1, Licensee agrees to be
         bound by the terms and conditions of this License Agreement.
         
                 ACCEPT
         
         
         CWI LICENSE AGREEMENT FOR PYTHON 0.9.0 THROUGH 1.2
         --------------------------------------------------
         
         Copyright (c) 1991 - 1995, Stichting Mathematisch Centrum Amsterdam,
         The Netherlands.  All rights reserved.
         
         Permission to use, copy, modify, and distribute this software and its
         documentation for any purpose and without fee is hereby granted,
         provided that the above copyright notice appear in all copies and that
         both that copyright notice and this permission notice appear in
         supporting documentation, and that the name of Stichting Mathematisch
         Centrum or CWI not be used in advertising or publicity pertaining to
         distribution of the software without specific, written prior
         permission.
         
         STICHTING MATHEMATISCH CENTRUM DISCLAIMS ALL WARRANTIES WITH REGARD TO
         THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
         FITNESS, IN NO EVENT SHALL STICHTING MATHEMATISCH CENTRUM BE LIABLE
         FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
         WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
         ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
         OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
         
         ZERO-CLAUSE BSD LICENSE FOR CODE IN THE PYTHON DOCUMENTATION
         ----------------------------------------------------------------------
         
         Permission to use, copy, modify, and/or distribute this software for any
         purpose with or without fee is hereby granted.
         
         THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH
         REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY
         AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,
         INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
         LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
         OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
         PERFORMANCE OF THIS SOFTWARE.
         Copyright (c) 2014, Al Sweigart
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
         * Redistributions of source code must retain the above copyright notice, this
           list of conditions and the following disclaimer.
         
         * Redistributions in binary form must reproduce the above copyright notice,
           this list of conditions and the following disclaimer in the documentation
           and/or other materials provided with the distribution.
         
         * Neither the name of the {organization} nor the names of its
           contributors may be used to endorse or promote products derived from
           this software without specific prior written permission.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.Copyright (c) 2017 Anthony Sottile
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
         of this software and associated documentation files (the ""Software""), to deal
         in the Software without restriction, including without limitation the rights
         to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
         copies of the Software, and to permit persons to whom the Software is
         furnished to do so, subject to the following conditions:
         
         The above copyright notice and this permission notice shall be included in
         all copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
         IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
         FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
         AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
         LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
         OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
         THE SOFTWARE.Copyright (c) 2015-2019 Jared Hobbs
         
         Permission is hereby granted, free of charge, to any person obtaining a copy of
         this software and associated documentation files (the ""Software""), to deal in
         the Software without restriction, including without limitation the rights to
         use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
         of the Software, and to permit persons to whom the Software is furnished to do
         so, subject to the following conditions:
         
         The above copyright notice and this permission notice shall be included in all
         copies or substantial portions of the Software.
         
         THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
         IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
         FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
         AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
         LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
         OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
         SOFTWARE.Developed by ESN, an Electronic Arts Inc. studio.
         Copyright (c) 2014, Electronic Arts Inc.
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         * Redistributions of source code must retain the above copyright
         notice, this list of conditions and the following disclaimer.
         * Redistributions in binary form must reproduce the above copyright
         notice, this list of conditions and the following disclaimer in the
         documentation and/or other materials provided with the distribution.
         * Neither the name of ESN, Electronic Arts Inc. nor the
         names of its contributors may be used to endorse or promote products
         derived from this software without specific prior written permission.
         
         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
         ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
         WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
         DISCLAIMED. IN NO EVENT SHALL ELECTRONIC ARTS INC. BE LIABLE
         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
         (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
         LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
         ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
         (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
         SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
         
         ----
         
         Portions of code from MODP_ASCII - Ascii transformations (upper/lower, etc)
         https://github.com/client9/stringencoders
         
           Copyright 2005, 2006, 2007
           Nick Galbreath -- nickg [at] modp [dot] com
           All rights reserved.
         
           Redistribution and use in source and binary forms, with or without
           modification, are permitted provided that the following conditions are
           met:
         
             Redistributions of source code must retain the above copyright
             notice, this list of conditions and the following disclaimer.
         
             Redistributions in binary form must reproduce the above copyright
             notice, this list of conditions and the following disclaimer in the
             documentation and/or other materials provided with the distribution.
         
             Neither the name of the modp.com nor the names of its
             contributors may be used to endorse or promote products derived from
             this software without specific prior written permission.
         
           THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
           ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
           LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
           A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
           OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
           SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
           LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
           DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
           THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
           (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
           OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
         
           This is the standard ""new"" BSD license:
           http://www.opensource.org/licenses/bsd-license.php
         
         https://github.com/client9/stringencoders/blob/cfd5c1507325ae497ea9bacdacba12c0ffd79d30/COPYING
         
         ----
         
         Numeric decoder derived from from TCL library
         https://opensource.apple.com/source/tcl/tcl-14/tcl/license.terms
          * Copyright (c) 1988-1993 The Regents of the University of California.
          * Copyright (c) 1994 Sun Microsystems, Inc.
         
           This software is copyrighted by the Regents of the University of
           California, Sun Microsystems, Inc., Scriptics Corporation, ActiveState
           Corporation and other parties.  The following terms apply to all files
           associated with the software unless explicitly disclaimed in
           individual files.
         
           The authors hereby grant permission to use, copy, modify, distribute,
           and license this software and its documentation for any purpose, provided
           that existing copyright notices are retained in all copies and that this
           notice is included verbatim in any distributions. No written agreement,
           license, or royalty fee is required for any of the authorized uses.
           Modifications to this software may be copyrighted by their authors
           and need not follow the licensing terms described here, provided that
           the new terms are clearly indicated on the first page of each file where
           they apply.
         
           IN NO EVENT SHALL THE AUTHORS OR DISTRIBUTORS BE LIABLE TO ANY PARTY
           FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES
           ARISING OUT OF THE USE OF THIS SOFTWARE, ITS DOCUMENTATION, OR ANY
           DERIVATIVES THEREOF, EVEN IF THE AUTHORS HAVE BEEN ADVISED OF THE
           POSSIBILITY OF SUCH DAMAGE.
         
           THE AUTHORS AND DISTRIBUTORS SPECIFICALLY DISCLAIM ANY WARRANTIES,
           INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY,
           FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.  THIS SOFTWARE
           IS PROVIDED ON AN ""AS IS"" BASIS, AND THE AUTHORS AND DISTRIBUTORS HAVE
           NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR
           MODIFICATIONS.
         
           GOVERNMENT USE: If you are acquiring this software on behalf of the
           U.S. government, the Government shall have only ""Restricted Rights""
           in the software and related documentation as defined in the Federal
           Acquisition Regulations (FARs) in Clause 52.227.19 (c) (2).  If you
           are acquiring the software on behalf of the Department of Defense, the
           software shall be classified as ""Commercial Computer Software"" and the
           Government shall have only ""Restricted Rights"" as defined in Clause
           252.227-7013 (c) (1) of DFARs.  Notwithstanding the foregoing, the
           authors grant the U.S. Government and others acting in its behalf
           permission to use and distribute the software in accordance with the
           terms specified in this license.Apache License
         Version 2.0, January 2004
         http://www.apache.org/licenses/
         
         TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
         
         1. Definitions.
         
         ""License"" shall mean the terms and conditions for use, reproduction, and
         distribution as defined by Sections 1 through 9 of this document.
         
         ""Licensor"" shall mean the copyright owner or entity authorized by the copyright
         owner that is granting the License.
         
         ""Legal Entity"" shall mean the union of the acting entity and all other entities
         that control, are controlled by, or are under common control with that entity.
         For the purposes of this definition, ""control"" means (i) the power, direct or
         indirect, to cause the direction or management of such entity, whether by
         contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the
         outstanding shares, or (iii) beneficial ownership of such entity.
         
         ""You"" (or ""Your"") shall mean an individual or Legal Entity exercising
         permissions granted by this License.
         
         ""Source"" form shall mean the preferred form for making modifications, including
         but not limited to software source code, documentation source, and configuration
         files.
         
         ""Object"" form shall mean any form resulting from mechanical transformation or
         translation of a Source form, including but not limited to compiled object code,
         generated documentation, and conversions to other media types.
         
         ""Work"" shall mean the work of authorship, whether in Source or Object form, made
         available under the License, as indicated by a copyright notice that is included
         in or attached to the work (an example is provided in the Appendix below).
         
         ""Derivative Works"" shall mean any work, whether in Source or Object form, that
         is based on (or derived from) the Work and for which the editorial revisions,
         annotations, elaborations, or other modifications represent, as a whole, an
         original work of authorship. For the purposes of this License, Derivative Works
         shall not include works that remain separable from, or merely link (or bind by
         name) to the interfaces of, the Work and Derivative Works thereof.
         
         ""Contribution"" shall mean any work of authorship, including the original version
         of the Work and any modifications or additions to that Work or Derivative Works
         thereof, that is intentionally submitted to Licensor for inclusion in the Work
         by the copyright owner or by an individual or Legal Entity authorized to submit
         on behalf of the copyright owner. For the purposes of this definition,
         ""submitted"" means any form of electronic, verbal, or written communication sent
         to the Licensor or its representatives, including but not limited to
         communication on electronic mailing lists, source code control systems, and
         issue tracking systems that are managed by, or on behalf of, the Licensor for
         the purpose of discussing and improving the Work, but excluding communication
         that is conspicuously marked or otherwise designated in writing by the copyright
         owner as ""Not a Contribution.""
         
         ""Contributor"" shall mean Licensor and any individual or Legal Entity on behalf
         of whom a Contribution has been received by Licensor and subsequently
         incorporated within the Work.
         
         2. Grant of Copyright License.
         
         Subject to the terms and conditions of this License, each Contributor hereby
         grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,
         irrevocable copyright license to reproduce, prepare Derivative Works of,
         publicly display, publicly perform, sublicense, and distribute the Work and such
         Derivative Works in Source or Object form.
         
         3. Grant of Patent License.
         
         Subject to the terms and conditions of this License, each Contributor hereby
         grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,
         irrevocable (except as stated in this section) patent license to make, have
         made, use, offer to sell, sell, import, and otherwise transfer the Work, where
         such license applies only to those patent claims licensable by such Contributor
         that are necessarily infringed by their Contribution(s) alone or by combination
         of their Contribution(s) with the Work to which such Contribution(s) was
         submitted. If You institute patent litigation against any entity (including a
         cross-claim or counterclaim in a lawsuit) alleging that the Work or a
         Contribution incorporated within the Work constitutes direct or contributory
         patent infringement, then any patent licenses granted to You under this License
         for that Work shall terminate as of the date such litigation is filed.
         
         4. Redistribution.
         
         You may reproduce and distribute copies of the Work or Derivative Works thereof
         in any medium, with or without modifications, and in Source or Object form,
         provided that You meet the following conditions:
         
         You must give any other recipients of the Work or Derivative Works a copy of
         this License; and
         You must cause any modified files to carry prominent notices stating that You
         changed the files; and
         You must retain, in the Source form of any Derivative Works that You distribute,
         all copyright, patent, trademark, and attribution notices from the Source form
         of the Work, excluding those notices that do not pertain to any part of the
         Derivative Works; and
         If the Work includes a ""NOTICE"" text file as part of its distribution, then any
         Derivative Works that You distribute must include a readable copy of the
         attribution notices contained within such NOTICE file, excluding those notices
         that do not pertain to any part of the Derivative Works, in at least one of the
         following places: within a NOTICE text file distributed as part of the
         Derivative Works; within the Source form or documentation, if provided along
         with the Derivative Works; or, within a display generated by the Derivative
         Works, if and wherever such third-party notices normally appear. The contents of
         the NOTICE file are for informational purposes only and do not modify the
         License. You may add Your own attribution notices within Derivative Works that
         You distribute, alongside or as an addendum to the NOTICE text from the Work,
         provided that such additional attribution notices cannot be construed as
         modifying the License.
         You may add Your own copyright statement to Your modifications and may provide
         additional or different license terms and conditions for use, reproduction, or
         distribution of Your modifications, or for any such Derivative Works as a whole,
         provided Your use, reproduction, and distribution of the Work otherwise complies
         with the conditions stated in this License.
         
         5. Submission of Contributions.
         
         Unless You explicitly state otherwise, any Contribution intentionally submitted
         for inclusion in the Work by You to the Licensor shall be under the terms and
         conditions of this License, without any additional terms or conditions.
         Notwithstanding the above, nothing herein shall supersede or modify the terms of
         any separate license agreement you may have executed with Licensor regarding
         such Contributions.
         
         6. Trademarks.
         
         This License does not grant permission to use the trade names, trademarks,
         service marks, or product names of the Licensor, except as required for
         reasonable and customary use in describing the origin of the Work and
         reproducing the content of the NOTICE file.
         
         7. Disclaimer of Warranty.
         
         Unless required by applicable law or agreed to in writing, Licensor provides the
         Work (and each Contributor provides its Contributions) on an ""AS IS"" BASIS,
         WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied,
         including, without limitation, any warranties or conditions of TITLE,
         NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are
         solely responsible for determining the appropriateness of using or
         redistributing the Work and assume any risks associated with Your exercise of
         permissions under this License.
         
         8. Limitation of Liability.
         
         In no event and under no legal theory, whether in tort (including negligence),
         contract, or otherwise, unless required by applicable law (such as deliberate
         and grossly negligent acts) or agreed to in writing, shall any Contributor be
         liable to You for damages, including any direct, indirect, special, incidental,
         or consequential damages of any character arising as a result of this License or
         out of the use or inability to use the Work (including but not limited to
         damages for loss of goodwill, work stoppage, computer failure or malfunction, or
         any and all other commercial damages or losses), even if such Contributor has
         been advised of the possibility of such damages.
         
         9. Accepting Warranty or Additional Liability.
         
         While redistributing the Work or Derivative Works thereof, You may choose to
         offer, and charge a fee for, acceptance of support, warranty, indemnity, or
         other liability obligations and/or rights consistent with this License. However,
         in accepting such obligations, You may act only on Your own behalf and on Your
         sole responsibility, not on behalf of any other Contributor, and only if You
         agree to indemnify, defend, and hold each Contributor harmless for any liability
         incurred by, or claims asserted against, such Contributor by reason of your
         accepting any such warranty or additional liability.
         
         END OF TERMS AND CONDITIONS
         
         APPENDIX: How to apply the Apache License to your work
         
         To apply the Apache License to your work, attach the following boilerplate
         notice, with the fields enclosed by brackets ""[]"" replaced with your own
         identifying information. (Don't include the brackets!) The text should be
         enclosed in the appropriate comment syntax for the file format. We also
         recommend that a file or class name and description of purpose be included on
         the same ""printed page"" as the copyright notice for easier identification within
         third-party archives.
         
            Copyright [yyyy] [name of copyright owner]
         
            Licensed under the Apache License, Version 2.0 (the ""License"");
            you may not use this file except in compliance with the License.
            You may obtain a copy of the License at
         
              http://www.apache.org/licenses/LICENSE-2.0
         
            Unless required by applicable law or agreed to in writing, software
            distributed under the License is distributed on an ""AS IS"" BASIS,
            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            See the License for the specific language governing permissions and
            limitations under the License.

**Python Version:** >=3.9

**Description:**
<div align=""center"">
  <img src=""https://pandas.pydata.org/static/img/pandas.svg""><br>
</div>

-----------------

# pandas: powerful Python data analysis toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |
| Meta 

**Key Dependencies:**
- numpy>=1.22.4; python_version < ""3.11""
- numpy>=1.23.2; python_version == ""3.11""
- numpy>=1.26.0; python_version >= ""3.12""
- python-dateutil>=2.8.2
- pytz>=2020.1

**Release Files:** 42 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [documentation](https://pandas.pydata.org/docs/)
- [homepage](https://pandas.pydata.org)
- [repository](https://github.com/pandas-dev/pandas)
","Powerful data structures for data analysis, time series, and statistics","[""api"",""aws"",""data-analysis"",""database"",""networking"",""python""]"
e485bc8f-6685-4d9b-b95e-e2022c20d8ac,fastapi@0.116.0,e0ec7b85-25a1-4fac-b427-33bddd886cb1,"fastapi 0.116.0 - FastAPI framework, high performance, easy to learn, fast to code, ready for production",https://pypi.org/project/fastapi/0.116.0/,"# fastapi 0.116.0

FastAPI framework, high performance, easy to learn, fast to code, ready for production

**Version:** 0.116.0
**Author:** Unknown
**License:** Not specified

**Python Version:** >=3.8

**Description:**
<p align=""center"">
  <a href=""https://fastapi.tiangolo.com""><img src=""https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png"" alt=""FastAPI""></a>
</p>
<p align=""center"">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster"" target=""_blank"">
    <img src=""https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&branch=master"" alt=""Test"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg"" alt=""Coverage"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package"" alt=""Package version"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img sr

**Key Dependencies:**
- starlette<0.48.0,>=0.40.0
- pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4
- typing-extensions>=4.8.0
- fastapi-cli[standard]>=0.0.8; extra == ""standard""
- httpx>=0.23.0; extra == ""standard""

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Changelog](https://fastapi.tiangolo.com/release-notes/)
- [Documentation](https://fastapi.tiangolo.com/)
- [Homepage](https://github.com/fastapi/fastapi)
","FastAPI framework, high performance, easy to learn, fast to code, ready for production","[""api"",""machine-learning"",""manufacturing"",""performance"",""python"",""reinforcement-learning""]"
6431e681-f2c7-40b1-a2a6-fb20ea3d7d1a,Django@5.2.4,e0ec7b85-25a1-4fac-b427-33bddd886cb1,"Django 5.2.4 - A high-level Python web framework that encourages rapid development and clean, pragmatic design.",https://pypi.org/project/Django/5.2.4/,"# Django 5.2.4

A high-level Python web framework that encourages rapid development and clean, pragmatic design.

**Version:** 5.2.4
**Author:** Unknown
**License:** BSD-3-Clause

**Python Version:** >=3.10

**Description:**
======
Django
======

Django is a high-level Python web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.

All documentation is in the ""``docs``"" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

* First, read ``docs/intro/install.txt`` for instructions on installing Django.

* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,
  ``docs/intro/tutorial02.txt``, etc.).

* If you want to set up an actual deployment server, read
  ``docs/howto/deployment/index.txt`` for instructions.

* You'll probably want to read through the topical guides (in ``docs/topics``)
  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific
  problems, and check out the reference (``docs/ref``) for gory details.

* See ``docs/README`` for instructions on building an HTML version of the docs.

Docs are updated rigorously. If yo

**Key Dependencies:**
- asgiref>=3.8.1
- sqlparse>=0.3.1
- tzdata; sys_platform == ""win32""
- argon2-cffi>=19.1.0; extra == ""argon2""
- bcrypt; extra == ""bcrypt""

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Documentation](https://docs.djangoproject.com/)
- [Funding](https://www.djangoproject.com/fundraising/)
- [Homepage](https://www.djangoproject.com/)
","A high-level Python web framework that encourages rapid development and clean, pragmatic design.","[""api"",""artificial-intelligence"",""database"",""machine-learning"",""python"",""tensorflow""]"
098b0814-c929-4438-a125-cef56e2ec25e,plotly@6.2.0,e0ec7b85-25a1-4fac-b427-33bddd886cb1,plotly 6.2.0 - An open-source interactive data visualization library for Python,https://pypi.org/project/plotly/6.2.0/,"# plotly 6.2.0

An open-source interactive data visualization library for Python

**Version:** 6.2.0
**Author:** Unknown
**License:** MIT License
        
        Copyright (c) 2016-2024 Plotly Technologies Inc.
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the ""Software""), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in
        all copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
        THE SOFTWARE.
        

**Python Version:** >=3.8

**Description:**
# plotly.py

<table>
    <tr>
        <td>Latest Release</td>
        <td>
            <a href=""https://pypi.org/project/plotly/""/>
            <img src=""https://badge.fury.io/py/plotly.svg""/>
        </td>
    </tr>
    <tr>
        <td>User forum</td>
        <td>
            <a href=""https://community.plotly.com/""/>
            <img src=""https://img.shields.io/badge/help_forum-discourse-blue.svg""/>
        </td>
    </tr>
    <tr>
        <td>PyPI Downloads</td>
        <td>
            <a href=""https://pepy.tech/project/plotly""/>
            <img src=""https://pepy.tech/badge/plotly/month""/>
        </td>
    </tr>
    <tr>
        <td>License</td>
        <td>
            <a href=""https://opensource.org/licenses/MIT""/>
            <img src=""https://img.shields.io/badge/License-MIT-yellow.svg""/>
        </td>
    </tr>
</table>

<div align=""center"">
  <a href=""https://dash.plotly.com/project-maintenance"">
    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" 

**Key Dependencies:**
- narwhals>=1.15.1
- packaging
- numpy; extra == ""express""
- kaleido>=1.0.0; extra == ""kaleido""
- pytest; extra == ""dev-core""

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Changelog](https://github.com/plotly/plotly.py/blob/main/CHANGELOG.md)
- [Documentation](https://plotly.com/python/)
- [Github](https://github.com/plotly/plotly.py)
",An open-source interactive data visualization library for Python,"[""announcement"",""api"",""artificial-intelligence"",""data-science"",""imaging"",""python""]"
17f36c77-2c8a-45ea-87bb-247f61f1aca7,streamlit@1.46.1,e0ec7b85-25a1-4fac-b427-33bddd886cb1,streamlit 1.46.1 - A faster way to build and share data apps,https://pypi.org/project/streamlit/1.46.1/,"# streamlit 1.46.1

A faster way to build and share data apps

**Version:** 1.46.1
**Author:** Snowflake Inc
**License:** Apache License 2.0

**Python Version:** !=3.9.7,>=3.9

**Description:**
<br>

<img src=""https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png"" alt=""Streamlit logo"" style=""margin-top:50px""></img>

# Welcome to Streamlit 👋

**A faster way to build and share data apps.**

## What is Streamlit?

Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.

### Why choose Streamlit?

- **Simple and Pythonic:** Write beautiful, easy-to-read code.
- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.
- **Live editing:** See your app update instantly as you edit your script.
- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.

## Installation

Open a terminal and run:

```bash
$ pip install streamlit
$

**Key Dependencies:**
- altair<6,>=4.0
- blinker<2,>=1.5.0
- cachetools<7,>=4.0
- click<9,>=7.0
- numpy<3,>=1.23

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Bug Tracker](https://github.com/streamlit/streamlit/issues)
- [Community](https://discuss.streamlit.io/)
- [Documentation](https://docs.streamlit.io/)
",A faster way to build and share data apps,"[""artificial-intelligence"",""computer-science"",""machine-learning"",""python"",""snowflake"",""tensorflow""]"
cd2972b4-a1b6-40e6-954f-982674117308,fastapi@0.115.14,e0ec7b85-25a1-4fac-b427-33bddd886cb1,"fastapi 0.115.14 - FastAPI framework, high performance, easy to learn, fast to code, ready for production",https://pypi.org/project/fastapi/0.115.14/,"# fastapi 0.115.14

FastAPI framework, high performance, easy to learn, fast to code, ready for production

**Version:** 0.115.14
**Author:** Unknown
**License:** Not specified

**Python Version:** >=3.8

**Description:**
<p align=""center"">
  <a href=""https://fastapi.tiangolo.com""><img src=""https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png"" alt=""FastAPI""></a>
</p>
<p align=""center"">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster"" target=""_blank"">
    <img src=""https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&branch=master"" alt=""Test"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg"" alt=""Coverage"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package"" alt=""Package version"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img sr

**Key Dependencies:**
- starlette<0.48.0,>=0.40.0
- pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4
- typing-extensions>=4.8.0
- fastapi-cli[standard]>=0.0.8; extra == ""standard""
- httpx>=0.23.0; extra == ""standard""

**Release Files:** 2 files
**Package Types:** bdist_wheel, sdist

**Project Links:**
- [Changelog](https://fastapi.tiangolo.com/release-notes/)
- [Documentation](https://fastapi.tiangolo.com/)
- [Homepage](https://github.com/fastapi/fastapi)
","FastAPI framework, high performance, easy to learn, fast to code, ready for production","[""api"",""machine-learning"",""manufacturing"",""performance"",""python"",""reinforcement-learning""]"