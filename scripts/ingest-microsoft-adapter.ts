#!/usr/bin/env tsx

import { config } from 'dotenv'
import { createClient } from '@supabase/supabase-js'
import { fetchAndParse, MICROSOFT_BLOG_CONFIG } from '../packages/adapters/microsoft-blog'

// Load environment variables
config({ path: '.env.local' })

// Create Supabase client
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
)

async function ingestMicrosoftAdapter() {
  console.log('üöÄ Microsoft Excel & Power BI Blog Ingestion Script\n')
  
  try {
    // Step 1: Get Microsoft source from database
    console.log('üìù Finding Microsoft source in database...')
    const { data: source, error: sourceError } = await supabase
      .from('sources')
      .select('id, name')
      .eq('name', MICROSOFT_BLOG_CONFIG.name)
      .single()
    
    if (sourceError || !source) {
      console.error('‚ùå Microsoft Excel & Power BI Blog source not found in database')
      console.log('Available sources should include:', MICROSOFT_BLOG_CONFIG.name)
      process.exit(1)
    }
    
    console.log(`‚úÖ Found source: ${source.name} (ID: ${source.id})`)
    
    // Step 2: Fetch and parse Microsoft feeds
    console.log('\nüì¶ Fetching Microsoft blog feeds...')
    const startTime = Date.now()
    
    const items = await fetchAndParse()
    
    const elapsed = Date.now() - startTime
    console.log(`‚úÖ Fetched ${items.length} blog posts in ${elapsed}ms`)
    
    if (items.length === 0) {
      console.warn('‚ö†Ô∏è  No items to ingest')
      return
    }
    
    // Step 3: Check for existing stories (by external_id and URL)
    console.log('\nüîç Checking for existing stories...')
    const externalIds = items.map(item => item.externalId)
    const urls = items.map(item => item.url)
    
    // Check for existing external_ids in this source
    const { data: existingByExternalId } = await supabase
      .from('stories')
      .select('external_id')
      .eq('source_id', source.id)
      .in('external_id', externalIds)
    
    // Check for existing URLs across all sources (due to unique constraint)
    const { data: existingByUrl } = await supabase
      .from('stories')
      .select('url')
      .in('url', urls)
    
    const existingIds = new Set(existingByExternalId?.map(s => s.external_id) || [])
    const existingUrls = new Set(existingByUrl?.map(s => s.url) || [])
    
    // Filter out existing items
    const candidateItems = items.filter(item => 
      !existingIds.has(item.externalId) && !existingUrls.has(item.url)
    )
    
    // Internal deduplication: remove duplicates within the batch
    const seenUrls = new Set<string>()
    const seenExternalIds = new Set<string>()
    const newItems = candidateItems.filter(item => {
      if (seenUrls.has(item.url) || seenExternalIds.has(item.externalId)) {
        console.log(`   Skipping duplicate: ${item.externalId} (${item.url})`)
        return false
      }
      seenUrls.add(item.url)
      seenExternalIds.add(item.externalId)
      return true
    })
    
    console.log(`üìä Found ${existingIds.size} existing by external_id, ${existingUrls.size} existing by URL`)
    console.log(`üìä After deduplication: ${candidateItems.length} candidates ‚Üí ${newItems.length} new stories`)
    
    if (newItems.length === 0) {
      console.log('‚úÖ No new stories to insert')
      return
    }
    
    // Step 4: Insert new stories
    console.log(`\nüíæ Inserting ${newItems.length} new stories...`)
    
    const storiesToInsert = newItems.map(item => ({
      source_id: source.id,
      source: MICROSOFT_BLOG_CONFIG.name, // For backward compatibility with old schema
      title: item.title,
      url: item.url,
      content: item.content,
      published_at: item.publishedAt.toISOString(),
      tags: item.tags,
      external_id: item.externalId
    }))
    
    const { error: insertError } = await supabase
      .from('stories')
      .insert(storiesToInsert)
    
    if (insertError) {
      throw new Error(`Failed to insert stories: ${insertError.message}`)
    }
    
    console.log('‚úÖ Successfully inserted new stories')
    
    // Step 5: Summary
    console.log('\nüìä Ingestion Summary:')
    console.log(`   ‚Ä¢ Source: ${MICROSOFT_BLOG_CONFIG.name} (ID: ${source.id})`)
    console.log(`   ‚Ä¢ Total blog posts fetched: ${items.length}`)
    console.log(`   ‚Ä¢ New stories inserted: ${newItems.length}`)
    console.log(`   ‚Ä¢ Existing stories skipped: ${existingIds.size + existingUrls.size}`)
    console.log(`   ‚Ä¢ Fetch time: ${elapsed}ms`)
    
    // Show sample of new stories
    if (newItems.length > 0) {
      console.log('\nüìù Sample new stories:')
      newItems.slice(0, 3).forEach((item, i) => {
        console.log(`   ${i + 1}. ${item.title}`)
        console.log(`      External ID: ${item.externalId}`)
        console.log(`      Published: ${item.publishedAt.toISOString().slice(0, 10)}`)
        console.log(`      Tags: ${item.tags.join(', ')}`)
      })
    }
    
    console.log('\nüéâ Microsoft ingestion completed successfully!')
    
  } catch (error) {
    console.error('‚ùå Microsoft ingestion failed:', error)
    process.exit(1)
  }
}

if (require.main === module) {
  ingestMicrosoftAdapter()
} 